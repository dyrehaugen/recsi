[["index.html", "Economic Issues 1 Economics", " Economic Issues Dyrehaugen Web Notebook 2023-12-31 1 Economics Economics is a broad field. Here we look at particular economic issues. Economics is too important to leave to the economists "],["the-economy.html", "2 The Economy 2.1 Institutions", " 2 The Economy There is no such thing as “the economy” as conceived separately from politics (ie: the state). 2.1 Institutions 2.1.1 Bretton Woods Coppola The Bretton Woods system enshrined the dominance of Western countries, and, in particular, the US. To this day, the two Bretton Woods institutions, the International Monetary Fund (IMF) and the World Bank, are always run by, respectively, a European and an American, and their funding predominantly comes from the US. The US has regarded it as its prerogative to decide which countries these institutions should support, and what form that support should take. Too often, “support” has meant imposing Western ideals of economic management, to the detriment of local people and customs. After the Bretton Woods system collapsed in 1971, there was a period of economic and political chaos, out of which eventually emerged the international dollar standard backed by the financial reserves of powerful oil-producing nations. Under the “petrodollar” standard, developing countries that paid for imports with borrowed dollars suffered repeated debt crises. The IMF – originally created to prevent destabilisation of the Bretton Woods currency system by trade imbalances – became the vehicle for enforcing the transfer of resources from developing countries to their Western creditors. The collapse of the Bretton Woods currency system is arguably attributable to the US’s burgeoning fiscal deficit during the Vietnam war, which made it impossible for the dollar to hold its peg to gold. 2.1.2 Post Bretton Woods The elimination of capital and exchange controls after the collapse of Bretton Woods has enabled the rich to place their wealth out of reach of tax authorities and caused damaging flows of hot money that have particularly, though not exclusively, destabilised developing countries. Monetary policy and “growth-friendly” fiscal policies have widened inequality, creating exorbitant wealth for a few at the expense of social safety nets and public investment. Coppola (2023) Paradigm Shift "],["duality.html", "3 Duality", " 3 Duality Perhaps the best definition of economics is that it is ‘secular theology’. Economics adopts the veneer of science, but, like theology, is based on untestable definiti ons. The most pervasive dualism is the equivalence between income and productivity. We have a logical problem with the productivity-income duality. Either it is correct but unnecessary, or it is both unnecessary and incorrect. Either way, the duality is unsound. Of course, there is nothing wrong with having a hypothesis that is logically unsound. It just means that when you notice the problem, you should abandon the hypothesis. But that’s not what happened in economics. Instead, the income-productivity duality became the basis for the entire system of national accounts. To address this logical problem, economists have invented more theology. Prices, they claim, are themselves a duality. Prices are both a monetary quantity and a measure of utility. The problem is that economists never measure utility. They infer it from prices. And so the whole operation becomes circular — a theological definition, and nothing more. Joan Robinson nicely summarized the situation: Utility is a metaphysical concept of impregnable circularity; utility is the quality in commodities that makes individuals want to buy them, and the fact that individuals want to buy commodities shows that they have utility. In other words, economists propose a definition, and use this definition to justify it. That’s theology. Assumptions When people like Milton Friedman spout theological nonsense, they’ll try to convince you that assumptions don’t matter. Friedman notoriously claimed that if a theory gives ‘correct predictions’, that’s good enough. Don’t believe him. Assumptions are the most basic part of science. If they are wrong, the whole theory is garbage. Fix (2022) Dualism "],["economic-rent.html", "4 Economic Rent 4.1 Digital Rent 4.2 CEO compensation is a rent", " 4 Economic Rent 4.1 Digital Rent Birch Big Tech ecosystems are important techno-economic sites of new and emerging forms of digital rentiership. Big Tech is characterized by the emergence of new and specifically digital forms of rentiership, defined as the construction and extraction of value through the techno-economic extension of ownership and/or control over assets, often resulting from some artificial or natural scarcity, quality, or productivity We outline four emerging forms of digital rentiership in Big Tech ecosystems reflecting the similarities and diversities in Big Tech firms themselves: ‘enclave rents’ created through the control of ecosystems; ‘expected monopoly rents’ created through the performative fulfilment of future narratives; ‘engagement rents’ constituted via rankings and metrics that differentiate users by their engagement with digital services and products; and ‘reflexivity rents’ obtained by exploiting ecosystem rules and norms. Birch (2021) Big Tech: Four Emerging Forms of Digital Rentiership 4.2 CEO compensation is a rent Tooze The fact that CEO compensation has grown much faster than the pay of the top 0.1% of wage earners indicates that CEO compensation growth does not simply reflect a competitive race for skills (the “market for talent”) that would also increase the value of highly paid professionals more generally. Rather, the growing pay differential between CEOs and top 0.1% earners suggests the growth of substantial economic rents (income not related to a corresponding growth of productivity) in CEO compensation. CEO compensation does not appear to reflect the greater productivity of executives but their ability to extract concessions from corporate boards—a power that stems from dysfunctional systems of corporate governance in the United States. But because so much of CEOs’ income constitutes economic rent, there would be no adverse impact on the economy’s output or on employment if CEOs earned less or were taxed more. Tooze (2023) Explaining CEO pay "],["scarcity.html", "5 Scarcity 5.1 Forced ‘scarcity’ (sabotage) 5.2 There is no Scarcity 5.3 How There’s More to Economics Than the Science of Scarcity 5.4 The Case against Civilization 5.5 Budget Constraints", " 5 Scarcity the economy may collapse from the reduced consumption. 5.1 Forced ‘scarcity’ (sabotage) Fix Stagflation as sabotage Created by political economists Jonathan Nitzan and Shimshon Bichler, the theory of capital as power is decidedly non-mainstream. (It’s about as far from neoclassical economics as you can get.) Whereas mainstream economists look at capitalist society and see ‘productivity’ everywhere, Nitzan and Bichler see business ‘sabotage’. It’s an idea that at first seems incendiary. But once you think about the behavior of real-world corporations, the notion of business sabotage seems on point. Everywhere we look, we find big corporations behaving badly. Sci-fi writer Cory Doctorow calls this behavior ‘enshittification’. Looking at social media companies, he notes that what people want from their social-media apps is simple. People want to connect with their friends. And they want a feed that shows them what their friends are up to. That’s it. The problem is that this simple desire is at odds with making a profit. You see, to make a profit, social media companies need to slam sponsored content down your throat — i.e. fill your feed with ads and clickbait. Because that’s the opposite of what people want, Doctorow notes that social-media apps follow a predictable trend. First, they lure people onto the platform by giving them what they want (a feed filled with their friends’ content). Then, once enough people are locked in, the company flips the money switch and enshittifies its platform with paid junk. What’s this enshittification got to do with inflation? Everything! Take Twitter as an example. After Elon Musk bought Twitter, he moved frantically to cut costs and raise prices. Almost immediately, Musk turned on the inflation dial by charging for blue checkmarks.1 Prior to this move, blue checkmarks had been Twitter’s way of authenticating ‘accounts of public interest’. Sure, the authentication was somewhat arbitrary … but at least it was free. Today, the same blue checkmark will cost you $8 a month. And it’s no longer a marker solely of prestige. No, the blue checkmark is now a pay-to-play system that gives you “priority ranking in search, mentions and replies”. In other words, if you want your followers to see your content (something you previously got for free), you’ve got to hand Mr. Musk $8 a month. That’s business sabotage, plain and simple. It’s inflation through enforced scarcity. The ‘enforced’ part is key. Yes, we live in a world in which resources are finite, and hence ‘scarce’. But regardless of a resources’ innate abundance, maintaining high prices requires restriction. Diamonds are a good example. Sure, they are rare. But as my colleague D.T. Cochrane observed in his seminal PhD thesis, diamonds are not rare enough to suit the diamond business. That’s why De Beers (a diamond cartel) spent years buying up diamonds to purposefully keep them off the market. Like all savvy businesses, De Beers knew that enforced scarcity (aka sabotage) was the key to high prices. Looking at capitalist society, Nitzan and Bichler argue that this enforced scarcity tends to come in waves, largely because it is unstable. For instance, if I bolster prices by restricting access to my property, the risk is that my competitor will undercut me. Conversely, if I undercut my competitor, the risk is that they will respond by cutting there prices in turn, resulting in a price-race to the bottom. Neoclassical economists look at these dynamics and conclude that they will lead to market equilibrium. But that’s because economists suppose that businesses won’t coordinate. In the real world, though, businesses coordinate all the time. It’s called herd behavior. If everyone else is cutting prices and selling more stuff, I’d better do the same. And if the herd decides to restrict supply and hike prices, I’d best join in. The result will be an oscillation between periods of economic boom with low inflation, and periods of economic bust with high inflation. As it turns out, this is exactly what happens in the real world. Across countries, economic growth (as measured by energy consumption) tends to be high when inflation is low (and vice versa). WAR War is a good example. For mainstream economists, war is simply not part of their theory. But for Nitzan and Bichler, war is the most extreme form of sabotage, frequently associated with price gouging and profiteering. In particular, the (differential) profitability of oil companies seems to be tightly related to war in the Middle East. Fix (2023) The Cause of Stagflation 5.2 There is no Scarcity Hickel When we look at the world in terms of real resources and energy (i.e., the stuff of provisioning), it becomes clear that there is no scarcity at all. The problem isn’t that there’s not enough, the problem, again, is that it is maldistributed. A huge chunk of global commodity production is totally irrelevant to human needs and well-being. Consider all the resources and energy that are mobilized for the sake of fast fashion, throwaway gadgets, single-use stadiums, SUVs, bottled water, cruise ships and the military-industrial complex. Consider the scale of needless consumption that is stimulated by manipulative advertising schemes, or enforced by planned obsolescence. Consider the quantity of private cars that people have been forced to buy because the fossil fuel industry and automobile manufactures have lobbied so aggressively against public transportation. Consider that the beef industry alone uses nearly 60% of the world’s agricultural land, to produce only 2% of global calories. There is no scarcity. Rather, the world’s resources and energy are appropriated (disproportionately from the global South) in order to service the interests of capital and affluent consumers (disproportionately in the global North). We can state it more clearly: our economic system is not designed to meet human needs; it is designed to facilitate capital accumulation. And in order to do so, it imposes brutal scarcity on the majority of people, and cheapens human and nonhuman life. It is irrational to believe that simply “growing” such an economy, in aggregate, will somehow magically achieve the social outcomes we want. Hickel 5.3 How There’s More to Economics Than the Science of Scarcity Gruen One way economists describe their discipline to themselves has proven beguilingly seductive since it was codified by Lionel Robbins 90 years ago — that economics is the science of scarcity and that it is, therefore, paradigmatically about trade-offs. This approach has become a kind of counterfeit metaphysics — a means by which practice becomes increasingly thoughtless and alienated from economic reality whilst practitioners affect rigor and insightfulness. Trade-offs As a Paradigm The reason for the tradeoff relationship between equality and efficiency is the empty one that equality is not efficiency. This structuring of entire bodies of thought around the empty observation that one thing is not another thing is replicated endlessly. Humans’ extraordinary and unique capacity for shared intentionality is the foundation of our astounding productivity. The most fundamental means by which this is done is via what I have called generative orders — language and culture being preeminent examples, though markets and money are others. Within these generative orders, our cognition of the world, our intentions, and our mutual expectations of each other are entangled. This foundation enables us to build other special-purpose institutions. My point has simply been to show one theoretical framing of the relationship between efficiency and equality that proceeds from careful, critical observation of and abstraction from reality. If this is well-judged, our understanding of reality improves as do our prospects of improving it. The textbook approach couldn’t be more different. Turns out that it is metaphysical fairy-floss. The “efficiency-equality” trade-off exists as a particular case of the general one that if you wish to achieve one thing, doing something else could get in your way. Gruen (2022) How There’s More to Economics Than the Science of Scarcity 5.4 The Case against Civilization Lanchester John Maynard Keynes’s famous 1930 essay “The Economic Possibilities for Our Grandchildren.” Keynes speculated that if the world continued to get richer we would naturally end up enjoying a high standard of living while doing much less work. He thought that “the economic problem” of having enough to live on would be solved, and “the struggle for subsistence” would be over: When the accumulation of wealth is no longer of high social importance, there will be great changes in the code of morals. We shall be able to rid ourselves of many of the pseudo-moral principles which have hag-ridden us for two hundred years, by which we have exalted some of the most distasteful of human qualities into the position of the highest virtues. We shall be able to afford to dare to assess the money-motive at its true value. The love of money as a possession—as distinguished from the love of money as a means to the enjoyments and realities of life—will be recognized for what it is, a somewhat disgusting morbidity, one of those semi-criminal, semi-pathological propensities which one hands over with a shudder to the specialists in mental disease. The world has indeed got richer, but any such shift in morals and values is hard to detect. Money and the value system around its acquisition are fully intact. Greed is still good. The study of hunter-gatherers, who live for the day and do not accumulate surpluses, shows that humanity can live more or less as Keynes suggests. It’s just that we’re choosing not to. A key to that lost or forsworn ability, Suzman suggests, lies in the ferocious egalitarianism of hunter-gatherers. For example, the most valuable thing a hunter can do is come back with meat. Unlike gathered plants, whose proceeds are “not subject to any strict conventions on sharing,” hunted meat is very carefully distributed according to protocol, and the people who eat the meat that is given to them go to great trouble to be rude about it. This ritual is called “insulting the meat,” and it is designed to make sure the hunter doesn’t get above himself and start thinking that he’s better than anyone else. “When a young man kills much meat,” a Bushman told the anthropologist Richard B. Lee, “he comes to think of himself as a chief or a big man, and he thinks of the rest of us as his servants or inferiors. . . . We can’t accept this.” The insults are designed to “cool his heart and make him gentle.” For these hunter-gatherers, Suzman writes, “the sum of individual self-interest and the jealousy that policed it was a fiercely egalitarian society where profitable exchange, hierarchy, and significant material inequality were not tolerated.” This egalitarian impulse, Suzman suggests, is central to the hunter-gatherer’s ability to live a life that is, on its own terms, affluent, but without abundance, without excess, and without competitive acquisition. The secret ingredient seems to be the positive harnessing of the general human impulse to envy. As he says, “If this kind of egalitarianism is a precondition for us to embrace a post-labor world, then I suspect it may prove a very hard nut to crack.” There’s a lot that we could learn from the oldest extant branch of humanity, but that doesn’t mean we’re going to put the knowledge into effect. A socially positive use of envy—now, that would be a technology almost as useful as fire. Lanchester (2017) The Case against Civilization 5.5 Budget Constraints Tooze If Keynes was right that we can afford anything we can actually do, then the political implications are heavy. The insights of left Keynesianism, MMT et al are generally taken to be liberatory, freeing democratic politics from the shackles of fiscal rules, cases in point being America’s debt ceiling or Germany’s “debt brake” (Schuldenbremse). What actually constrains our choices, are the scarcity of economic resources, available technologies and our ability to reach political agreement. Questions of finance and “affordability” are secondary, organizational and technical matters. They reduce to matters of law and accounting. These are not trivial. They have their own institutional ramifications. But they are malleable. Keynesian logic once again forces us to face the politics directly. Tooze (2023) Chartbook 246: How America can pay for “two wars” at once "],["crowding-out.html", "6 Crowding Out 6.1 Crowding In", " 6 Crowding Out 6.1 Crowding In Dayen The remarkable changes in manufacturing construction over the past year, since the passage of two key Biden administration industrial-policy laws, is rapidly putting to rest a concept that has been embedded into the old understanding of the economy. The concept is called “crowd-out,” and it asserts that increases in government involvement in a business sector lead to reductions in private spending in that sector. For decades in Washington, this was not just an economic theory but something of an iron law. The Penn Wharton Budget Model, which is heavily influential in Washington, maintains that any government investment will reduce private capital investment. The model continually rated Biden administration policies that directed public spending as reducing GDP and private productive capital. Even government economic modelers regard government investment as wasteful by definition. As the Prospect noted in its special issue on economic modeling, the Congressional Budget Office explicitly assumes that public-sector investments are half as productive as investments in the private sector. If a private-sector investment returns 10 percent on an annual basis, public spending of the exact same amount is supposed to return 5 percent. This was never a particularly robust theory. It’s rooted in biases about government waste and private-sector efficiency, and it neglects the perfectly reasonable concept that private businesses, seeing the interest from public-sector investment in a particular sector, will crowd into it, if only to earn some of the spoils of all that public money. We are nearly a year into the passage of the Inflation Reduction Act, and we have enough evidence to render this theory, which created disadvantages for policymakers trying to direct public investment, as essentially wrong. As the Treasury Department has pointed out, total manufacturing plant construction has doubled, and real spending on computer, electronics, and electrical manufacturing, which through CHIPS and the IRA is one of the most targeted areas for public investment, has almost quadrupled in a year, adjusted for the increase in construction costs. Manufacturing construction spending, as Council of Economic Advisers member Heather Boushey recently pointed out, is at a six-decade high. Under the crowd-out theory, policies like the IRA and CHIPS are supposed to shrivel up private investment, because there is a fixed pool of capital and any government use of it leaves less for others. But yesterday, White House spokesperson Andrew Bates stated that $500 billion in private-sector investment has been created by the slate of industrial-policy bills, with most of that showing up today in plant fabrication. Analyst Jack Conness puts the number lower, at $227 billion, but that’s still quite substantial for such a short time frame. The Treasury report notes that real overall nonresidential construction (which doesn’t just include manufacturing) has gone up by 15 percent from November 2021 to April 2023. “Real public spending, which increased by 7 percent, did not crowd out real private spending, which increased by nearly 20 percent,” the report states. Treasury saw the same dynamic in infrastructure construction, where public spending is up over 20 percent but real private spending has still kept up, up around 14 percent. As Treasury puts it: “Again, the legislation increased public spending but has not crowded out private spending.” This is happening at a time when interest rates are increasing due to the Federal Reserve’s inflation-fighting, and at a time when working from home is depressing commercial real estate. Under classical analysis, that should be enough to depress private investment. The Biden administration bought into crowding in from the beginning. They’re being proven prescient. The public investments are leveraging private money in a host of contexts. Public dollars are attracting private dollars, not displacing them. Not everyone agrees with this analysis if you extend it out across the entire economy. Jason Furman, former Obama administration official and economics professor at Harvard, notes that overall private fixed investment is down for three straight quarters and below trend from before the pandemic by 2 percent. He doesn’t see that as an argument against the policies. “I support the policies. But I did not expect them to net add to GDP or jobs, at least on a one- or two-year horizon, and I don’t believe there is much or any evidence they have,” Furman said. “I cannot think of a better example of crowd-out in action than the current macroeconomic experience.” Of course, that is a macroeconomic outlook, and looking at the aggregate commingles with the shooting up of interest rates. Dayen (2023) Death of an Economic Theory "],["rationing.html", "7 Rationing", " 7 Rationing Climate Change Economics Curbing consumption can be done either through rationing or draconian taxation. Both are feasible technically although their political acceptability may not be the same. If one were to use rationing, one could introduce physical targets: there will be only x liters of gas per car annually and no family will be allowed to have more than two cars; or y kilograms of meat per person per month; or z kilowatts of electricity per household per month (or rolling blackouts). Clearly, there may be a black market for gas or meat, but the overall limits will be observed simply because they are given by the total availability of coupons. Some people might think that rationing is extraordinary, and I agree with them. But it has been done in a number of countries under wartime, and at times even during peacetime conditions, and it has worked. If indeed we face an emergency of such “terminal” proportions as the advocates of climate change claim, I do not see any reason why we should not resort to extreme measures. Milanovic "],["taxing.html", "8 Taxing 8.1 Tax Shifting 8.2 Georgism - Land Value Tax", " 8 Taxing Climate Change Economics But another approach (draconian taxation) is possible too. Instead of limiting physical quantities of goods and services that fulfill criteria (a) and (b) we would impose extremely heavy taxes on them. There is always a tax rate that would drive consumption of a good down to the level that we have in mind. It is here that I think we can use—again if we believe that the climate emergency is so dire—the lessons of covid. Economic dislocations would be huge. It is not only the question of the entire upper middle class and the rich in advanced countries (and, as we have seen, elsewhere) losing significant parts of their real income as prices of most “staple” commodities (for them) increase by two, three or ten times; the dislocation will affect large sectors of the economy. The effects will trickle down: unemployment will increase, incomes will plummet, the West will record the largest real income decline since the Great Depression. However if such policies were steadfastly pursued for a decade or two, not only would emissions plummet too (as they have done in 2020), but our behavior and ultimately the economy would adjust. People will find jobs in different activities that will remain untaxed and thus relatively cheaper and whose demand will go up. Revenues collected from taxing “bad” actvities may be used to subsidize “good” activities or retrain people who have lost their jobs. This is not magical thinking. These are policies that, with intergovernmental cooperation, knowledge of economics, data on global inequality, and the experience of covid, could be implemented. Milanovic 8.1 Tax Shifting FT US president Joe Biden’s plan to reform global corporate taxation will do little to help the countries most inneed of more tax revenues, say developing economies which are lobbying for greater power over multinationals. Washington’s ambitious proposal would tax 100 of the world’s largest companies on profits made in countries where they have little or no physical presence but derive substantial revenues and would introduce a global minimum tax rate, in a bid to end what it dubbed a “race to the bottom” where businesses channel profits through low-tax jurisdictions. But companies would pay most of their taxes in the country where they are headquartered, even if their profits — and in many cases the labour and raw materials used — are sourced from developing countries, senior diplomats and lobby groups told the Financial Times. They are also concerned that many developing countries are not participating in the negotiations over the proposal at the OECD and fear the eventual agreement is unlikely to reflect their interests. FT 8.2 Georgism - Land Value Tax Doucet Georgism is a school of political economy that is really upset about, among other things, the Rent Being Too Damn High. It seeks to liberate labor and capital alike from those who gatekeep access to scarce “non-produced assets,” such as land and natural resources, while still affirming the virtues of hard work and free enterprise. George uses the term “Land” to mean not just regular land, but everything that is external to human beings and the things they produce–nature itself, really. Georgism’s chief insight is to move economic thinking from a two-factor model (Labor and Capital) to a three-factor model (Land, Labor, and Capital). It’s chief (but not only) policy prescription is the Land Value Tax (LVT), which taxes real estate at as close to 100% of its “land rent” as possible (the amount of rent due to the land alone apart from “improvements” such as buildings). In actual practice, most Georgists seem to think 85% is a reasonable figure to target. Lars Doucet (2022) Does Georgism Work? Part 1: Is Land Really A Big Deal? Noah Smith (2022) How to sell Georgism to the middle class "],["nudging.html", "9 Nudging 9.1 Randomness", " 9 Nudging 9.1 Randomness Memo There is strong evidence from the lab that people have misperceptions about what randomness looks like. When a person is asked to generate a series that approximates the flipping of a coin, they will alternate between heads and tails too often, and balance the frequencies of heads and tails over too short a sequence. When people are asked to judge which of two different sequences of coin flips are more likely, they tend to pick sequences with more alternation, despite their probability being the same. What happens we look for a failure to perceive randomness in the outside world? Out of the lab? When people watch basketball, they often see a hot hand. They will describe players as “hot” and “in form”. Their belief is that the person who has just hit a shot or a series of shots is more likely to hit their next one. But is this belief in the “hot hand” a rational belief? Or is the hot hand an illusion, whereby, just like they do with coins, they are seeing streaks in what is actually randomness? In a famous examination of this question, Thomas Gilovich, Robert Vallone and Amos Tversky took shot data from a variety of sources, including the Philadelphia 76ers and Boston Celtics, and examined it for evidence of a hot hand. What did they find? The hot hand was an illusion. As Daniel Kahneman wrote in Thinking, Fast and Slow when describing this research: The hot hand is entirely in the eye of the beholders, who are consistently too quick to perceive order and causality in randomness. The hot hand is a massive and widespread cognitive illusion. Possibly even more interesting was the reaction to the findings from those in the sporting world. Despite the analysis, many sports figures denied that it could be true. Red Auerbach, who coached the Boston Celtics to nine NBA championships, said “Who is this guy? So he makes a study. I couldn’t care less.” This provides another insight, about which Gilovich wrote: The story of our research on the hot hand is only partly about the misperception of random events. It is also about how tenaciously people cling to their beliefs even in the face of hostile evidence. So, this isn’t just about the misperception of the hot hand, but also about the failure of people to see their error when presented with evidence about it. Let’s delve into how Gilovich, Vallone and Tversky showed the absence of a hot hand. Imagine a person who took ten shots in a basketball game. A ball is a hit, an X is a miss. What would count as evidence of a hot hand? What we can do is look at shots following a previous hit. For instance, in this sequence of shots there are 6 occasions where we have a shot following a previous hit. Five of those shots, such as the seventh here, are followed by another hit. We can then compare their normal shooting percentage with the proportion of shots they hit if the shot immediately before was a hit. If their hit rate after a hit is higher than their normal shot probability, then we might say they get a hot hand. This is effectively how Gilovich, Vallone and Tversky examined the hot hand in coming to their conclusion that it doesn’t exist. They also looked at whether there was a hit or miss after longer streaks of hits or misses, but this captures the basic methodology. It seems sensible. But let me take a detour that involves flipping a coin. Suppose you flip a coin three times. Here are the eight possible sequences of heads and tails. Each sequence has an equal probability of occurring. What if I asked you: if you were to flip a coin three times, and there is a heads followed by another flip in that sequence, what is the expected probability that another heads will follow that heads? Here is the proportion of heads following a previous flip of heads for each sequence. In the first row of the table, the first flip is a head. That first flip is followed by another head. After the second flip, a head, we also have a head. There is no flip after the third head. 100% of the heads in that sequence followed by another flip are followed by a head. In the second row of the table, 50% of the heads are followed by a head. In the last two rows, there are no heads followed by another flip. Now, back to our question: if you were to flip a coin three times, and there is a heads followed by another flip in that sequence, what is the expected probability that another heads will follow that heads? It turns out it is 42%, which I can get by averaging those proportions. 8 possible combinations of heads and tails across three flips Flips p(Ht+1|Ht) HHH 100% HHT 50% HTH 0% HTT 0% THH 100% THT 0% TTH – TTT – Exp.val 42% That doesn’t seem right. If we count across all the sequences, we see that there are 8 flips of heads that are followed by another flip. Of the subsequent flips, 4 are heads and 4 are tails, spot on the 50% you expect. What is going on in that second column? By looking at these short sequences, we are introducing a bias. The cases of heads following heads tend to cluster together, such as in the first sequence which has two cases of a heads following a heads. Yet the sequence THT, which has only one shot occurring after a heads, is equally likely to occur. The reason a tails appears more likely to follow a heads is because of this bias whereby the streaks tend to cluster together. The expected value I get when taking a series of three flips is 42%, when in fact the actual probability of a heads following a heads is 50%. As the sequence of flips gets longer, the size of the bias is reduced, although it is increased if we examine longer streaks, such as the probability of a heads after three previous heads. Why have I bothered with this counterintuitive story about coin flipping? Because this bias is present in the methodology of the papers that purportedly demonstrated that there was no hot hand in basketball. Because of this bias, the proportion of hits following a hit or sequence of hits is biased downwards. Like our calculation using coins, the expected proportion of hits following a hit in a sequence is lower than the actual probability of hitting a shot. Conversely the hot hand pushes the probability of hitting a shot after a previous hit up. Together, the downward bias and the hot hand roughly cancelled each other out, leading to the conclusion by researchers that each shot is independent of the last. The result is, that when you correct for the bias, you can see that there actually is a hot hand in basketball. When Miller and Sanjurjo crunched the numbers for one of the studies in the Gilovich and friends paper, they found that the probability of hitting a shot following a sequence of three previous hits is 13 percentage points higher than after a sequence of three misses. There truly is a hot hand. If Red Auerbach had coached as though there were no hot hand, what would his record have looked like? I should say, this point does not debunk the earlier point about people misperceiving randomness. The lab evidence is strong. People tend to see the hot hand when people flip coins. It is possible that people overestimate the strength of the hot hand in the wild, although that is hard to show. But the hot hand exists. Let’s turn back to one of the quotes I showed earlier. The story of our research on the hot hand is only partly about the misperception of random events. It is also about how tenaciously people cling to their beliefs even in the face of hostile evidence. The researchers expanded the original hot hand research from a story about people misperceiving randomness, to one of them continuing to do so even when presented with evidence that they were making an error. But, as we can now see, their belief in the hot hand was not an error. The punters in the stands were right. Their accumulated experience had given them the answer. The researchers were wrong. Rather than the researchers asking whether they themselves were making an error when people refused to believe their research, they double downed and identified a second failure of human reasoning. The blunt dismissal of people’s beliefs led behavioural scientists to hold an untrue belief for over thirty years This is a persistent characteristic of much applied behavioural science. It was an error I made many times when I first came to the discipline. We spend too little time questioning our understanding of the decisions or observations other people make. If we believe they are in error, we should first question whether the error is ours. Jason Collins on Nudgestock 2020 "],["discounting.html", "10 Discounting", " 10 Discounting Ecological Fallacy Solow (1974) like many other defenders of standard economics, resorts to an old paper of Harold Hotelling (1931) to convince us that neoclassical economists have not ignored the problem of intergenerational allocation. But he overlooks the important point that Hotelling’s analysis referred to some known amount of resources owned by an individual who discount future royalities. Of course, Hotelling was completely correct about the last point. Any individual must certainly discount the future for the indisputable reason that, being mortal, he stand a chance of dying any day. But a nation, let alone the whole of mankind, cannot behave on the idea that it might die tomorrow. They behave as if they were immortal and, hence, value future welfare situations without discounting. Georgscu-Roegen (1986) The Entropy Law and the Economic Process in Retrospect (pdf) "],["risking.html", "11 Risking 11.1 Uncertainty 11.2 Pooling Risk", " 11 Risking 11.1 Uncertainty Abstract Uncertainty is critical to questions about climate change policy. Recently developed recursive integrated assessment models have become the primary tool for studying and quantifying the policy implications of uncertainty. The first wave of recursive models has made valuable, pioneering efforts at analyzing disparate sources of uncertainty. We decompose the channels through which uncertainty affects policy and quantify them in a recursive extension of a benchmark integrated assessment model. We argue that frontier numerical methods will enable the next generation of recursive models to better capture the information structure of climate change and to thereby ask new types of questions about climate change policy Lemoine (2016) Uncertainty Recursive IAM Cubic damage triple the risk premium Traeger (ref.Gernot Wagner) 11.2 Pooling Risk We each put $100 a month in our individual piggy banks to cover potential medical costs one day. We all chip in $100 a month, for anyone who needs medical care this month. Same costs for everyone, totally different risks, totally different societies. Pooling risk reduces/eliminates volatility. Off course it also introduces adverse selection and moral hazard. (Peters/Pienar (Twitter)) "],["assumptions.html", "12 Assumptions", " 12 Assumptions The assumptions are a tall order, and sometimes one will hold but not the other. But that’s the world we live in. "],["common-good-economics.html", "13 Common Good Economics", " 13 Common Good Economics Mazzucato The concept of ‘good’ has its roots predominantly in welfare economics, where it captures the relative benefits of consumers and producers through individualistic utility definitions - the ‘good’ is an aggregated form of private interest. Economic theory has avoided providing a comprehensive definition of ‘good’ as a collective approach to the ‘what’ and ‘how’ of economic activity. Recognising this limitation, the notion of economic activity serving the common good and wellbeing has garnered considerable attention from scholars aiming to develop alternatives to gross domestic product (GDP) as the primary objective of economics (e.g. Felber and Hagelber 2017; Costanza et al. 2018; Coscieme et al. 2019; Dolderer et al. 2021). While this scholarship has contributed to important critiques of neoclassical economics, and formulated alternative organisational metrics at a micro-level (Felber et al. 2019) and accounting metrics at a macro- level (Stiglitz et al. 2009), less attention has been paid to the theoretical framework guiding the state’s role in governing the economy through collaborating with other actors for the common good. The paper addresses this question by building on, while distinguishing itself from, previous work on the public good (Samuelson 1954), the global public good (Kaul et al. 1999), and the commons and common-pool resources (Ostrom 1990). In particular, the concept of global public goods, while advancing the need for global cooperation in promoting wellbeing, remains conceptually largely attached to an understanding of the state as mitigating externalities that need fixing. While the literature on the commons and common-pool resources has successfully foregrounded and significantly advanced our understanding of the value of community involvement (Ostrom 2010; Saunders 2014), this focus has often been linked to an implicit assumption of insufficient or ineffective government activity, and positioned as a way of moving beyond the state/market dichotomy (Ostrom 2010; Sanderson et al. 2020). In other words, governments are seen as part of the problem, due to weak capacity or state capture. The view that states only fix when markets fail prevails. The public sector is understood to fill the gap created by markets, rather than setting ambitious objectives and promoting collective action towards achieving them The 17 UN Sustainable Development Goals (SDGs) are an important moment in which the need for ambitious governance and collective action – regionally, nationally and globally – towards ambitious goals becomes clear. The paper argues that the increasing focus on viewing the SDGs as collective challenges in need of a ‘common agenda’ (United Nations 2023a), requires a renewed focus on achieving objectives that are collectively considered ‘good’. The paper puts forth a framework of the common good as an objective, centring the ‘what’ and the ‘how’ as central questions that guide collective economic activity. A renewed conception of the common good, one that is nested in market-shaping and public value, may be a productive way of forming synergies between previous contributions, while moving beyond existing shortcomings and informing what is being considered an urgent moment for collective action. It draws valuable insight from political philosophy as highlighting the relational and mutual nature of the common good. The paper puts forth a framework where the ways in which actors work together towards collective goals are guided by five key principles: (1) purpose and directionality; (2) co-creation and participation; (3) collective learning and knowledge-sharing; (4) access for all and reward-sharing; and (5) transparency and accountability. Section 2 reviews how the common good has been discussed in political philosophy, with an emphasis on relational attributes. Section 3 reviews how ‘good’ has been framed in economics, tied to a theory of markets and market failures. This section compares public goods, global public goods, the commons and common-pool resources in order to distinguish previous approaches to economic goods from a renewed theory of the common good. Section 4 builds a new approach to the common good through five pillars. Section 5 concludes. Mazzucato (2023) Governing the economics of the common good (pdf) "],["cooperation.html", "14 Cooperation", " 14 Cooperation Handley Abstract A fundamental puzzle of human evolution is how we evolved to cooperate with genetically unrelated strangers in transient interactions. Group-level selection on culturally differentiated populations is one proposed explanation. We evaluate a central untested prediction of Cul- tural Group Selection theory, by assessing whether readiness to cooperate between indivi- duals from different groups corresponds to the degree of cultural similarity between those groups. We documented the normative beliefs and cooperative dispositions of 759 indivi- duals spanning nine clans nested within four pastoral ethnic groups of Kenya—the Turkana, Samburu, Rendille and Borana. We find that cooperation between groups is predicted by how culturally similar they are, suggesting that norms of cooperation in these societies have evolved under the influence of group-level selection on cultural variation. Such selection acting over human evolutionary history may explain why we cooperate readily with unrelated and unfamiliar individuals, and why humans’ unprecedented cooperative flexibility is never- theless culturally parochial. We conclude that group-level selection on cultural variation has likely left a mark on the human cooperative psychology and continues to influence which social norms and institutions prevail in human societies. Handley (2020) Human large-scale cooperation as a product of competition between cultural groups (pdf) 14.0.1 The benefits of Cooperation Peters (see also ecsb/ergodicity) This innocuous-looking gamble is a powerful tool, a window affording us a rather different view of economics, ecology, evolution, and complexity science. (Simulator in post) This coin toss is taunting us — it has that wonderful expected value, increasing exponenti ally, and yet when we play it, we’re bound to lose. Isn’t there some trick we can apply? S ome way of harvesting something of those great expectations, carrying over the promise fro m the statistical ensemble into the individual trajectory? The answer is yes — and that’s one reason why ergodicity economics has become such a hot t opic. There’s a very simple cooperation protocol which allows us to benefit from the coin toss. Here it is: find a partner, independently play one round each, then pool your wealth and split it evenly. Then play the next round independently. With the parameters of the gamble, pairing up in this way leads to a time-average growth r ate of the wealth of the cooperating pair of -0.2% per round, compared to -5% per round fo r the individual player — the cooperators outperform the non-cooperators exponentially and almost break even. With one extra cooperator applying the same protocol — play independen tly, pool wealth, share equally — the gambling gang moves into positive territory. The tim e-average growth rate of the cooperating triumvirate is +1.5% per round. In this simple ga me, Gibran is very literally — mathematically — right: the solitary entity decays, dies wi th certainty. A few entities who have learned to give, on the other hand, may live. The cooperating gamblers are not doing anything new, in a sense. They’re still just gambli ng, they haven’t developed any special skills, they can’t predict how the coin will land. All they’ve learned is to share, and just that allows them exponentially to outperform the ir non-cooperating peers (or former selves). We can keep growing the group, and in the limit of infinitely many cooperators, wealth grows at the growth rate of the expected value. By focusing on expected value, mainstream economics focuses on an object which grows as fast as the wealth of an infinite cooperative. The most skilled can still do better by joining a less skilled collective, and we see mathematically how important it is to maintain diversity and avoid loss of identity in a cooperating group. Athena Aktipis and her coworkers have studied attitudes and moral codes concerning cooperation in different societies, for instance among Maasai pastoralists in East Africa. Their work indicates that where survival is key, more generous systems of mutual aid emerge. Generosity may be thought of as a spectrum reaching from the individual coin toss, where no aid is ever received or provided, to cooperative coin tossing where “aid” is provided at every step whether it’s needed or not. In between lie different forms, where records may be kept to ensure future repayment of aid, or where the severity of need determines the degree of aid provided, with no expectation of repayment. The coin toss says: where unintended consequences can be avoided, the more sharing takes place the faster we will make progress. The optimal level of cooperation appears not as the minimum required to avoid disaster. It is instead the maximum we can get away with without triggering unintended consequences. Peters (2023) For to withhold is to perish "],["green-growth.html", "15 Green Growth 15.1 Decoupling 15.2 Rebound (Jevons Paradox) 15.3 Artefact? 15.4 The Green Growth Delusion 15.5 Measurement Issues 15.6 Cross-border Transition Risks", " 15 Green Growth 15.1 Decoupling Decoupling: the end of the correlation between increased economic production and decreased environmental quality. The needed decoupling does not occur! Not GLOBAL, not FAST-ENOUGH, not LONG-ENOUGH Vaden (abstract) The idea of decoupling “environmental bads” from “economic goods” has been proposed as a path towards sustainability by organizations such as the OECD and UN. Scientific consensus reports on environmental impacts (e.g., greenhouse gas emissions) and resource use give an indication of the kind of decoupling needed for ecological sustainability: global, absolute, fast-enough and long-enough. This goal gives grounds for a cate- gorisation of the different kinds of decoupling, with regard to their relevance. We conducted a survey of recent (1990–2019) research on decoupling on Web of Science and reviewed the results in the research according to the categorisation. The reviewed 179 articles contain evidence of absolute impact decoupling, especially between CO2 (and SOX) emissions and evidence on geographically limited (national level) cases of absolute decoupling of land and blue water use from GDP, but not of economy-wide resource decoupling, neither on national nor international scales. Evidence of the needed absolute global fast-enough decoupling is missing. Vaden 2020 Decoupling for sustainability (pdf) 15.2 Rebound (Jevons Paradox) Lange Literature on the rebound phenomenon has grown significantly over the last decade. However, the field is characterized by diverse and ambiguous definitions and by substantial discrepancies in empirical estimates and policy proposals. As a result, cumulative knowledge production is difficult. To address these issues, this article develops a novel typology. Based on a critical review of existing classifications, the typology introduces an important differentiation between the rebound mechanisms, which generate changes in energy consumption, and the rebound effects, which describe the size of such changes. Both rebound mechanisms and rebound effects can be analytically related to four economic levels – micro, meso, macro and global – and two time frames – short run and long run. The typology is populated with eighteen rebound mechanisms from the literature. This contribution is the first that transparently describes its criteria and methodology for developing a rebound typology and that gives clear definitions of all terms involved. The resulting rebound typology aims to establish common con­ ceptual ground for future research on the rebound phenomenon and for developing rebound mitigation policies. Lange (2021) Jevons Unravelled (pdf) 15.3 Artefact? Fix I investigate the hypothesis that the evidence for decoupling is a methodological artifact that arises from the use of monetary value to measure output. As demonstrated below, when the price of energy is used to deflate nominal GDP (rather than the GDP deflator), evidence for decoupling al- most entirely disappears. I hypothesize that monetary value, rather than represent the quantity of output, functions as a feedback device for controlling the flow of resources. Further investigation suggests that this feedback is not random; rather, it is fundamentally related to the biophysical labor productivity of the mining sector. I should be clear that my argument is not that statistical agencies have somehow made a ‘mistake’ in their calculation of output. To the contrary, I hypothesize that the notion of ‘output’ (and therefore, ‘decoupling’) is a conceptual artifact that re- sults from the misapplication of linear thinking to a non-linear system. If we think in biophysical terms, the economy is a complex, non-equilibrium system that uses biophysical flows to sustain itself. The only linear output of such a system is its waste. The economy has no output; rather, it has a resource throughput. Our mistake comes when we label certain internal pro- cesses as ‘output’: this gives the illusion of linearity where none actually exists. All of the outputs of the myriad of internal processes within the economy are destined to become inputs to other processes. Thus the internal workings of the economy are inherently circular, meaning the notion of a linear output is difficult to justify. The notion of ‘output’ (at the level of the entire economy) is a con- ceptual artifact that arises from the focus on monetary value. That is, we conflate a sale (a monetized exchange) with the creation of an output. By aggregating sales (and calling this output), we create the illusion that the economy is a linear process. If we drop the assumption that a sale represents an output, the illusion of linearity disappears: all internal processes become circular and the very notion of output (and hence, decoupling) becomes untenable. At the level of the entire econ- omy, the only linear flow is the stream of biophysical throughput, which ends in the output of waste. Rather than treat monetary value as an output, I offer the alternative hypothesis that monetary value functions as a feedback device for controlling the flow of bio- physical throughput. We can frame this paradigm shift by asking the following question: how does the economy ‘know’ to consume more resources? In the animal kingdom, the stimulus to consume resources comes from sensory feed- back: animals ‘know’ to consume resources because they ‘feel’ hungry. What is the corollary of this sensory feedback in the economic system? My hypothesis is that monetary value functions as such a feedback mechanism, stimulating or stifling the flow of resources. Prices constitute a feedback system that regulates the flow of resources through the economy. By thinking in this way, however, we place a heavy emphasis on the price of energy (the price of electricity in this case). Thus, we must ask – where does the price of energy come from? It is rather disconcerting to think that random market fluctuations might cause a change in the price of energy that somehow leads to a change in the entire economy’s ability to consume useful work. This would lead us straight back to the neoclassical view that the market is the ultimate arbiter of the economy. The task of biophysical economics should be to show that energy prices are, in fact, not random at all. Instead, they are a reflection of a broader biophysical reality. The nominal price of fossil fuel is a simple function of two variables: nominal GDP and the biophysical productivity of the mining sector. The evidence for decoupling almost completely disappears when nominal GDP is deflated by the price of electricity, rather than by the GDP deflator. This implies that evidence for decoupling is a methodological artifact – a result of the decision to measure output in terms of monetary value. The evidence presented here supports the alternative hypothesis that monetary value functions as a feedback device for controlling biophysical throughput. When moving from neoclassical theory to the real world, our ability to measure decoupling is undermined by serious (and I would argue, insurmountable) epistemological difficulties. The conventional measure of decoupling – the energy intensity of GDP – fails all three conditions for an effective efficiency metric. Thus, any evidence for decoupling that is provided by this metric should be met with appropriate scepticism. As such, I argue that the neoclassical notion of decoupling is untestable. Fix (2015) Biophysical Growth Theory (pdf) 15.4 The Green Growth Delusion Ketcham “Green Growth” is the idea that the organizing principle of our civilization — endless growth of economies and populations — can be decarbonized swiftly in a way that will involve no material disruption. In the annals of industrial civilization, the Green New Deal counts as one of the more ambitious projects. Its scale is vast, promising to reform every aspect of how we power our machines, light our homes and fuel our cars. At this late hour of ecological and climate crisis, the Green New Deal is also an act of desperation. The consensus on the need for scaling up renewable energy is rarely disturbed by a disquieting possibility: What if techno-industrial society as currently conceived — based on ever-increasing GDP, global trade and travel, and complex global production and distribution chains designed to satisfy the rich world’s unquenchable appetite for bigger, faster, more of everything — what if that simply cannot function without energy-dense fossil fuels? Ketcham (2023) The Green Growth Delusion 15.5 Measurement Issues Semieniuk Abstract Efforts to assess the possibilities for decoupling economic growth from resource use and negative environmental impacts have examined their historical relationship, with varying and inconclusive results. This paper shows that ambiguities in the historical measurement arising from definitional changes to GDP are sufficiently large to affect the results. I review the history of structural revisions to GDP using the example of the United States, and on international comparisons of purchasing power parity, and compare decoupling results using GDP vintages reported between 1994 and 2021 for most countries. Between vintages, 10–15% of countries switch between relative decoupling and recoupling from energy or materials on decadal intervals, and up to as many countries as decouple absolutely in an older vintage stop or newly start absolutely decoupling in the newer vintage. GDP vintages also affect environmental Kuznets curve results on absolute decoupling in Grossman and Krueger’s seminal paper and accelerate the International Energy Agency’s annual global decline in energy intensity by up to −0.2 percentage points. Inconsistencies in economic measurement introduce ambiguity into historical decoupling evidence and model projections into the future. To advance debate, rigorous reporting and sharing of data vintage for subsequent comparison and replication are urgently needed. Semieniuk Discussion Evidence for relative and absolute decoupling varies in an economically important way with GDP revisions over time. It follows that the entrenched debate about whether environmental Kuznets curves exist or not, and the extent to which a growing economy can limit its environmental impact, is marred by an ambiguity that hasn’t previously been acknowledged. Evidence from different sources can only be directly compared if both sources use the same GDP definition. If … Semieniuk Conclusion This paper has traced structural revisions in how GDP is accounted for and shown that these revisions impact measures of decoupling in both quantitatively and qualitatively important ways. Between vintages, 10–15% of countries switch between relative decoupling and recoupling from energy or materials on decadal intervals, and up to as many countries as decouple absolutely in an older vintage stop or newly start absolutely decoupling in the newer vintage. Some of the largest swings in decoupling … Semieniuk (2023) Inconsistent definitions of GDP: Implications for estimates of decoupling (paywall) 15.6 Cross-border Transition Risks Espargne Structural differences between countries in relation to the degree of fossil dependence, the penetration of renewables and their underlying supply-chains may have important cross-border macroeconomic implications. Changing patterns of trade in energy commodities (fuel, technologies, materials) may have substantial impacts, negative or positive, on the balance of payments of both exporting and importing countries, as well as on international financial flows. At the country level these changes can be large, even if opposite effects nearly cancel out at the global level. During the “mid-transition” period, when the economic transformation is most rapid, cross-border risks could generate or exacerbate instability in the economic, political, and financial spheres, which may become detrimental to the global transition process itself. We then define the “mid- transition” period as the time span during which low-carbon and fossil-based energy and industrial socio- technical regimes are both undergoing rapid transformation, co-exist on a large scale, and operate in a highly contested space, which causes new or exacerbates existing instabilities and volatility of global energy markets, with knock-on effects on the global economy. The existing literature on the macroeconomic effects of climate mitigation policies often simplifies nation-level challenges and policy contexts with the goal of seeking simple narratives that apply at the global level Developed as general basis scenarios of globally coordinated decision-making and cost-effective economic evolution, this literature sheds little light on the links between the transition and the structural transformation of the global economy. Global decarbonization will very likely change the structure of international trade and capital flows. Among the world’s largest economies, China, India and Japan are likely to benefit the most from the transition, while Russia, Saudi Arabia the U.S. could be negatively affected To empirically simulate the potential impacts of a decarbonization of the world economy on trade, we employ E3ME-FTT, a global macro-econometric model that integrates a range of social and environmental processes. E3ME is a widely used global macroeconometric model disaggregated over 70 regions and 43 industrial sectors. Its core is a demand-led input-output and econometric-driven accounting system. The demand for final goods and services is first estimated based on domestic and import prices and disposable household income. This drives the demand for intermediate products and investment goods (production capital) through the input-output framework and through investment equations. Disposable income is determined econometrically from employment and wages, while investment is determined based on the needs of industry to expand and the prices of capital goods. The allocation of finance is demand-driven, which implies that loans are created based on the creditworthiness of projects, where new investment ventures do not necessarily crowd out other investment elsewhere in the economy. The inconsistency between committed emissions from invested capital and carbon budgets suggests that an increasing share of the global fossil infrastructure is at risk of premature decommissioning. Second-round effects can occur through a liquidity channel if assets becoming stranded have previously been used as collateral for cross-border loans. Asset managers could be exposed to large transition-induced financial losses given that, since the GFC, their profitability has increasingly relied on “real(-economy) assets” exposed to transition risks in a scenario of global decarbonization, including energy, transportation and water supply infrastructure, farmland, and housing. Similarly, the decarbonization strategies of states that are “global owners” through their state-owned enterprises (SOEs) and/or sovereign wealth funds (SWFs) (e.g., China, Norway, Qatar, Russia, UAE) could have cross-border financial impacts, notably through changes in SWFs’ “carbon portfolios”. Uncoordinated and/or sudden divestment decisions could in turn generate second-round effects such as widespread carbon asset selloffs. A deterioration of the international investment position of fossil exporting countries could affect flows of petrodollars into international financial centers, notably the U.S. Without the substantial foreign inflows into U.S. government bonds, the 10-year Treasury yield could be 80 basis points higher. There are cross-border links between sovereign debt crises, extractivism, and transition risks. By acting as a price-insensitive buyer and supporting companies with low creditworthiness, major central banks change the structure of market prices, notably the debt of fossil companies. By bailing out companies that may not be creditworthy, major central banks send a signal to foreign investors and asset managers whereby high-carbon activities are a good bet, because profits are privatized while losses are socialized. In the short term the social losses of not bailing out unsustainable firms are greater than those associated with a bailout. Under those expectations, firms may engage in excessively correlated behavior, and excessive investment in fossil fuels. We estimate that the implied temperature rise (ITR) of purchases made under the Fed’s Secondary Market Asset Purchase Program (SMCCF, specifically its exchange-traded fund portfolio) and the ECB’s Corporate Sector Purchase Program (CSPP) during the COVID- 19 crisis stood at 3.1°C and 2.3°C, respectively. Espargne (2023) Cross-Border Risks of a Global Economy in Mid-Transition (pdf) "],["economics-of-demography.html", "16 Economics of Demography", " 16 Economics of Demography Beslik On November 15, 2022, the global population hit a significant milestone – 8 billion people. A 1% growth rate in 2022 means adding ten new cities, almost the size of New York, within a year. The ongoing expansion until at least 2060 raises concerns about its consequences for global climate mitigation and adaptation efforts. Recognizing that we’re entering an era of stabilized population growth and eventual decline adds a new dimension to the discussion about demography’s role in climate change. This becomes particularly important when juxtaposed with trajectories of energy consumption and economic growth still driven by expectations of exponential growth. Climate change and demographic change are two megatrends that are often discussed without regard for their complex interactions. Demography plays a pivotal role, getting influenced by and influencing economic, social, political, and environmental changes. The way populations shift has become a mega-trend shaping both global and local developments. Looking ahead to 2035, we can expect one out of four people in Europe to be aged 65 or older, a significant jump from one in thirteen back in 1950. The demographic challenge in Europe is like a ticking time bomb, as the EU Commission warns that the aging population and declining birth rates will hit a peak around 2026. The working-age population is anticipated to drop by a whopping 57.4 million by the end of the century. This is likely to worsen labor shortages and put more strain on public budgets. While the global population is aging overall, Europe, especially in the southern and eastern regions, stands out, facing record-low fertility rates and high emigration. IMF economist Philipp Engler predicts a shrink in gross domestic product per capita in advanced economies, with substantial reductions expected in the EU. The European Commission forecasts a 2.3 percentage point increase in spending on healthcare for older people and pensions by 2040, already accounting for 25 percent of the EU’s GDP. As population growth slows down and the peak approaches, the global population’s trajectory now resembles the “S” shape of a logistic function rather than the indefinite exponential growth envisioned by neo-Malthusians in the 1970s. The European workforce is on the brink of a rapid decline, posing a potential obstacle to Europe’s ambitious economic objectives. Shifts in population trends do indeed have multiple implications in the climate change context. However, their nature and actual impact are often misunderstood or oversimplified, a fact which tends to have population dynamics ignored both in intergovernmental climate change negotiations, as well as in the practice of adaptation to climate change. Beslik (2023) Demographic Dynamics and Climate Challenges "],["firm-and-hierarchy.html", "17 Firm and Hierarchy", " 17 Firm and Hierarchy Blair Fix The recent rise in US income inequality is being driven by a redistribution of income within firms. In short, I believe that corporate hierarchies have become more despotic. Corporate elites have taken income that once went to the bottom of the hierarchy and redirected it to the top. Blair Fix (2022) Firming Up Hierarchy "],["welfare.html", "18 Welfare", " 18 Welfare The welfare state is the result of a defeat of worker demands for control over production and planning. "],["corporations.html", "19 Corporations", " 19 Corporations Austin Having given greed freer rein, we have gradually super-sized the impulse via the creation of corporations – ‘corporate persons’. These larger-than-life figures roam the cultural landscape leaving us at Lilliputian scale to thread a careful path among them. Not only are they 1,000-fold – or 10,000- or 100,000-fold – larger than individual persons – the original ‘person’ concept – but they are legally bound to pursue self-interest in a way that we would never think of legally binding real individuals, and which no non-sociopath would ever accept. Indeed, we have effectively created gargantuan sociopaths and given them full protection of the law and increasing capacity to shape those laws. 78 Challenging these entities of our own creation is becoming difficult: exercise some moral leadership from within and you risk being denigrated as a ‘whistle-blower’. We have both super-sized and super-empowered self-interest, out of a fundamental belief that the market system can adequately harness the consequences. Austin (2021) Market-led Sustainability is a ‘Fix that Fails’… (pdf) Dessler Out-of-control corporate power is the real cause of climate change It started with Reagan, who articulated in his inaugural address that “Government is not the solution to the problem, government is the problem.” Reagan and his cronies then began diminishing the levers of power of the government. One of the most important things they did was relax antitrust enforcement and oversight of corporate mergers. With this new, lenient stance, the 1980s witnessed a surge in corporate mergers, putting us on the road to the creation of the all-powerful corporate behemoths we live with today. Along with other deregulation, the power of corporations has been accumulating ever since. It was turbocharged by the Supreme Court’s Citizens United decision, which increased political spending by corporations and the wealthy, shifting political power away from the average voter. This has laid bare the error in Reagan’s position. As the government ceases to regulate markets, the power does not descend to the individual. Rather, the power is grabbed by corporations. Today, a small number of corporations control the information we get about our elections, they control what we pay for lifesaving drugs, they control the food system, they control the housing market, and many other things. Their goal is to extract every penny from consumers while paying their workers as little as possible. You might say there’s nothing wrong with this! You’d rather have profit-minded corporations running our society rather than the government. The corporate world doesn’t care if the world ends in 10 years — they care about next quarter or, if they’re really a long-term thinker, a few years. This is how you get out-of-control climate change. If you plan to live more than a decade, or if you have kids, or you just have a soul, then you should care about the long-term future of the world and therefore want to control climate change. Sadly, corporations don’t. Dessler (2023) I stand with Unions "],["economic-growth.html", "20 Economic Growth 20.1 Grwoth as a positive feedback cycle 20.2 Undevelopment 20.3 Degrowth 20.4 Growth without Economic Growth 20.5 Ditching Economic Growth 20.6 Growth Waves", " 20 Economic Growth As the fossil-fuel era wanes, economic growth will become a relic of the past. Just like a bike that finds its balance with speed, the economy needs to grow in order to remain stable, growth acting as a promise that pacifies social conflicts and creates consent for certain kinds of politics. Since economic growth is both an idea, a social process, and a material process, an agenda for social change cannot only focus on changing GDP as an indicator, which would be akin to changing the dashboard of a car running full speed towards a cliff. Escaping from the growth paradigm requires to deconstruct growth as an idea, to problematise the role it plays in broader power dynamics, and to carefully understand its relation with nature. Quite a project indeed: going against growth means reinventing most of what we know about modern economies. Parrique (2023) The future is Degrowth 20.1 Grwoth as a positive feedback cycle Ayres The generic positive feedback cycle, in economics, operates as follows: cheaper resource inputs, due to discoveries, economies of scale and experience (or learning-by-doing) enable tangible goods and intangible services to be produced and delivered at ever lower cost. This is another way of saying that resource flows are productive, which is our point of departure. Lower cost, in competitive markets, translates into lower prices for all products and services. Thanks to non-zero price elasticity, lower prices encourage higher demand. Since demand for final goods and services necessarily corresponds to the sum of factor payments, most of which go back to labor as wages and salaries, it follows that wages of labor tend to increase as output rises. This, in turn, stimulates the further substitution of natural resources, especially fossil fuels, and mechanical power produced from resource inputs, for human (and animal) labor. This continuing substitution drives further increases in scale, experience, learning and still lower costs. Declining resource prices can have a direct impact on growth, via the positive feedback loop. In contrast to earlier treatments that introduced (commercial) energy(exergy), or energy(exergy) and materials separately, as factors of production, we consider physical work (or `exergy services’) as the appropriate independent variable for the production function. Ayres (2005) Accounting for Growth: The Role of Physical Work (pdf) 20.2 Undevelopment Smith Watching the experiences of the UK, Japan, and Italy raises the uncomfortable possibility that there’s such a thing as an “undeveloping country”. Standard economic growth theory suggests that once a country gets rich there’s no going back — getting poorer would require willful disinvestment or the forgetting of technology. But the world is more complicated than those simple models, and countries in the past have certainly seen their living standards go into long-term periods of secular decline. So it’s worth worrying whether the end stage of a some countries’ economic lives is not a permanent spot at the apex of development, but a long slow slide back into middle-income status. Smith (2022) Are the UK, Japan, and Italy “undeveloping countries”? 20.3 Degrowth Phillips Rather than viewing the market’s irrational production as the source of environmental challenges, the degrowth position views the source to be economic growth. Phillips (2022) The degrowth delusion Hickel The global economy is structured around growth — the idea that firms, industries and nations must increase production every year, regardless of whether it is needed. This dynamic is driving climate change and ecological breakdown. High-income economies, and the corporations and wealthy classes that dominate them, are mainly responsible for this problem and consume energy and materials at unsustainable rates1. Yet many industrialized countries are now struggling to grow their economies, given economic convulsions caused by the COVID-19 pandemic, Russia’s invasion of Ukraine, resource scarcities and stagnating productivity improvements. Governments face a difficult situation. Their attempts to stimulate growth clash with objectives to improve human well-being and reduce environmental damage. Researchers in ecological economics call for a different approach — degrowth. Wealthy economies should abandon growth of gross domestic product (GDP) as a goal, scale down destructive and unnecessary forms of production to reduce energy and material use, and focus economic activity around securing human needs and well-being. This approach, which has gained traction in recent years, can enable rapid decarbonization and stop ecological breakdown while improving social outcomes. It frees up energy and materials for low- and middle-income countries in which growth might still be needed for development. Degrowth is a purposeful strategy to stabilize economies and achieve social and ecological goals, unlike recession, which is chaotic and socially destabilizing and occurs when growth-dependent economies fail to grow. Reports this year by the Intergovernmental Panel on Climate Change (IPCC) and the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES) suggest that degrowth policies should be considered in the fight against climate breakdown and biodiversity loss, respectively. Policies to support such a strategy include the following. Reduce less-necessary production. This means scaling down destructive sectors such as fossil fuels, mass-produced meat and dairy, fast fashion, advertising, cars and aviation, including private jets. At the same time, there is a need to end the planned obsolescence of products, lengthen their lifespans and reduce the purchasing power of the rich. Improve public services. It is necessary to ensure universal access to high-quality health care, education, housing, transportation, Internet, renewable energy and nutritious food. Universal public services can deliver strong social outcomes without high levels of resource use. Introduce a green jobs guarantee. This would train and mobilize labour around urgent social and ecological objectives, such as installing renewables, insulating buildings, regenerating ecosystems and improving social care. A programme of this type would end unemployment and ensure a just transition out of jobs for workers in declining industries or ‘sunset sectors’, such as those contingent on fossil fuels. It could be paired with a universal income policy. Reduce working time. This could be achieved by lowering the retirement age, encouraging part-time working or adopting a four-day working week. These measures would lower carbon emissions and free people to engage in care and other welfare-improving activities. They would also stabilize employment as less-necessary production declines. Enable sustainable development. This requires cancelling unfair and unpayable debts of low- and middle-income countries, curbing unequal exchange in international trade and creating conditions for productive capacity to be reoriented towards achieving social objectives. Some countries, regions and cities have already introduced elements of these policies. Many European nations guarantee free health care and education; Vienna and Singapore are renowned for high-quality public housing; and nearly 100 cities worldwide offer free public transport. Job guarantee schemes have been used by many nations in the past, and experiments with basic incomes and shorter working hours are under way in Finland, Sweden and New Zealand. But implementing a more comprehensive strategy of degrowth — in a safe and just way — faces five key research challenges, as we outline here. Remove dependencies on growth Fund public services Manage working-time reductions Reshape provisioning systems Political feasibility and opposition Government action is crucial. This is a challenge, because those in power have ideologies rooted in mainstream neoclassical economics, and tend to have limited exposure to researchers who explore economics from other angles. Political space will be needed to debate and understand alternatives, and to develop policy responses. Strong social movements are necessary. Forms of decision-making that are decentralized, small-scale and direct, such as citizens’ assemblies, would help to highlight public views about more equitable economies Addressing the question of how to prosper without growth will require a massive mobilization of researchers in all disciplines. Hickel (2022) Degrowth can work — here’s how science can help 20.4 Growth without Economic Growth Key messages The ongoing ‘Great Acceleration’ [1] in loss of biodiversity, climate change, pollution and loss of natural capital is tightly coupled to economic activities and economic growth. Full decoupling of economic growth and resource consumption may not be possible. Doughnut economics, post-growth and degrowth are alternatives to mainstream conceptions of economic growth that offer valuable insights. The European Green Deal and other political initiatives for a sustainable future require not only technological change but also changes in consumption and social practices. Growth is culturally, politically and institutionally ingrained. Change requires us to address these barriers democratically. The various communities that live simply offer inspiration for social innovation. EEA 20.5 Ditching Economic Growth Russel Growth may be central to mainstream economics but nature has paid the price through pollution, waste and climate change. Some economists say it’s time for a completely different approach. It was only in the mid-20th century, in the wake of the shattering impact of World Wars and when capitalism and communism were competing for global dominance, that we began to measure the success of an economy in terms of gross national product, or GDP. The faster GDP was rising, the better an economy could be said to be performing. But something happens as all that economic activity expands. The amount of energy and resources we use also increase. Ever since the industrial revolution, fossil fuels have set us on a course of furiously expanding production, which has also meant more waste and more pollution. Historically, greenhouse gas emissions have risen alongside GDP. As economies have grown richer, nature has paid the price. And as the climate crisis has become ever-harder to ignore, more people are questioning whether infinite economic growth is possible on a planet of finite resources. Zero-emissions with twice the GDP “The Intergovernmental Panel on Climate Change in their Fifth Assessment, have 116 mitigation scenarios with a chance of staying below the 2 degree Celsius threshold. All of those scenarios assume 2-3% GDP growth rates,” says Jon Erickson, an ecological economist at the Gund Institute for Environment in Vermont, adding that this implies doubling the global economy by somewhere around 2050 . These scenarios rely not just on switching to renewables, but also on the large-scale extraction of massive volumes of carbon from the atmosphere using as-yet unproven technology, which Erickson describes as “wildly unrealistic.” “None of those models and the IPCC community even bother simulating a scenario where the global economy contracts, stabilizes and maybe even degrows,” Erickson says. “Yet that’s probably the one realistic scenario that would significantly affect greenhouse gas emissions.” It is easy to see why the idea that we must keep growing is hard to give up. When economic activity declines and we go into recession, people lose their jobs and are plunged into poverty. Yet those arguing for “degrowth” — a managed contraction of economic activity— say it doesn’t have to be this way. ime for a different approach? Federico Demaria, an economist at the Autonomous University of Barcelona, who has authored several books on degrowth, says that neoclassical economics — which has dominated economic discourse over recent decades, has “never looked at the question of how an economy could be managed without growth. It only looked at questions like, why do economies grow? If it’s not growing, how can we make it grow? Or, how can we make it grow even faster?” These have become pertinent questions even — or especially — for wealthy, industrialized economies, where growth has slowed over recent decades. “What the mainstream economists are doing is just trying to relaunch growth,” Demaria says. A different approach, which aims to rein in growth without inflicting the pain that recession has traditionally entailed, comes from the field of ecological economics. Embedding economics in ecology Neoclassical economic models picture economies as closed systems, with no inputs of materials or energy and no outputs of pollution and waste. But ecological economists insist there is no real separation between economy and ecology. After all, if we destroy the planet that feeds us, economic activity will collapse pretty quickly too. In an effort to fix this oversight, Demaria is among those devising new economic models that include factors like emissions and resources use. They are also working in things like social equality, debt, deficits and monetary systems, which have social impacts, and play into cycles of boom and bust. Degrowthers argue that we do have to tighten our belts — and it doesn’t have to be painful. If we could reverse the central logic of economic systems that prioritize growth over human and ecological wellbeing, they don’t believe we would miss the furious activity that’s keeping a minority of the human population in must-have products and ever-more material wealth. Russel (2020) Climate crisis: Is it time to ditch economic growth? 20.6 Growth Waves Schwartz The fifth Schumpeterian growth wave, which was built on information and communication technologies (ICT) as well as traditional and first-generation bio-engineered pharmaceuticals, has exhausted much of its growth impulse. A potential sixth wave built on artificial intelligence (AI), machine learning (ML), and second-generation biotech (CRISPR) as general-purpose technologies. Joseph Schumpeter’s analysis of dynamic change in capitalist economies—what he termed “creative destruction”— sheds light on the economic risks facing the core firms in these seven economies[Big dominant firm in small country]. Schumpeter argued that the central puzzle in economics was explaining the sources of dynamic growth. In an economy that actually embodied the starting assumptions of mainstream economics—small, competitive firms with no barriers to entry and no pricing power—profits would fall to the cost of capital plus some managerial wage for owner-operators. In essence, profits would only cover depreciation. Consequently, extensive and intensive growth would slow to the rate of population growth plus some gains from the normal productivity creep that incremental innovation produced for firms—in short, the kind of economic slump that prompted the secular stagnation debate of the 2010s. This circular economy, as Schumpeter called it, would never produce the periodic eruptions of rapid technological change and growth that he observed in the two centuries after the industrial revolution. As he put it, in these circumstances, you could add as many stagecoaches as you wanted to the economy but never get to a railroad. Dynamic growth required radical innovation in five key, interconnected areas: new modes of transportation, new energy sources, new consumer goods, new general purpose production technologies, and, though he underemphasized this, new legal organization or governance structures for firms. Writing on the eve of the Second World War, Schumpeter identified four such revolutions that had so far taken place. The first initiated the industrial revolution, centering on canals, water mills, textiles, and other household nondurables, with small owner-operated factories collecting handicraft producers under one roof. The second, in the mid-1800s, focused on steam power, railroads, and iron goods, as well as larger but still owner-operated factories using custom made machinery. The third, towards the end of the 1800s, emerged from electricity, steel steamships, urban trams, bicycles and chemicals, and saw the rise of large corporations which began to separate ownership and management. The fourth wave centered on internal combustion engines for land and air transport, petroleum, mass consumption of consumer durables like vehicles, continuous flow-assembly line production, and vertically integrated and often multidivisional firms with full separation of ownership and management. This wave began in the United States, most famously with Ford’s Model T, and spread to Europe and Japan. The transition periods between each of these growth spurts all saw increased domestic or international conflict and decreased investment in ageing growth sectors. Thus the transition from wave three to wave four saw the beginning of industrial unions and coincided with rising interimperial conflict that eventually produced World War I. The fifth wave began in the US in the 1960s with the development of the semiconductor chip, and soon spread globally. It is based on connectivity via electronics (ICT), negative energy consumption via digitalization, pharmaceuticals and first-generation biotechnologies, software and semiconductors, global supply chains, and de jure vertically disintegrated firms with de facto control by lead firms. This era’s iconic product, the smartphone, embodies the whole range of electronics products developed from the 1940s onward in a compact and relatively cheap form. There are many reasons to think this fifth wave is nearing exhaustion. Smartphone sales levelled off in 2018 and then declined somewhat, signalling replacement sales rather than growth. Despite the annual iPhone launch hype, most of the improvements to smartphones in recent years are largely marginal. Roughly 80 percent of the world’s population has 4G access—if they can pay for it. The entire global electronics industry is linked to personal computers and smartphones, which account for roughly half of global chip sales. Similarly, new pharmaceutical discovery levelled off in the 2000s, with most new drugs being copies or modifications of older drugs.2 Mark I and Mark II Innovation Creating the huge reticulation networks that made new energy and transport sources useful required equally huge investments. One mile of railroad was pointless, 100 miles was revolutionary. Schumpeter, in his earlier works, argued that only entrepreneurs hyping potential monopoly profits could induce bankers to finance these huge investments. He called this the Mark I model, in which small start-up firms run by visionaries upend existing incumbents. Many of the current software giants fit this pattern, but it also characterized the earliest days of Ford. Later, Schumpeter noted that high-profit corporations could channel their monopoly profits to dedicated internal research labs and generate the same kind of revolutionary innovation—his Mark II innovation model. Here, think of ATT’s Bell Labs, which invented the transistor. Software aside, we largely live in a world that combines both Mark I and Mark II innovation in a complex web that mostly favors larger firms. Typically, states promote radical innovation, often by funding basic research in university labs and their small firms spin-offs—classic Mark I innovation. But larger firms then typically provide those small firms with more funding to develop a commercializable product that their own Mark II R&amp;D teams will perfect. While these smaller firms often get the publication glory, the larger firms usually get the bulk of patents and thus profits. Schumpeter’s two pathways to radical innovation are more intermingled today than in the past, when vertically integrated firms were rather sealed off from both universities and small firms. These days, much Mark I innovation is often captured by the larger firms through acqui-hires, acquisition, or copycat innovation and litigation. Success in generating high-value exports and their associated profits permits these societies to exchange a small volume of high-value exports for a much larger volume of lower-value imports. The overlap of high profitability and profit share, export share and R&amp;D share, is not accidental. It indicates past competitiveness and near monopoly or dominant positions in world markets. Profits fund the R&amp;D that enables dominance and thus continued above average shares of global profits; those profits fund high levels of per capita income—among others, all those researcher jobs. And they fund, in some cases, extensive welfare states or at least state-education funding that generates the human capital those researchers possess, and which is the basis for past and potentially future dominance in high-tech sectors. The franchise economy The shift from the fourth (mass production) to the fifth (ICT) Schumpeterian wave involved changes in corporate strategy and structure that had significant knock-on effects. Chief among them, it boosted income inequality and increased the degree to which firms’ profitability depended on the legal regime around intellectual property (IPRs—patent, copyright, brand, and trademark). In the Fordist era, corporate strategy aimed at monopoly or oligopoly profits through control over large masses of physical capital arranged into continuous flow, assembly line systems. Profitability rested on running those systems at something close to their full capacity. This pushed firms to vertically integrate and negotiate peace with their typically unionized labor forces, which in turn reduced income inequality and funded internal research labs. But as more and more firms adopted this vertically integrated, unionized structure, profits began to decline. Workers revolted against the monotony and pace of assembly line production, and decolonization enabled raw-materials producers to push up prices, disrupting energy and metals markets. Put simply, once everyone adopted a Fordist product and production structure, the world ran out of cheap oil and docile semi-skilled assembly-line workers. Firms reacted to this militance by changing their corporate structure. They shrank their labor forces and opted to subcontract or offshore their low-wage, low-skill workforce. Similarly, they expelled physical assets—machines—used for producing undifferentiated goods into spin-off firms. At the same time, they began seeking more durable monopolies based on IPRs produced by a labor force high in human capital and supported by an army of subcontractors. This shift, which both coincided with and enabled the emergence of the fifth Schumpeterian wave, produced what I call a franchise structure. In the franchise economy, lead firms with lots of human capital, few actual employees, and substantial intellectual property portfolios outsource much of production to two other generic types of subordinated firms. Second layer firms are typically more capital-intensive firms with some barrier to entry for their production processes. Third layer firms are labor intensive firms producing undifferentiated goods and services. The lead firm orchestrates almost everything in its value chain without bearing any of the risk of holding that physical capital or dealing with masses of workers. While the shift to a franchise structure was good for firms with robust IPR portfolios and, by extension, for the high human-capital intensity of the workforce. It was less good for workers and firms producing undifferentiated goods and services. Downsizing meant shifting relatively well-paying jobs to low-wage countries, hollowing out the middle of the income distribution. The sixth Schumpeterian wave, should it indeed appear, poses serious risks for the largest, export-focused firms of our seven economies. Presently, a narrow set of IPR-based firms in the Mark II model does the forward-looking investment in R&amp;D that enables the transformation and scaling up of Mark I innovation required to catch that wave. It also generates both the jobs and the revenues needed to sustain a politically acceptable level of imports, employment, and growth in general. The potential inability of the big, highly profitable firms that anchor local research ecosystems to transition from their current production model to the novel production models emerging will have serious consequences. This risk extends beyond the “innovator’s dilemma.” Domestically, the loss of core manufacturing jobs in the second layer of the franchise economy has provoked populist backlashes. In both Israel and Sweden, this has empowered parties hostile to state-led industrial policy favoring highly paid knowledge workers. Growing geopolitical tension between the United States and China has prompted efforts to reshore or “near-shore” the ICT sector, particularly semiconductor production. All told, this probably tilts the global playing field towards firms from the larger and more geopolitically powerful countries. For the pawns of the global economy—smaller economies without national champions like Nokia or Samsung, and without oil-fund assets as in Norway—these challenges are even more pronounced. They enter this race with greater headwinds, weighed down by external debt, relatively untrained workers, and, in the worst cases, an over-reliance on unprocessed raw materials exports. Schwartz (2023) The Nokia Risk Schmelzer Economic growth as a policy goal, as well as the broader societal obsession with growth as we know it today, are relatively new developments that can be traced to attempts in the middle of the twentieth century to stabilize and plan capitalist economies through state intervention, to measure capitalist economies against state socialist ones, and to appease the increasingly militant working class. It was only through the new idea that ‘the economy’ could be measured through GDP that it became possible to justify the belief that growth is natural, necessary, good, and unlimited. We need to analyse economic growth as three interlinked processes that have evolved dynamically over time. First, growth is a relatively recent idea, the hegemony of which is the core ideology of capitalism, justifying the belief that growth is natural, necessary, and good, and that growth, as the increase of output and the development of productive forces, is linked to progress and emancipation. Second, growth is a social process that has long preceded the current hegemony of growth in contemporary society: a specific set of social relations resulting from and driving capitalist accumulation that stabilizes modern societies dynamically and at the same time makes them dependent on expansive dynamics of growth, intensification, and acceleration. Third, growth is a material process – the ever-expanding use of land, resources, and energy and the related build-up of physical stocks – which fundamentally transforms the planet and increasingly threatens to undermine the foundations of growth itself. Schmelzer (2023) The Future is Degrowth "],["macroeconomics.html", "21 Macroeconomics", " 21 Macroeconomics Caballero The root cause of the poor state of affairs in the field of macroeconomics lies in a fundamental tension in academic macroeconomics between the enormous complexity of its subject and the micro-theory-like precision to which we aspire. This tension is not new. The old institutional school concluded that the task was impossible and hence not worth formalizing in mathematical terms (for example, Samuels, 1987, and references therein). Narrative was the chosen tool, as no mathematical model could capture he richness of the world that is to be explained. However, this approach did not solve the conundrum; it merely postponed it. The modern core of macroeconomics swung the pendulum to the other extreme, and has specialized in quantitative mathematical formalizations of a precise but largely irrelevant world. One primary driving force behind modern macroeconomics (both core and periphery) was an attempt to circumvent the Lucas critique—the argument that market participants take the policy regime into account and so estimates of economic parameters for one policy regime may well not be valid if the policy regime changes. If we now replace some first-order conditions by empirical relationships and their distributions, doesn’t this critique return to haunt us? The answer must be “yes,” at least to some extent. But if we do not have true knowledge about the relationship and its source, then assuming the wrong specific first-order condition can also be a source of misguided policy prescription. Both the ad-hoc model and the particular structural model make unwarranted specific assumptions about agents’ adaptation to the new policy environment. The Lucas critique is clearly valid, but for many (most?) policy questions we haven’t yet found the solution—we only have the pretense of a solution. Ultimately, for policy prescriptions, it is important to assign different weights to those that follow from blocks over which we have true knowledge, and those that follow from very limited knowledge. Some of this has already been done in the asset pricing literature: for example, Ang, Dong, and Piazzesi (2007) use arbitrage theory to constrain an otherwise nonstructural econometric study of the yield curve and Taylor’s rule. Perhaps a similar route can be followed in macroeconomics to gauge the order of magnitude of some key effects and mechanisms, which can then be combined with periphery insights to generate back-of-the- envelope-type calculations. For now, we shouldn’t pretend that we know more than this, although this is no reason to give up hope. We have made enormous progress over the last few decades in the formalization of macroeconomics. We just got a little carried away with the beautiful structures that emerged from this process. The periphery of macroeconomics has much to offer in terms of specific insights and mechanisms, but to fulfill the ambition of the core we need to change the paradigm to go from these insights on the parts to the behavior of the whole. It is not about embedding these into some version of the canonical real business cycle model. It is, among other things, about capturing complex interactions and the confusion that they can generate. Caballero (2010) Macroeconomics after the Crisis: Time to Deal with the Pretense-of-Knowledge Syndrome Durlauf Durlauf (2004) Durlauf 2004 Complexity and Empirical Economics "],["spatial-economics.html", "22 Spatial Economics 22.1 Urban Economics 22.2 History of Urban Economics 22.3 Regional Economics", " 22 Spatial Economics Agglomoration Economics (Ongoing Research Programme) HSCIF 22.1 Urban Economics When and why did the expertise associated with economics as an academic discipline become so highly valued in the world of public policy? The embedding of agglomerationism within the thinking of policy-makers and governmental institutions provides a fascinating example of a broader shift towards the growing impact of economic expertise, and indeed of individual economists, on policy-making. This focus sits within a wider field of study which is interested in the complex roles that economists have at times played – as public intellectuals, policy experts and academic specialists. How different kinds of analytical tools and a particular style of economic reasoning made their way into the world of elite decision-making is a major theme of interest for many historians and social scientists. So too is the related question of how quantification (testable theoretical hypotheses, measurement technique and indicators, as well as decision-models) has over the last few decades gained ascendancy in policy circles. History of Urban Policy Expertise 22.1.1 Expertise ExpertiseunderPressure What is the role of experts in understanding social change? Expert judgment today is both intensely sought out, across private and public spheres, and also intensely criticised and derided with well-publicised failures to predict various high profile social and natural phenomena. Does the problem lie with the very idea that objective expertise about complex processes is attainable? Or does it stem from the way that expert judgment is developed and communicated? Or, perhaps it reflects the diminished standing of experts and expert knowledge in democratic and pluralistic societies? To explore these questions, we propose three case studies in which expert judgment is both consequential and controversial. They are the UK Government’s emergency response, the use of agglomeration theory in city planning, and deep philosophical controversies about the possibility and objectivity of social science. These cases differ in scope and focus but they enable us to analyse four distinct features of legitimate expertise: sensitivity to temporal scale, translatability in space, ambivalence about precision, and moral responsibility. The overarching goal of the project is to establish a broad framework for understanding what makes expertise authoritative, when experts overreach, and what realistic demands communities should place on experts. CRASSH Expertise under Pressure Programme 22.1.2 Trusting Science Bennett Trust is necessary for many kinds of policy, particularly where that policy requires citizens to comply with rules that come at significant cost, and coercion alone would be ineffective. What is distinctive about our pandemic policies is that they depend not just on public trust in policy, but public trust in the science that we are told informs that policy. When public policy claims to follow the science, citizens are asked not just to believe what they are told by experts, but to follow expert recommendations. While ministers defer to scientists, those same scientists have been eager to point out that their role is exclusively advisory. We are still being asked by the government to trust in recommendations provided by experts, even if the government is not being led by evidence in the way it would have us believe. The communications strategy may not be honest. Public trust in science is both a necessary and desirable feature of an effective public health response to the pandemic. But it is desirable only insofar as it is well placed trust. What makes trust in experts reasonable, when it is? A perceived threat to knowledge about a range of basic facts that most of us don’t have the resources to check for ourselves. If an expert tells me that something is the case this is enough reason for me to believe it too, provided that I have good reason to think that the expert in question has good reason to believe what they tell me. Is it still reasonable to trust science when it doesn’t just provide policy-relevant facts, but leads the policy itself? Knowledge regarding the relevant facts might not reliably indicate ability to reason well about what to do in light of the facts. Well-placed trust in the recommendation of an expert is more demanding than well-placed trust in their factual testimony. A good reason for an expert to think I should do something is not necessarily a good reason for me to do it. This is because what I value and what the expert values can diverge without either of us being in any way mistaken about the facts of our situation. One helpful measure to show the public that a policy does align with their interest is what is something called expressive overdetermination: investing policy with multiple meanings such that it can be accepted from diverse political perspectives. Reform to French abortion law is sometimes cited as an example of this. After decades of disagreement, France adopted a law that made abortion permissible provided the individual has been granted an unreviewable certification of personal emergency. This new policy was sufficiently polyvalent to be acceptable to the most important parties to the debate; A second helpful measure, which complements expressive overdetermination, is to recruit spokespersons that are identifiable to diverse groups as similar to them in political outlook. This is sometimes called identity vouching. The strategy is to convince citizens that the relevant scientific advice, and the policy that follows that advice, is likely not to be a threat to their interests because that same consensus is accepted by those with similar values. Expressive overdetermination and identity vouching are ways of showing the public that a policy is in their interests. Whether they really are successful at building public trust in policy, and more specifically in science-led policy, is a question that needs an empirical answer. What I have tried to show here is that we have good theoretical reasons to think that such additional measures are needed when we are asking the public not just to believe what scientists tell us is the case, but to comply with policy that is led by the best science. Public trust in science comes in at least two very different forms: believing expert testimony, and following expert recommendations. Efforts to build trust in experts would do well to be sensitive to this difference. [Bennett - Trusting the experts take more than belief(Blog Post)]https://hscif.org/trusting-the-experts-takes-more-than-belief/) 22.2 History of Urban Economics Cherrier and Rebours The field of ‘Urban Economics’ is an elusive object. That economic phenomena related to the city might need a distinctive form of analysis was something economists hardly thought about until the early 1960s. In the United States, it took a few simultaneous scholarly articles, a series of urban riots, and the attention of the largest American philanthropies to make this one of the hottest topics in economics. The hype about it was, however, short-lived enough so­­­ that, by the 1980s, urban economics was considered a small, ‘peripheral’ field. It was only through the absorption into a new framework to analyze the location of economic activities – the ‘New Economics Geography’ – in the 1990s that it regained prominence. Understanding the development of urban economics as a field, or last least the variant which originated in the US and later became international, presents a tricky task. This is because the institutional markers of an academic field are difficult to grasp. A joint society with real estate economists was established in 1964, and a standalone one in 2006; a journal was founded in 1974, with an inaugural editorial which stated that: “Urban economics is a diffuse subject, with more ambiguous boundaries than most specialties. Situated within a master-discipline (economics) that is often described as exhibiting an articulated identity, clear boundaries with other sciences and strict hierarchies, urban economics is an outlier. There is, however, one stable and distinctive object that has been associated with the term ‘urban economics’ throughout the 1970s, the 1980s, the 2000s and the 2010s: the Alonso-Muth-Mills model (AMM). It represents a monocentric city where households make trade-offs between land, goods and services, and the commuting costs needed to access the workplace. The price of land decreases with distance from the city center. The model was articulated almost simultaneously in William Alonso’s dissertation, published in 1964, a 1967 article by Edwin B. Mills, and a book by John Muth published in 1969. This trilogy is often considered as a “founding act” of urban economics. Agglomeration In 1956, William Alonso moved from Harvard, where he had completed architecture and urban planning degrees at the University of Pennsylvania. He became Walter Isard’s first graduate student in the newly founded department of “regional science.” He applied a model of agricultural land use developed 150 years earlier by the German economist Johann Von Thünen to a city where all employment is located in a Central Business District. His goal was to understand how the residential land market worked and could be improved. His resulting PhD, Location and Land Use, was completed in 1960. Around that time, young Chicago housing economist Richard Muth spent a snowstorm lockdown thinking about how markets determine land values. The resulting model he developed was expanded to study population density. And a book based on it was published a decade later: Cities and Housing. Drafts of Alonso and Muth’s work reached inventory specialist Edwin Mills in 1966, while he was working at the RAND corporation, and trying to turn models describing growth paths over time into a model explaining distance from an urban center. His “Aggregative Model of Resource Allocation in a Metropolitan Area” was published the next year. This new set of models immediately drew attention from a wide array of transportation economists, engineers and geographers concerned with explaining the size and transformation of cities, why citizens chose to live in centers or suburbs, and how to develop an efficient transportation system. The economists included Raymond Vernon and Edgar Hoover, whose study of New York became the Anatomy of the Metropolis; RAND analyst Ira Lowry, who developed a famous spatial interaction model; spatial and transportation econometrician Martin Beckman, based at Brown; and Harvard’s John Kain, who was then working on his spatial mismatch hypothesis and a simulation approach to model polycentric workplaces. Through the early works of Brian Berry and David Harvey, quantitative urban geographers also engaged with these new urban land use models. But the development of a new generation of models relying on optimization behavior to explain urban location was by no mean sufficient to engender a separate field of economics. Neither Alonso, who saw himself as contributing to an interdisciplinary regional science, nor Muth, involved in Chicago housing policy debates, cared much about its institutionalization. But both were influenced and funded by men who did. Muth acknowledged the influence of Lowdon Wingo, who had authored a land use model. Together with Harvey Perloff, a professor of social sciences at the University of Chicago, they convinced the Washington-based think-thank Resource for the Future to establish a “Committee for Urban Economics” with the help of a grant by the Ford Foundation. The decision was fueled by urbanization and dissatisfaction with the urban renewal programs implemented in the 1950s. Their goal was to “develop a common analytical framework” through the establishment of graduate programs in urban economics. Their agenda was soon boosted by the publication of Jane Jacobs’ The Death and Life of Great American Cities, and by growing policy interest in the problems of congestion, pollution, housing segregation and ghettoization, labor discrimination, slums, crime and local government bankruptcy, and by the stream of housing and transportation acts which were passed in response to these. The Watts riots, followed by the McCone and Kerner commissions, acted as an important catalyst. The Ford Foundation poured more than $ 20 millions into urban chairs, programs and institutes through urban grants awarded to Columbia, Chicago, Harvard and MIT in 1967 and 1970. The first round of funds emphasized “the development of an analytical framework”, and the second sought “a direction for effective action.” As a consequence of this massive investment, virtually every well-known US economist turned to urban topics. At MIT, for instance, Ford’s money was used to set up a two-year “urban policy seminar,” which was attended by more than half of the department.The organizer was welfare theorist Jerome Rothenberg, who had just published a book on the evaluation of urban renewal policies. He was developing a large-scale econometric model of the Boston area with Robert Engle and John Harris, and putting together a reader with his radical colleague Matt Edel. Department chair Carry Brown and Peter Diamond were working on municipal finance. Robert Hall was studying public assistance while Paul Joskow examined urban fire and property insurance. Robert Solow developed a theoretical model of urban congestion, published in a 1972 special issue of the Swedish Journal of Economics, alongside a model by taxation theorist Jim Mirrlees investigating the effect of commuter and housing state tax on land use. Solow’s former student Avinash Dixit published an article modeling a tradeoff between city center economies of scale and commuting congestion costs in another special issue on urban economics in the Bell Journal the next year. A survey of the field was also published in the Journal of Economic Literature, just before the foundation of the Journal of Urban Economics in 1974. Segregation But the publication of a dedicated journal, and growing awareness of the “New Urban Economics” was not the beginning of a breakthrough. It turned out to be the peak of this wave. On the demand side, the growing policy interest and financial support that had fueled this new body of work receded after the election of Richard Nixon and the reorientation of federal policies. On the supply side, the mix of questions, methods and conversations with neighboring scholars that had hitherto characterized urban economics was becoming an impediment. More generally, the 1970s was a period of consolidation for the economics profession. To be considered as bona fide parts of the discipline, applied fields needed to reshape themselves around a theoretical core, usually a few general equilibrium micro-founded workhorse models. Others resisted, but could rely on separate funding streams and policy networks (development and agricultural). Urban economics was stuck. Policy and business interest was directed toward topics like housing, public choice and transportation. And, combined with the growing availability of new microdata, micro-econometrics advances, and the subsequent spread of the personal computer, this resulted in an outpouring of applied research. Computable transportation models and real estate forecasting models were especially fashionable. On the other hand, a theoretical unification was not in sight. Workhorse models of the price of amenities, the demand for housing, or suburban transportation, were proposed by Sherwin Rosen, William Wheaton and Michelle White, among others. But explanations of the size, number, structure and growth of cities were now becoming contested. J. Vernon Henderson developed a general equilibrium theory of urban systems based on the trade-off between external economies and diseconomies of city size, but in these agglomeration effects did not rely on individual behavior. Isard’s former student Masahita Fujita proposed a unified theory of urban land use and city size that combined externalities and the monopolistic competition framework pioneered by Dixit and Joseph Stiglitz, but without making his framework dynamic or relaxing the monocentric hypothesis. At a point when there was growing interest in the phenomenon of business districts – or Edge cities as journalist Joël Garreau called them, this was considered a shortcoming by many economists. General equilibrium modelling was rejected by other contributors, including by figures like Harry Richardson, and a set of radical economists moving closer to urban geographers (such as David Harvey, Doreen Massey and Allen Scott) working with neo-Marxist ideas. Renewal In the 1990s, various trends aimed at explaining the number, size, evolution of cities matured and were confronted to one another. In work which he framed as contributing to the new field of “economic geography,” Krugman aimed to employ his core-periphery model to sustain a unified explanation for the agglomeration of economic activity in space. At Chicago, those economists who had spent most of the 1980s modeling how different types of externalities and increasing returns could help explain growth – among them Robert Lucas, José Scheikman and his student Ed Glaeser – increasingly reflected on Jane Jacob’s claim that cities exist because of the spillover of ideas across industries which they facilitate. Some of them found empirical support for her claim than for the kind within-industry knowledge spillovers Henderson was advocating. Krugman soon worked with Fujita to build a model with labour mobility, trade-offs between economies of scale at the plant level and transportation costs to cities. Their new framework he was adamant to compare to Henderson’s general equilibrium model of systems of cities. He claimed that their framework enabled the derivation of agglomeration from individual behavior and could explain not only city size and structure, but also location. In his review of Krugman and Fujita’s 1999 book with Venables, Glaeser praised the unification of urban, regional and international economics around the microfoundations of agglomeration theory. He also contrasted Krugman’s emphasis upon transportation costs – which were then declining – with other frameworks focusing on people’s own movement, and began to sketch out the research program focused on idea exchanges that he would develop in the next decades. He also insisted on the importance of working out empirically testable hypotheses. The “New Economic Geography” was carried by a newly-minted John Bates Clark medalist who had, from the outset, promised to lift regional, spatial and urban economics from their “peripheral” status through parsimonious, micro-founded, tractable and flexible models. It attracted a new generation of international scholars, for some of whom working on cities was a special case of contributing to spatial economics. In the process, however, olders ties with geographers were severed, and questions that were closely associated with changing cities, like the emergence of the digital age, congestion, inequalities in housing, segregation, the rise of crime and urban riots, became less central to the identity of this field. The field lost some sort of autonomy. Most recently, Glaeser’s insistence that urban models need to be judged by their empirical fit may be again transforming the identity of urban economics. The shift is already visible in the latest volume of the series of Handbooks in Urban and Regional Science. Its editors (Gilles Duranton, Henderson and William Strange) explain that, while its previous volume (2004) was heavily focused on agglomeration theory, this one is “a return to more traditional urban topics.” And the field is now characterised not in terms of a unified, theorical framework, but with reference to a shared empirical epistemology about how to develop causal inferences from spatial data. Overall, the successive shifts in urban economists’ identity and autonomy which we describe here, were sometimes prompted by external pressures (urban crises and policy responses) and sometimes from internal epistemological shifts about what counts as “good economic science.” A key development in the 1970s was the unification around general equilibrium, micro-founded models. It is widely held that the profession is currently experiencing an “applied turn” or a “credibility revolution”, centered on the establishment of causal inference (gold) standards. How this will affect urban economics remains unclear. Cherrier and Rebours 22.2.1 Jane Jacobs Considering her contribution to economic theory may seem counter-intuitive. In addition to lacking academic credentials, she took little interest in engaging the discipline of economics. Her models were neither formal nor developed in reference to existing models. And her view of economic theory in general was dismissive. In the opening chapter of Cities and The Wealth of Nations, “Fool’s paradise,” Jacobs lays out a history of economic thought and arrives at this sweeping conclusion: “Choosing among the existing schools of thought is bootless. We are on our own.” The same dismissive stance extended to academic institutions, as she refused numerous honorary degrees from various Universities. Jacobs Externalities Some economists picked up on her insights. A type of economic externality has been derived from her detailed historical accounts of new economic activities arising from urban diversity. Chicago and Harvard urban economists Glaeser, Kallal, Scheinkman, and Shleifer credited Jacobs in 1992 for identifying cross-industry knowledge transfers, which they dubbed “Jacobs externalities.” The concept was based on Jacobs’ The Economy of Cities and posits that knowledge transfer occur between different industries, and that local competition supports economic growth. This came four years after future Nobel prize recipient Robert Lucas pointed to Jacobs’ work while investigating the external effects of human capital in his 1988 article On the Mechanics of Economic Development, although without formalizing his insight. Lucas’ endorsement earned Jacobs increasing recognition among economists over the following decades. Paul Krugman described her as a “patron saint of the new growth theory” and her unusual status was summed up by Robert Dimand and Robert Koehn who saw her as “her own distinctive kind of political economist … an exceptional instance of a woman without academic affiliation or university training achieving recognition among leading academic economists”. And a considerable literature grew up after Glaeser et al.’s piece. Despite this interest in her work, extended reassessments of her contribution to economic thought have yet to appear. The city economy model, first developed in The Economy of Cities,argues that the desirable diversification of local economic activities depends largely on the destination of goods and services entering the city’s economy. The key claim is that imports are key to economic development: they embody knowledge and allow further diversifications in the local economy, as imports are gradually replaced by local supply, and make “room” for new imports – in a similar manner to import substitution. Jacobs uses this model to stress the long-term undesirability of overspecialization derived from a focus on maximizing exports, and the importance of a large and diverse local economy – ultimately delivering a critique of comparative advantages as an organizing principle of trade. The more niches that are filled in a given natural ecology, other things being equal, the more efficiently it uses the energy it has at its disposal … That is another way of saying that economies producing diversely and amply for their own people and producers, as well as for others, are better off than specialized economies … The most elaborate study of Jacobs’ use of biological and ecological analogies is provided in mathematician and philosopher David Ellerman’s paper How Do We Grow? Jane Jacobs on Diversification and Specialization (2005). Depicting the city economy’s boundaries as an open system governed by evolutionary dynamics: “development is a conceptualized form of social learning.” Incoming goods, the products of foreign know-how, are vectors of developmental learning. And exports of commodities and services fund these imports. When imports feed into the somewhat enclaved export economy (i.e. overspecialized), they have a lesser effect then when they are dissipated in local consumption. Following Geoffrey Hodgson’s taxonomy in Economics and Evolution (1993), part of Jacobs’ system could be characterized as phylogenetic and non-consummatory, that is, as exhibiting an open-ended process of evolutionary selection among a population of firms and individuals. Jacobs targeted development schemes developed by the World Bank. She pointed to the inherent weaknesses of Robert McNamara’s development strategies for addressing “basic human needs” (literacy, nutrition, reduction in infant mortality, and health) of poor populations. She argued that because economic development is a process, it cannot be thought of as a “collection of things” which can be bought or provided. The “basic human needs approach” ignored the necessity for solvent markets to support increased agricultural yields and the populations that were being displaced. As they could no longer rely on agricultural work to sustain themselves, displaced workers failed to find jobs in nearby city economies, where labor markets had not evolved alongside the increased agricultural yields through a succession of appropriate feedback mechanisms triggering the needed corrections. And she made the same argument against technology transfers in the “Green Revolution” of the 1960s and 1970s. The mechanism of feedback relationships is one example among others of Jacobs’ usage of systemic concepts to draw boundaries around the city economy as a system and elaborate on its behavior. Further examination of Jacobs’ use of these concepts within the paradigm she adopted may reveal a consistent link between her analysis of cities as economic units and the policies she is tended to critique. In short, future attempts at more comprehensive interpretations of Jacobs’ economic thought might benefit from stepping away from the urban focus of The Death and Life of Great American Cities while considering more carefully her later economic writings. Divry on Jacobs 22.3 Regional Economics Rebours The history of regional science offers an interesting case study, as well as a one of the few examples, of the institutionalization of an entirely new scientific field in the years after 1945. Its foundation by Walter Isard and a group of social scientists in the 1950s represents the most institutionalized attempt to stimulate the relationship between economics and geography. The original project of Isard, who was trained as an economist at Harvard, was to promote the study of location and regional problems. And at the outset, regional science was, in various ways, a success. It attracted many scholars from different disciplines, mostly economics, geography and urban/regional planning, and it quickly became institutionalized formally through the foundation of the Regional Science Association (RSA) in 1954 and establishment of a Regional Science Department at the University of Pennsylvania in 1958. At the same time, the creation of the Papers and Proceedings of The Regional Science Association in 1955 and of the Journal of Regional Science in 1958, offered new publication venues for scholars interested in location analysis, in particular quantitative geographers who found it difficult to publish in traditional geography journals. Within economics, regional science influenced analytical works in urban economics, as, for instance, William Alonso’s thesis, widely recognized as one of the foundational works of urban economics, was written at Penn under the supervision of Isard in 1960. However, the prevailing processes of knowledge production and evaluation which shaped the emergence of this new field were deeply influenced by economics. Geographers became dissatisfied with Isard’s vision of the hierarchical division between geographers and economists, and the primacy given to economic theorizing and modelling as the core of the new regional science. Thus, the social organization of the field of regional science and its interactions with other disciplines mirrored the particularity of economics, a hierarchical discipline organized around a strong theoretical core and an insularity from the rest of social sciences. In the late 1940s, Isard became increasingly concerned about the lack of interest among economists in the location of economic activities. His perception of the subject was not really different to his colleagues, but he wanted to improve the theory they used, which, following the British tradition of the late 19th century, suffered from a lack of spatial dimension. He did not seek to challenge the general equilibrium economic theory that was becoming dominant, but sought instead to integrate a spatial aspect within it. In 1949 Isard was recruited to Harvard by Wassily Leontief to develop an input-output approach to regional development. During the war, input-output analysis received much attention because it enabled the American Air Force to identify the best targets for bombing. As a consequence, Leontief had received large research funds to develop his input-output framework. Isard expressed a hierarchical division between economists, who provided the analytical foundations of regional science, and the geographers, who provided the empirical facts and testing. While, the identity of economics was legitimated and reinforced by its success during the war, in geography, there was an increasing dissatisfaction with the regional geography approach that dominated the field in the1950s. The Cold War context facilitated the promotion of a new generation of quantitative geographers looking for more scientific methods. By the mid-1970s, regional science experienced a progressive decline when geographers started to distance themselves from the analytical methods that were promoted by Isard. But even after the Regional Science Department at Penn closed its doors in 1993, regional science journals remained a going concern and continued to promote studies of spatial issues notably from urban economics and, after 1991, New Economic Geography. Rebours "],["functional-finance.html", "23 Functional Finance", " 23 Functional Finance Functional finance is a heterodox macroeconomic theory developed by Abba Lerner during World War II that seeks to eliminate economic insecurity (i.e., the business cycle) through government intervention in the economy. Functional finance emphasizes the result of interventionist policies on the economy. It actively promotes government deficit spending as an effective way of reducing unemployment. Functional finance is based on three major beliefs: It is the role of government to stave off inflation and unemployment by controlling consumer spending through the raising and lowering of taxes. The purpose of government borrowing and lending is to control interest rates, investment levels, and inflation. The government should print, hoard or destroy money as it sees fit to achieve these goals. Functional finance also says that the sole purpose of taxation is to control consumer spending because the government can pay its expenses and debts by printing money. Furthermore, Lerner’s theory does not believe it is necessary for governments to balance their budgets. Lerner was a follower of the extremely influential economist John Maynard Keynes and helped to develop and popularize some of his ideas. Keynesian economics embraced the concept that optimal economic performance could be achieved by using economic intervention policies by the government to influence Investopedia Levy Publications Tankus on Krugman vs MMT "],["macro-finance.html", "24 Macro-Finance 24.1 Institutional Supercycles 24.2 Central Banking", " 24 Macro-Finance The ultimate driver of government financing costs is the central bank. 24.1 Institutional Supercycles Dafermos Supercycle We build upon the Minskyan concepts of ‘thwarting mechanisms’ and ‘supercycles’ to develop a framework for the analysis of the dynamic evolutionary interactions between macrofinancial, institutional and political processes. Thwarting mechanisms are institutional structures that aim to stabilise the macrofinancial system. The effectiveness of such structures changes over time, creating a secular cyclical pattern in capitalism: the supercycle. We develop a macrofinancial stability index and identify two supercycles in the post-war period, which we label the industrial and financial globalisation supercycle respectively. For each, we apply a four-phase classification system, based on the effectiveness of institutions, customs and political structures for stabilising the macrofinancial system. The supercycles framework can be used to explain and anticipate macroeconomic, financial and thus political developments, and moves beyond conventional approaches in which such developments are treated as exogenous shocks. Framework Despite the bidirectional and dynamic nature of the interactions between institutions and macrofinancial processes, analysis is often partial and static. In the political economy literature, institutional change is linked with exogenous macroeconomic or financial shocks, such as shifts in inflation or the policy interest rate (Iversen and Soskice, 2012; Gabor and Ban, 2013). Conversely, macrofinancial developments are explained as arising from exogenous institutional change, such as alterations to financial regulation or labour market legislation. A framework in which institutional change and macrofinancial processes are dynamically interlinked is still missing. In this paper, we develop an evolutionary framework that connects macrofinancial processes and institutional change. The foundations of this framework lie with two largely overlooked concepts in Minsky’s analysis of financial capitalism (Palley, 2011). The first is that of ‘thwarting mechanisms.’ This concept draws on Minsky’s insight that, although capitalism is inherently unstable, this instability rarely becomes explosive because of the existence of ‘customs, institutions or policy interventions’ that tame destabilising forces (Ferri and Minsky, 1992, p. 84). Thwarting mechanisms counteract the inherent instability of capitalism, allowing for long periods of high economic activity and social and financial stability. However, the effectiveness of thwarting mechanisms varies over time, eventually diminishing as a result of the profit-seeking actions of economic agents and the generation of new sources of long-run instability. This endogenous erosion gives rise to crises, which, in turn, lead to the development of new thwarting mechanisms. The rise and fall of thwarting mechanisms generates secular cycles in macrofinancial stability. This ‘supercycle’ is the second concept we borrow from Minsky and Palley. Thwarting Mechanisms Minsky’s concept of thwarting mechanisms: ‘customs, institutions or policy interventions that make observed values of macroeconomic variables different from what they would have been if each economic agent pursued “only his own gain”’. Thwarting mechanisms reduce the amplitude of basic cycles, constraining instability by imposing ceilings and floors on the dynamic path of the economic system. Floor mechanisms aim to ensure a minimum level of aggregate demand, thereby placing a floor under the level of economic activity. These mechanisms may be the result of deliberate policy interventions, (e.g. activist fiscal policy), or a side effect of other developments (e.g. expansion of household debt to maintain consumption spending). Ceiling mechanisms aim to impose upper limits on the economic expansion by restricting activities that may enhance growth but also generate instability. Examples of ceiling mechanisms include inflation targeting, financial regulation aimed at reducing procyclicality and leverage, and capital controls to restrict speculative financial inflows. The supercycle is a long-run institutional and political cycle over which the effectiveness of a particular configuration of thwarting mechanisms first increases and then declines. The configuration of thwarting mechanisms shapes the supercycle, hardwiring powerful macroeconomic ideas into policy regimes. Macrofinancial stability is primarily driven by the effectiveness of thwarting mechanisms. Four Phases Four phases of the supercycle: expansion, maturity, crisis and genesis. During the expansion phase, newly introduced thwarting mechanisms are effective, leading to economic expansion and broad social and financial stability: economic and financial activity is disrupted by the recessions of the basic cycles, but thwarting mechanisms prevent a systemic crisis. Economic agents learn how to adapt to the new institutional environment, however, innovating to preserve or increase their profits and thereby reducing the effectiveness of thwarting mechanisms. Further, mechanisms introduced to reduce one source of instability may over time create others, potentially as a result of interaction with other thwarting mechanisms. Once the effectiveness of thwarting mechanisms starts to decline, the cycle enters the maturity phase, during which economic expansion continues but the macrofinancial stability of the system is diminishing. The declining effectiveness of thwarting mechanisms ultimately leads to crisis, because the institutional framework is no longer sufficient to constrain the dynamics of the basic cycle. At this point, a basic-cycle recession leads to deep economic, political and social instability, and institutional restructuring. While government intervention may stabilise the economy, broad-based recovery is impossible because existing thwarting mechanisms are ineffective: the institutional structure is can no longer ensure macrofinancial stability. The ensuing genesis phase sees attempts to establish a new configuration of thwarting mechanisms, attempts shaped by political struggles. When – or if – effective new mechanisms are introduced, the next supercycle begins. In the case that – for political, social or technological reasons – such mechanisms cannot be introduced, the crisis phase will be prolonged, likely accompanied by political and social turmoil. Institutional Change Institutions, understood as ‘rules of the game’ provide mechanisms to facilitate market exchange in the presence of transactions costs that prevent an optimal frictionless equilibrium. Causation is largely unidirectional: given the presence of transactions costs, exogenously imposed ‘good’ institutions – usually understood as the rule of law, secure property rights, well-developed financial markets etc. – produce good economic outcomes. Changes in institutional structure, understood as formal ‘rules of the game’ result from ongoing optimisation by market participants over the costs of reconfiguration relative to the benefits, given incomplete information sets, technology, and firm-specific knowledge. Recent work goes beyond this micro-based analysis to examine the possibility of emergent properties in complex evolutionary systems, using agent-based modelling techniques. Thwarting mechanisms can be viewed as constraining the macrofinancial instability that arises from the emergent properties of such complex systems. Institutional structure itself, understood as the configuration of thwarting mechanisms, emerges and evolves as a result of the profit-seeking behaviour of agents. The agent-driven erosion of thwarting mechanisms can give rise to macrofinancial instability. The distribution of power influences the design of thwarting mechanisms; it does not merely affect macroeconomic targets and the associated policy design. Policy makers need to establish mechanisms that keep a range of key macroeconomic variables within certain bounds, irrespective of the primary macroeconomic target; otherwise, macroeconomic instability would undermine political stability. The way that thwarting mechanisms are eroded is specific to each supercycle: the strength of labour in the 1970s undermined the wage-price consensus, giving rise to inflationary pressure, while the strength of finance in the 2000s placed limits on the effectiveness of mechanisms to constrain financial instability. Macrofinancial Stability Index (MSI) The MSI is constructed using a number of ‘floor’, ‘ceiling’ and ‘corridor’ macroeconomic and financial variables. The MSI is calculated as one minus an average of the normalised distances of floor, ceiling and corridor variables from their maximum, minimum and average values respectively (over the period under investigation. The MSI thus takes values between 0 (minimum stability) and 1 (maximum stability). High-income countries experienced common secular cyclical movements in their macrofinancial stability in the post-World War II period. The ideological shift on macroeconomic management at the end of the 1970s brought independent central banks oriented to inflation targeting and fiscal deficits financed on sovereign debt markets. Mass privatisation reduced the state’s economic footprint, while previous gains on employment protection and unemployment benefits were substantially rolled back. Growth increasingly relied on rapid expansion of leverage and increasing financial activity. The financial sector, in turn, found that new institutional structures were required to enable leverage to expand beyond traditional constraints. During the expansionary phase of the FG supercycle, shadow banking expanded significantly, absorbing the flow of assets resulting from the continued expansion of credit. Securitisation and the originate-to-distribute model allowed banks to transform illiquid assets, mortgage loans in particular, into marketable securities. These securities were financed with short-term liabilities such as repos and asset-backed commercial paper (ABCP). Growth became increasingly reliant on collateral-based financial activity. Collateral plays a central role in funding neorentier balance sheets. Neorentiers issue short-term (often overnight) repo deposits secured by tradable collateral. For lenders such as institutional cash pools or money market funds, collateral makes repos a better liquidity management vehicle than unsecured bank deposits. Repo borrowing allows a wide range of institutions to access money market funding, while rising asset prices lead to increasing leverage capacity because repo collateral is marked to market. The use of collateral functionally, and imperfectly, replaces direct sovereign guarantees on short-term liquid assets. The rise of collateral-based finance fundamentally changed the relationships between central banks and governments. In the 1990s, central banks in high-income countries collectively sanctioned neorentiers’ turn to shadow deposits by liberalising repo markets, often to enable Ministries of Finance to develop liquid government bond markets. States turned to neorentiers in the age of independent central banks and capital market financing of budget deficits, introducing reforms in sovereign bond markets designed according to neorentier preferences: regular auctions facilitated by primary dealers and deregulated repo markets The promise of liquidity for sovereign bonds entrenches the ‘infrastructural power’ of finance: neorentiers promise liquidity to Ministries of Finance, and well-functioning monetary transmission mechanisms to central banks, improving their ability to oppose policy innovations or tighter regulatory measures. The rising power of neorentiers thus serves to discipline states, curbing fiscal and regulatory thwarting mechanisms: market financing of fiscal deficits privileges neorentiers as mediators between the monetary and the fiscal arms of the state, and creates conflicting objectives for the central bank and the Treasury. Easy credit conditions allowed sustained expansion of private debt, enabling aggregate demand to keep up with productive capacity in the face of weak income growth and government retrenchment. Credit-financed consumption took over from capital investment as the driver of growth. While crisis-era innovations succeeded in preventing financial system collapse and depression, growth has not returned. In our framework, this less due to ‘secular stagnation’ than to the institutional architecture of the FG supercycle – weak and ‘flexible’ labour, high inequality and government retrenchment. Without a change in this architecture – without a new set of thwarting mechanisms – it is difficult to identify a likely source of sustained demand growth other than a return to credit expansion. Overall, while institutional changes improved the effectiveness of stabilising mechanisms in the period prior to the coronavirus pandemic, the continuous push for asset-based welfare reinforced the structural drivers of neorentier capitalism without delivering a new engine of growth. When the coronavirus crisis struck, a new configuration of thwarting mechanisms that could foster economic expansion alongside financial stability had not yet emerged. The thwarting mechanisms of the next supercycle will be, at least in part, the result of the rapid institutional change that has taken place as a result of this crisis, and of greater awareness of the potential for future pandemics. Inevitably, the next supercycle will also be conditioned by the even greater crisis of climate change. Green Supercycle What is most urgently required, in light of the COVID-19 and climate crises, is a detailed understanding of the current genesis phase and the prospects for the emergence of a new set of thwarting mechanisms that would underpin a green supercycle. Dafermos Gabor (2020) Institutional Supercycles (pdf) 24.2 Central Banking Braun The impact of international economic integration on social protection is conditional on the monetary regime. The role of the European Central Bank (ECB) as the supranational enforcer of the economic logic of integration since monetary union. While Polanyi conceptualized central banking as an institution of non-market coordination that evolved to protect the domestic economy from gold standard pressures, the ECB has acted as an enforcer of disembedding “euro standard” pressures vis-à-vis national labor market and welfare state institutions. Despite lacking the mandate or the authority to override national legislation, the ECB, strategically pursuing its organizational and systemic interests, pushed for structural reforms via discursive advocacy and conditionality. Our results show that Europe’s prospects for Polanyian non-market coordination are determined by Frankfurt as much as by Luxembourg and Brussels. The death of ‘Social Europe’ The European Commission’s slogan of “a Europe that protects”, introduced in 2019, subtly diverges from the Treaty of Rome’s commitment to “proper social protection.” This is no accident. The euro area debt crisis accelerated labor market deregulation and welfare state retrenchment, and the idea of a “Social Europe” has been declared “dead.” At the same time, and particularly among those most affected by by these developments, protectionist and nationalist sentiments have been on the rise. Brussels watchers have read “a Europe that protects” as a bellwether of a new, non-liberal politics of protection. The European Union (EU) as a unique case combining high levels of protection with full “globalization in the strict sense of the word”, namely unrestricted competition for capital, goods and services. The strictures of the euro considerably amplified the economic logic of integration relative to the legal and political logics expressed through the ECJ and the Commission, respectively. With the introduction of the euro in 1999, this economic logic of integration found its institutional expression in the European Central Bank (ECB). Since then, the relationship between economic integration and social protection has been shaped in Frankfurt as much as in Brussels and Luxembourg. Structural Reforms The ECB defined structural reforms, in strikingly Polanyian terms, as policies that “change the fabric of an economy, the institutional and regulatory framework in which businesses and people operate.” This advocacy constitutes a puzzle: The ECB lacks both a mandate and the legal means to shape labor market and social policies at the member-state level. Pushing to “change the fabric” of societies therefore entails significant reputational risks. Why, then, did the ECB chose to push for structural reforms? Our explanatory framework places the emphasis on the ECB’s organizational (credibility and legitimacy) and systemic (survival of the euro) interests. In pursuing those interests, the ECB strategically adjusted the method and content of its structural reform advocacy to fit the economic and political context. In the wake of the euro area debt crisis, the ECB acquired the power—shared with the Commission and the International Monetary Fund (IMF)—to impose and enforce policy conditionality. Polanyi Our analysis, while drawing on Polanyi, fills an important gap in Polanyian thinking on the political economy of central banking. According to Polanyi, national central bank- ing evolved as an expression of the countermovement to the commodification of money under the international gold standard. Whereas Polanyi said little about potential con- flicts between non-market coordination in the domain of money (central banks) and social protection in the domain of labor (social policies and trade unions), this conflict subse- quently moved to the very center of macroeconomic governance. A large literature has since studied the interaction between national central banks and national labor market policies and wage-setting actors. The institutional setting of this interaction changed dramatically with EMU, which established a supranational monetary regime with its own supranational central bank. From the beginning, heterogeneous labor market in- stitutions and social policies threatened divergent national inflation developments, which clashed with the ECB’s one-size-fits-all monetary policy. Whereas Polanyi would have expected a central bank to protect national economies from the disembedding pressures of the monetary regime, the ECB has instead embodied these very pressures, acting as a—if not the—key planner of laissez-faire in national labor markets. Looking beyond Europe, our analysis contributes to the literature on policy diffusion in the context of economic globalization. Here, national policymakers routinely encounter the problems of translating and enforcing perceived functional pressures emanating from the international level. The IMF, guided by the “Washington Consensus,” made its emergency lending conditional on gov- ernments’ implementing specific structural reforms, playing the role of both translator and enforcer. Central banks, as the ultimate repositories of “epistemic authority” on economic matters, are uniquely positioned to play a similar role at the domestic level. In the euro area, the role of translator and—to a lesser but significant extent—enforcer of perceived functional pressures was assumed by the ECB ECB identified—and sought to counter via structural reforms and public-sector wage restraint—the diverging trend in unit labor costs as early as 2005, years before the European Commission. The ECB has been a highly articulate proponent of specific structural reforms in national labor markets and social policy regimes. When unit labor cost divergence, first recognized and prioritized by Trichet, threatened the very effectiveness of supranational monetary policy, the ECB began to pro- mote structural reforms as means of macroeconomic adjustment, both in public speeches and behind the scenes with national policymakers. Executive Board members urged gov- ernments to seek downward wage adjustments, both via structural labor market reforms and by imposing wage restraint on the public sector. When the euro-area debt crisis hit, the ground for its interpretation as a crisis of competitiveness divergence had already been prepared by the ECB. When circumstances added formal and informal conditionality to the ECB’s toolkit, it wielded those instruments to help enforce labor market liberalization, internal devaluation, and public sector wage cuts. It was only when deflationary pressures and criticism in the European Parliament and elsewhere threatened its legitimacy that the ECB abandoned its advocacy of structural reforms. Despite lacking both a mandate and the legal means to directly override national regulations, the ECB has been a keen supranational advocate of market-enhancing integration in the field of labor market and social policy. This analysis also sheds new light on the broader political economy of central banking. Polanyi and others have shown that national central banking evolved under the interna- tional gold standard to buffer the disruptive adjustment pressures on national economies. The supranational ECB provided such protection for the financial system, but not for labor. Instead, emulating the role the IMF in other parts of the world, the ECB trans- lated—and subsequently helped to enforce—the perceived functional pressures of interna- tional monetary and financial integration. Whether the ECB is constitutionally wedded to the role of “prime mover in the move to a market society” remains to be seen. 135 Its recent shift from structural reform advocacy to calls for wage increases has been echoed in the US, where the Federal Reserve has signaled that it will prioritize employment and wage growth over consumer and asset price stabilization. Central banks may yet again become “active agents of the countermovement.” Braun (2021) Planning Laissez-faire: Supranational Central Banking (pdf) 24.2.1 ECB - Implosion? Gabor Under the financial capitalism supercycle of the past decades, inflation-targeting central banks have been outposts of (financial) capital in the state, guardians of a distributional status-quo that destroyed workers’ collective power while building safety nets for shadow banking. The limits of this institutional arrangement that concentrates (pricing) power and profit in (a few) corporate hands are now plain to see. ….. What if Zugzwang is that last stage of a central banking paradigm, when it implodes under the contradictions of its class politics? In turning European states into a collateral factory for private finance, the founding fathers did not consider the financial stability implications for the ECB. The eurozone’s macro-financial architecture is wired to amplify volatility in sovereign spreads to the German Bund, via the €9tn repo market. This wholesale money market provides the plumbing for private credit creation, both on bank balance sheets and through securities markets. It was designed — by the ECB and the European Commission — to mainly rely on eurozone sovereign bonds as repo collateral. Yet we know from the eurozone sovereign debt crisis that repo collateral valuation means cyclical market liquidity in eurozone sovereigns except Germany, threatening liquidity spirals that only the ECB can prevent. Liquidity spirals, it is worth remembering, or not just bad for eurozone governments, but also for private institutions that use those bonds as collateral. Putin’s invasion of Ukraine, coupled with the reluctance of European governments to act decisively with energy price caps, have left the ECB as a convenient scapegoat. Scapegoating invariably turns dovish central bankers into hawks, particularly when their peers elsewhere act as obedient vassals to the dollar hegemon. Indeed, monetary historians will marvel at that brief period when European politicians believed so much in the euro’s potential to unseat the US dollar … With that illusion behind us and the euro below parity, the ECB is just another central bank trapped in the global dollar financial cycle, prey to facile comparisons with other central bank interest rates. “If the climate and geopolitical (shocks, sic) of 2022 are omens of Isabel Schnabel’s Great Volatility that most central banks and pundits expect for the near future, then macro-financial stability requires new framework for co-ordination between central banks and Treasuries that can support a state more willing to, and capable of, disciplining capital.” Such a (new) framework would threaten the privileged position that central banks have had in the macro-financial architecture and in our macroeconomic models. The history of central banking teaches us that policy paradigms die when they cannot offer a useful framework for stabilising macroeconomic conditions, but never at the hands of central bankers themselves. Gabor by Tooze (2022) Zugzwang in Central Banking "],["externalities.html", "25 Externalities 25.1 Commons 25.2 History of Economics’ ‘Externalities’ 25.3 Ecosystem Services 25.4 Environmental Degradation 25.5 Energy and Transport", " 25 Externalities 25.1 Commons Resource extraction and pollution of the commons power the beating heart of global economic prosperity. 25.1.1 Hardin and Ostrom Nijhuis The features of successful systems, Ostrom and her colleagues found, include clear boundaries (the ‘community’ doing the managing must be well-defined); reliable monitoring of the shared resource; a reasonable balance of costs and benefits for participants; a predictable process for the fast and fair resolution of conflicts; an escalating series of punishments for cheaters; and good relationships between the community and other layers of authority, from household heads to international institutions. Like Hardin, many conservationists assume that humans can only be destructive, not constructive, and that meaningful conservation can be achieved only through total privatisation or total government control. In southern Africa in the 1980s, some conservationists recognised that parks and reserves, many created by colonial governments, had divided subsistence hunters and farmers from much of the wildlife that had long sustained them – and which, in some cases, they’d managed as a commons for generations. The resulting lack of local support meant that even the best-patrolled park boundaries were vulnerable to incursions by human neighbours, people unlikely to tolerate – much less protect – the large, sometimes troublesome species that ranged beyond even the largest reserves. In 1987, when the South African conservationist Garth Owen-Smith attended a conference on community-based conservation in Zimbabwe, a comment by Harry Chabwela, the director of Zambia’s national parks, left a lasting impression. ‘At this conference we have talked a lot about giving local people this and giving them that, but what has been forgotten is that they also want power,’ Chabwela said. ‘They want a say over the resources that affect their lives. That is more important than money.’ In 1996, the Namibian National Assembly passed a law that allowed groups of people living on communal land to establish institutions called conservancies. Conservancies would be governed by elected committees, and all members would share the benefits of any tourism or commercial hunting within conservancy boundaries. Nijhuis (2021) The miracle of the commons 25.2 History of Economics’ ‘Externalities’ Duncan Austin Incomplete Markets Externalities were generally ignored through most of the 20 Century. After Pigou had identified the problem in the 1920s, there followed a long barren period for “welfare economics”, the natural home for this type of thinking. This lasted until the early 1970s when there were the first stirrings of renewed interest by serious economists. Framed as “externalities”, market failures could be more easily dismissed. The term encourages a perception of unpriced damages as being mere residuals to the centrepiece of a priced economy. Since Pigou, some have sought to “beef up” the terminology. K. William Kapp, for example, bluntly described the market mechanism, in toto, as a “cost-shifting” institution. In this framing, externalities are not a bug, but a feature. The mathematization of economics – another marker of the discipline’s scientific aspiration – exacerbated the situation. The desire for manageable equations and functioning models further pushed troublesome market imperfections away. Possibly, there was the sense that positive and negative externalities might roughly cancel each other out, leaving GDP incomplete but still reliable enough as a directional indicator. That rests on the assumption that positive and negative externalities are symmetrical in nature. However, there is an important asymmetry. Positive externalities take the form of “free goodies”, whereas certain negative externalities constitute systemic risks that may be catastrophic to “trip” or breach. While you generally cannot have too much of a positive externality – a “free good thing” – too much of certain unwanted harms may induce systemic failure. Externalities exist because markets have an incomplete grasp of what humans value. Markets work off prices and not everything has – or can have – a price. As such, marketed values – or prices – exist amidst a broader “value field” of things that humans care about and which have an influence on our wellbeing. Pigou’s proposition was an inconvenient truth for economics. It suggested that there are real limits to what conventional economics might say about matters of human value and, hence, to how far markets might serve human wellbeing. The inconvenience of his idea may be why Pigou is not better known – seemingly more tolerated, than celebrated Complete markets…? As a discipline, economics did the very human thing of trying to ignore a difficult proposition. By not confronting Pigou’s awkward challenge, the door was opened for a line of theorizing th that led in exactly the opposite direction. Economists for most of the 20 Century sought to establish economics as a comprehensive corpus of thought with universal application. Hence, by the 1950s, a very appealing theory of complete markets had been developed. No externalities in this theory, none at all. Complete market theory is the laying down of a conceptual blanket over all our preferences that leaves no space for externalities. The formulation of complete markets theory was deemed a major milestone for economics. Its authors were Kenneth Arrow and Gérard Debreu. It provided the cornerstone for the discipline’s claim for the superiority of markets as a mechanism for social coordination. Rather, the key mistake made by 20 Century economics was not in misunderstanding externalities, but in grossly underestimating their magnitude and so foreclosing a debateon the innate limits of economic thinking. The discipline considered that markets were “complete enough” to safely proceed as if they were actually complete! We are now waking up to the consequences of that misjudgement. … Or very incomplete markets? Consider, for example, a recent study by Robert Costanza and colleagues. They estimated the monetary value of the “services” provided free by the Earth’s ecosystem at $125 trillion in 2 2011, nearly twice the value of global GDP (gross domestic product) at the time. The authors believe this to be a conservative estimate because it grasps only about half of the “services” we know ecosystems provide. Other studies have contemplated the value of unmonetized social systems, including one estimate that unpaid housework in the UK in 2016 was about 65 percent of GDP – another 3 huge block of value not captured by the market. Just combining this figure with the Costanza et al. figure suggests that measured GDP captures about a third of some larger conception of value. From its very inception, GDP has been derided as an incomplete measure of wellbeing. However, in elevating GDP to its current perch of influence, the working assumption has been that GDP, and the market system it reflects, captures the lion’s share of what matters. What the latest estimates of “externalities” and non-market values suggest – and what our sustainability crisis seems to underscore – is that our perception of GDP’s reach may be horribly off. Such an estimate suggests that it is not that the market does not capture all things of value, it does not even capture most things of value. Far from externalities being peripheral, they may be the main event! The failure of economics to fully incorporate externalities in its 20th-century theorizing now appears to be the dropped stitch that defines the whole discipline. For a long time, this was a tolerable neglect as markets were more robustly counterbalanced by pre- market institutions that upheld unpriced values, and as the environment was able to absorb the fewer demands of a smaller, less consumptive population. But, with the onset of climate and biodiversity emergencies, the context has changed considerably. It matters more and more that we might not have slightly incomplete markets, but very incomplete markets. It has left us at the start of the 21 Century transforming the matter and energy of the world using economic and financial tools that have only a very limited grasp of the reality they fashion. In a world of very incomplete markets, things of human value lie in two separate realms – the marketed domain and the non-marketed domain. Some of the growth of the marketed economy genuinely arises from human ingenuity and creativity unlocking better ideas and products from new combinations of inputs. This is “good” growth, which ought to be celebrated and encouraged. However, other parts of monetized “growth” arise from simply running down the stocks of what is valuable but in the non-marketed realm. This is the illusion of wealth creation based on registering the increase in marketed value, but not recording the decrease in unmarketed values. In contrast to growth from genuine ingenuity, this is robbing Peter to pay Paul. Measured economic “growth” overall combines in unknown proportions a “creative growth”, which we want to encourage, and a “parasitic growth”, which we do not. At an aggregate level, it is almost impossible to trace the origins – creative or parasitic – of GDP growth, and very few official metrics make any attempt to do so. Our working assumption is that all economic growth is good – as it indeed would be if we had complete markets eliminating the possibility of parasitic growth. However, in not knowing the real-world mix between creative and parasitic growth, do we want more GDP growth, or less? It is not clear. And, given that companies work to the same price register as GDP, do we want companies to beat profit expectations or would it be better if they missed them? Who really knows? The conventional argument – captured by the notion of an Environmental Kuznets Curve – is that it is only by increasing monetary wealth that we can develop better technology to protect the environment. However, it is not clear in the aggregate whether the deployment of such new capabilities ever makes good the damage done by the initial enabling wealth creation. While anecdotes can be summoned to support the idea – electric cars, wind turbines, LEDs etc – thus far, at the global level that matters, data shows we remain in net ecological destruction mode. Markets within Cultures Paradoxically, then, to use markets more than we are, to introduce more externality pricing, would require a new cultural level reassertion that markets are a tool within culture. We need not a sustainable economy, but a sustainable culture that has an economy. Such a culture would establish room for governments to introduce new markets which powerful market ncumbents may not like, but which improve human wellbeing. In turn, such a culture would also invigorate non-market means to protect our environment, for we must remember that not everything of human value can be priced and “internalized”. The interesting question, worth a moment’s reflection, is: why is that? Commodifiable Externalities Though Pigou identified commodifiable externalities, there are many things of human value that cannot withstand the disembedding from their context necessary for them to be commodified and, hence, be transactable via market exchange. Such values are non- transactable because they are irrevocably embedded either in specific things – they are unique – or in specific relations – they exist “between” certain things. Some examples: friendship, reputation, loyalty, integrity, trust, community, mental health, etc. If you believe you have purchased any of these items, you might want to check the label. What is tricky is that most things in the world bear both separable transactable values and intrinsic non-transactable values. A tree has both separable value as a feedstock for furniture and paper and intrinsic value as part of the ecosystem in which it is relationally embedded. We tend to value trees in managed plantations for their separable values, but we value General Sherman, the 26-story-tall giant sequoia that is the largest known tree on Earth, for its non-separable attribute of being uniquely the tree we call General Sherman. With General Sherman, we have chosen to perceive and value its uniqueness over its instrumental value. Indeed, we might say that General Sherman is price-less. The “economist” denies the validity of this perspective by arguing that everything has a price. To say that something is priceless is merely to say that nobody has yet offered a high enough price. In turn, the “ecologist” denies the “economist’s” perspective, arguing that while you can apply such economic thinking to General Sherman, it is the wrong sort of thinking to apply. Both the “economic” transactional perspective and the “ecological” intrinsic perspective are beneficial and valid, but they are incompatible. The decision to apply an economic perspective to the external world is always a value judgment that necessarily transcends economics. More, it is a value judgment that can never be justified or refuted on economic grounds precisely because it is an argument about the validity of applying an economic perspective. All this is a discussion that the field of economics may well have taken more seriously 100 years ago, had it been more open to the significance and implications of Pigou’s formulation of externalities. Alas, we are now having to unknit to pick up this dropped stitch in a world now confronting large-scale problems of missed externalities. Incompletness Theorem of Economics Economics might be well served by formalizing an incompleteness theorem that would act as a proverbial knot-in-a-handkerchief reminder about the limits of claims that economics can make. It is an oddity of human intellectual thought that the most logical of our sciences, mathematics, had a formal Incompleteness Theorem as early as 1930, while economics formalized a complete market theory in the 1950s and seemingly still has no definitive statement of incompleteness. Ringfecing Economics One of the ways, then, that we could better protect ecological values is for economics to recognize – re-cognize – the wisdom of culturally ring-fencing where economic thinking is preferred. In other words, to recognize the non-monetizable value of non-economic thinking. Designating areas as protected are to explicitly restrain the ever-eager economic perspective. Such boundaries need to be upheld at the social or cultural level to count for anything. If not individuals can always free ride and extract the monetary instrumental value that others have agreed not to pursue. While economics is undoubtedly a valuable form of knowledge, it is a way of seeing things, not the way. A full century after Pigou formalized the idea of externalities, we might mark the anniversary by taking more seriously the effort to clarify the appropriate reach of economics and markets within the broader social and cultural context. Economics is about solved Political Problems Arguably, one of the most important questions in economics is not even an economic question. The field effectively punts the matter of its own ontology – the things that economics can talk about – to a different discipline. In Abba Lerner’s words: “An economic transaction is a solved political problem. Economics has gained the title of Queen of the Social Sciences by choosing solved political problems as its domain.” Economics has been strangely content to focus its efforts on pattern-seeking within a domain it leaves other disciplines to define, but in the absence of contemplating its boundaries more explicitly, it has hubristically come to believe it has greater reach than it really has. In turn, this leaves most economists – and the great many people who think and act economically in conducting their professional duties – dangerously unaware of where economic thinking is beneficial and valid and where it ultimately hits limits. Duncan Austin: Pigou and the dropped stitch of economics RWER95 (pdf) 25.3 Ecosystem Services Constanza • Global loss of ecosystem services due to land use change is $US 4.3–20.2 trillion/yr. • Ecoservices contribute more than twice as much to human well-being as global GDP. • Estimates in monetary units are useful to show the relative magnitude of ecoservices. • Valuation of ecosystem services is not the same as commodification or privatization. • Ecosystem services are best considered public goods requiring new institutions. In 1997, the global value of ecosystem services was estimated to average $33 trillion/yr in 1995 $US ($46 trillion/yr in 2007 $US). In this paper, we provide an updated estimate based on updated unit ecosystem service values and land use change estimates between 1997 and 2011. We also address some of the critiques of the 1997 paper. Using the same methods as in the 1997 paper but with updated data, the estimate for the total global ecosystem services in 2011 is $125 trillion/yr (assuming updated unit values and changes to biome areas) and $145 trillion/yr (assuming only unit values changed), both in 2007 $US. From this we estimated the loss of eco-services from 1997 to 2011 due to land use change at $4.3–20.2 trillion/yr, depending on which unit values are used. Global estimates expressed in monetary accounting units, such as this, are useful to highlight the magnitude of eco-services, but have no specific decision-making context. However, the underlying data and models can be applied at multiple scales to assess changes resulting from various scenarios and policies. We emphasize that valuation of eco-services (in whatever units) is not the same as commodification or privatization. Many eco-services are best considered public goods or common pool resources, so conventional markets are often not the best institutional frameworks to manage them. However, these services must be (and are being) valued, and we need new, common asset institutions to better take these values into account. Constanza (2014) Global value of ecosystem services (Paywall) (pdf) Constanza (2019) Natural Capital and ERcosystem Services “The fossil fuel industry has been granted the greatest market subsidy ever: the privilege to dump its waste products into the atmosphere at no charge.” (Michael Mann) 25.4 Environmental Degradation 25.4.1 Kuznets and Engel Curves Environmental Kuznets and Engel’s Curve I think degrowth is wrong on the merits (environmental kuznets curves and environmental engel curves are both concave), but it’s also an obvious nonstarter even if it was founded on solid footing, To spell this out: if EKC and EECs are concave, redistribution within or between countries doesn’t necessarily reduce environmental damages. (John Voorheis (twitter)) Abstract Maneejuk: This study aims to examine the relationship between economic development and environmental degradation based on the Environmental Kuznets Curve (EKC) hypothesis. The level of CO 2 emissions is used as the indicator of environmental damage to determine whether or not greater economic growth can lower environmental degradation under the EKC hypothesis. The investigation was performed on eight major international economic communities covering 44 countries across the world. The relationship between economic growth and environmental condition was estimated using the kink regression model, which identifies the turning point of the change in the relationship. The findings indicate that the EKC hypothesis is valid in only three out of the eight international economic communities, namely the European Union (EU), Organization for Economic Co-operation and Development (OECD), and Group of Seven (G7). In addition, interesting results were obtained from the inclusion of four other control variables into the estimation model for groups of countries to explain the impact on environmental quality. Financial development (FIN), the industrial sector (IND), and urbanization (URB) were found to lead to increasing CO 2 emissions, while renewable energies (RNE) appeared to reduce the environmental degradation. In addition, when we further investigated the existence of the EKC hypothesis in an individual country, the results showed that the EKC hypothesis is valid in only 9 out of the 44 individual countries. Maneejuk (2020) Does the Environmental Kuznets Curve Exist? (pdf) The Kuznets curve expresses a hypothesis advanced by economist Simon Kuznets in the 1950s and 1960s. As an economy develops, market forces first increase and then decrease economic inequality. Since 1991 the environmental Kuznets curve (EKC) has become a standard feature in the technical literature of environmental policy, though its application there has been strongly contested. The environmental Kuznets curve (EKC) is a hypothesized relationship between environmental quality and GDP growth: according to its argument, which is spurious, various indicators of environmental degradation tend to get worse as modern economic growth occurs until average income reaches a certain point over the course of development, at which point some studies have argued, they improve. It first became popular as introduced by Gene Grossman and Paul Krueger in their working paper: “Environmental Impacts of a North American Free Trade Agreement.” This paper simply showed that the non-direct Greenhouse gas, Sodium Dioxide, Dark Matter, and Suspended particles followed an inverted-U shaped pattern. This was almost immediately misinterpreted by the World Bank and Beckerman and adopted into policy as an argument that all negative environmental effects would follow an EKC pattern. Copious research has concluded that beyond these pollutants, and issues like water quality, that immediately threaten human health, GDP growth essentially harms, and does not help the environment with no lasting “turning point.” The EKC has led to poor policy choices reaping untold environmental damage. Wikipedia 25.5 Energy and Transport The “hidden cost” of our largely fossil fuel-based energy and transport systems could add up to around $25 trillion (£18 trillion) – the equivalent of more than a quarter of the world’s entire economic output. That’s according to new research, which estimates the hidden environmental, social and health costs associated with different forms of transport and electricity generation. Sovacol (2021) (pdf) Independent "],["capital.html", "26 Capital 26.1 Produced Capital 26.2 Natural Capital 26.3 Human Capital 26.4 Social Capital", " 26 Capital 26.1 Produced Capital 26.1.1 Cambridge Controversy Spash Produced capital. What is it? Produced capital is defined as ‘capital goods embodied in human-made goods or structures, such as roads, buildings, machines, and equipment’ (Dasgupta, 2021, p. 507). These are physical assets, generated by human transformation of natural capital, that are used to provide a flow of goods or services, e.g. a sewing machine, factory or computer. A private house counts as produced capital because it provides services (e.g. shelter) repeatedly over time. Intangible assets, such as company patents, are also included. Produced capital is then a diverse stock measured as a value in national wealth accounts, and an increase of which contributes to economic growth (GDP). Thus, ‘inclusive wealth increases if and only if aggregate consumption is less than net domestic product (NDP), that is, GDP less the depreciation of all capital assets’ (Dasgupta, 2021, p. 138, emphasis original). Measuring the value of capital is then essential to the whole approach. How is it valued? Different forms of capital cannot be aggregated physically (i.e. hammers and tractors do not add together). So what is the aggregate or total amount of capital? The stock can be measured either as: (i) the monetary cost of production or (ii) the monetary returns attributed to specific capital on future output produced (i.e. future profits). The former, (i) involves capital itself in the pro- duction of capital and so ends in circularity with the value of capital determining the value of capi- tal, ad infinitum. One work around is to adopt a labour theory of value, so that all produced capital s valued by the labour required for its production. Today this classical economic theory is generally rejected outside of classical Marxist economics. As a neoclassical economist, Dasgupta opts for (ii), claiming that: ‘[a]ssets acquire their value from the services they provide over their remaining life’ (Dasgupta, 2021, p. 138). This leads to an asset man- agement approach whereby different types of assets, or forms of capital, are required to produce the same rate of return in order to achieve an optimally managed investment portfolio (i.e. that maximizes returns by equating returns on every investment at the margin). More simply, this means whether investing in produced capital, education or blue whales the economic agent (‘citizen investor’) seeks the same return. What is ignored by Dasgupta is a long history, that involved his own University and Economics Department, concerning problems with measuring capital. What is problematic about it? The failings of both approaches, (i) and (ii) above, were the subject of the ‘Cambridge Capital Con- troversy’, involving combat between economists in Cambridge England and USA. Starting in the 1950s this continued for two decades, or more, and was never resolved (see Cohen &amp; Harcourt, 2003). In case (i) there is the need to take into account a flow of costs over time (period of pro- duction) which, in economics, requires knowing the rate of interest as a basis for equating values in different time periods. In case (ii) knowing the value of (profit from) a stream of future output (over a period of production) means calculating the net present value and so discounting it at a rate of interest. Knowing the rate of interest is required in both cases. However, the rate of interest is the return on capital investment, which requires knowing the quantity of capital. So, the value of capital cannot be determined without knowing the stock of capital, that, for multiple forms of capital, becomes a value which cannot be known without the rate of interest, which is defined by already knowing the stock of capital, and so on … Neoclassical economists (aka Cambridge USA) then opted for naïve empiricism and claimed they could collect data and observe the rates of return in actual markets without explanation as to how, or from where, it is produced. Thus, in this tradition Dasgupta claims that: ‘[t]he yield on investment in produced capital is its marginal product’. Solow, whom Dasgupta cites as a major influence on his economics, has sought to justify this approach. Yet, the basic problem remains, the value of capital and, indeed, its definition, are left indeterminate and the empirical approach lacks validity. The alternative is to admit that neoclassical theory bears no relationship to reality, and capital investment is not about simplistic production functions specifying the rate of return to different factors (i.e. land, labour, capital) measured by disaggregated marginal products, but, rather, con- cerns institutional arrangements to capture surplus. Indeed, outside of economic textbooks, the contributions of the separate factors to output cannot be determined, let alone a marginal product attributed to each (i.e. what is due to labour vs. capital, say the farmer versus the tractor, let alone the land!). Rather than marginal productivity theory we might instead consider that profit is derived from the social power of those able to appropriate the technological achievements of society as a whole. They may be capitalists in market dominated economies or functionaries of the State in centrally planned economies. Under capitalism the key to power lies in gaining private property rights over resources, and this then lies at the heart of the debate over biodiversity. What is at stake is the legal right and economic authority to capture the surplus created by the production process. This is why classical political economy (as opposed to neoclassical economics) connected individuals’ dependence on the market for their livelihoods with social class, as the fundamental unit of analysis. Rather than marginal productivity theory we might instead consider that profit is derived from the social power of those able to appropriate the technological achievements of society as a whole. They may be capitalists in market dominated economies or functionaries of the State in centrally planned economies. Under capitalism the key to power lies in gaining private property rights over resources, and this then lies at the heart of the debate over biodiversity. What is at stake is the legal right and economic authority to capture the surplus created by the production process. This is why classical political economy (as opposed to neoclassical economics) connected individuals’ dependence on the market for their livelihoods with social class, as the fundamental unit of analysis. 26.2 Natural Capital (se renv on Spash/Dasgupta) Spash (2021) The Dasgupta Review deconstructed: an exposé of biodiversity economics (pdf) 26.3 Human Capital Spash The concept of human capital is heavily related to productivity in a wage labour economy. People who can be more productive have more value and those who live longer (i.e. the young) can be productive for longer, and so have more value, than others. The two main elements, health and education, must be converted into monetary values to oper- ationalize the human capital approach. Dasgupta (2021, p. 256) notes that: ‘The value of a statistical life (VSL), [is] a concept central to the meaning and measurement of human capital’. The idea is that monetary values can be placed on human life without specifying the people who will actually lose their lives as a result of a public policy decision. There are two main methods for assessing the risk of death or VSL. First, an indi- vidual may be directly asked their willingness-to-pay to avoid a risk or their willingness-to-accept compensation for incurring a risk. CVM surveys have been commonly applied but also been severely criticized. The other main alternative is to use measures related to earnings, a revealed preference method, technically termed a hedonic wage approach. This might, for example, use actual wage differentials in jobs with a range of risks. The definition of human capital as productive wealth could be understood as framing a govern- ment’s relations with its citizens primarily through the lens of their economic contribution, as if a human resources department, ensuring good health and education to the extent that it contrib- utes to productivity. This implies allocating resources according to the expected payback, e.g. prior- itizing young healthy adults. This productivist logic led some economists to justify eugenics. There are also long standing racist associations with references to lazy indigenous peoples by colonizing Europeans, and classist associations as in the history of removing common rights to force the poor into wage labour relations so they could become productive. The apparently simple case of investment in education also quickly runs into trouble. Financial returns neither require being educated nor does education bring financial returns per se. Under capitalism it is business, banking and finance that ‘makes money’ not just being educated. Health (mortality/morbidity) as a capital investment is even worse. Producing money numbers here requires the conjuring trick of talking about abstracted non-real people who are represented as ‘statistical lives’, under the VSL. For example, the results are used in transportation assessment to decide upon road building programmes and the installation of safety equipment. However, the public rejection of this approach is exposed when there is a train crash, people are killed and the public discover the lack of safety equipment is due to the calculation that it cost more than the expected fatalities times the VSL. Politicians rarely defend the numbers in such circumstances, although their transport departments may continue to use them on a daily basis. Spash (2021) The Dasgupta Review deconstructed: an exposé of biodiversity economics (pdf) 26.4 Social Capital Spash Social capital is defined as mutual trust and associated norms of reciprocity that enable people to engage with one another. Taken together, trust in others, confidence in government to deliver and in markets to function well, and the institutional arrangements that enable people to engage with one another for mutual benefit, is called social capital – a concept central to the economics of biodiversity. While mainstream economics commonly views society as comprising three classes of institutions (households, firms, government), ‘the idea of social capital illuminates a fourth class, comprising communities and civil society’ Dasgupta’s understanding here, and his capitalist reductionism, appear quite limiting. Social capital must be optimized for several reasons. First, he believes trust and economic growth are positively related so that more cooperation improves efficient allocation of resources and so increases wealth. Second, civic engagement and membership in associations discipline governments and improve governance. Third, communities and civil society are regarded as essential for controlling Nature conservation and restoration programmes initiated by government or national/international NGOs. Bringing together a range of actors – governments, NGOs and ‘increasingly’ private firms – is advocated to build local institutions to engage people in collective action and set rules. This is necessary because ‘beliefs do not appear out of nowhere’. Accordingly, this will be an institutional process: ‘That helps to align beliefs’ The danger of blanket calls to ‘align beliefs’, Diversity of opinions, different stake- holders perspective, ‘misaligned beliefs’ and public debates are what democracy is about. Aligning beliefs is more inline with totalitarianism. Absolute trust in government by all is also neither likely nor something to be ‘optimised’ via investment. Promoting such ‘social capital’ might easily be instru- mentalised to silence critical voices, blame civil society for being uncooperative, depoliticize issues and dismiss genuine concerns – class struggle, power relations, value conflicts. Civil society is also divided. The concept and promotion of social capital by The Review appears fuzzy, double-edged and dangerous for democracy. Spash (2021) The Dasgupta Review deconstructed: an exposé of biodiversity economics (pdf) "],["money.html", "27 Money 27.1 Money Creation 27.2 Inflation 27.3 Inflation as Restructuring 27.4 Modern Monetary Theory (MMT) 27.5 Milton’s Money 27.6 The Dollar’s Imperial Circle", " 27 Money Money is formalized debt. Informal debt, ie. favour-based, or gift economies, CAN be ecologically sustainable. But by formalizing this kind of social obligation and cohesion, we necessarily commoditize our social ecologies. Formalization leads to ecological unsustainability. (BichlerNitzan tweet) Money is, above all, a subtle device for linking the present to the future (Keynes) Everyone can create money; the problem is to get it accepted.(Hyman Minsky) 27.1 Money Creation Addiction Bank of England ‘addicted’ to creating money, say peers. The Bank of England risks becoming addicted to creating money and needs to come clean about how it plans to unwind its £895bn bond-buying programme, the House of Lords has warned. Guardian McLeay (2014) Money Creation in the Modern Economy (pdf) The value of money depends on not creating too much of them relative to future value creation. (Pengenes verdi henger på at det ikke lages for mye av dem i forhold til fremtidig verdiskaping.) Bankplassen: Hvordan skapes penger (‘How money is created’ in Norwegian) 27.2 Inflation Monetarist theory, which came to dominate economic thinking in the 1980s and the decades that followed, holds that rapid money supply growth is the cause of inflation. The theory, however, fails an actual test of the available evidence. In our review of 47 countries, generally from 1960 forward, we found that more often than not high inflation does not follow rapid money supply growth, and in contrast to this, high inflation has occurred frequently when it has not been preceded by rapid money supply growth. There are several implications. The most relevant of these seems to be that the current efforts of central banks to engender inflation are unlikely to be successful. Based on our examination of countries that together constitute 91 percent of world GDP, we suggest that high inflation has infrequently followed rapid money supply growth, and in contrast to this, high inflation has occurred often when it has not been preceded by rapid money supply growth. The U.S. economy may well experience some increase in inflation in the coming year, but if it does, it is likely it will be due to factors other than monetary policy. Money Growth Inflation 27.2.1 FED Policy and Inflation When you view money supply and velocity together, one notices they tend to offset each other. However, we highlight the late 1970s, the last highly inflationary period, to show a period they did not counteract each other. The Fed and Treasury are playing a dangerous game. The numbers we discuss above are massive and dwarf anything seen in American history. If consumers start spending their savings, and the government keeps borrowing and spending unprecedented amounts, velocity can pick up rapidly. We leave you with a vital question to better understand the prospects for inflation. Lebowitz Lebowitz and Freeze 27.3 Inflation as Restructuring Fix In the hands of economists, the idea of an ‘average price level’ is, to echo Joan Robinson, “a powerful instrument of miseducation”. The trouble is that averages are a mathematical identity — they are true by definition. I can calculate the average of any conceivable set of numbers. But that doesn’t mean my calculation will be informative. That’s because averages define a central tendency, yet do not indicate if this tendency actual exists. The idea that averages should be reported together with a measure of variation is a basic part of empirical science. And yet when economists study inflation, this practice is conspicuously absent. Economists won’t call it an ‘average’. They’ll call it a ‘price index’. Most people will assume that the movement of the average price indicates a strong central tendency. In other words, they’ll assume that inflation is uniform. Figure: Price change in the real world. The black line shows the change in the US consumer price index since January 1, 2020. The colored lines show the indexed price of all the individual commodities tracked by the CPI. Many commodities are tracked in multiple locations. Inflation is never uniform … it is always differential. And that makes it highly significant. Inflation restructures the social order, producing winners and losers. It is this restructuring that is the most important aspect of inflation. And yet it is this feature that economists almost completely ignore. If price change varies wildly by commodity (as it does in the real world), then the movement of the average price tells you little (if anything) about the movement of individual prices. And that means the money supply tells you little (if anything) about real-world inflation. Actually-existing societies produce many commodities whose prices do not change uniformly. And that creates a problem. It means that the quantity of production, \\(Q\\), is hopelessly ambiguous. Having seen that price change varies greatly between commodities, you might wonder why this matters. Well, it matters because it means that inflation is not purely a ‘monetary’ phenomenon. Inflation redistributes income. Nitzan and Bichler have discovered, for instance, that inflation systematically benefits big business. Notice how this evidence changes your view of inflation. It makes it hard to blame government for the problem. You see, if big business is systematically benefiting from inflation, it implies that these big corporations are raising prices faster than everyone else. In other words, it is oligopolies that are driving inflation. Inflation is a power struggle over who can raise prices the fastest. Fix (2021) The Truth About Inflation 27.4 Modern Monetary Theory (MMT) As a theoretical school of thought, MMT draws heavily on J.M. Keynes’s analysis of monetary production economy, Abba Lerner’s theory of Functional Finance (FF), Hyman Minsky’s Financial Instability Hypothesis (FIH), Wynne Godley’s Sectoral Balance (SB) approach to macro modeling, and the work of G.F. Knapp and A. Mitchell Innes, who independently developed chartalist or state theories of money. These are our forefathers and MMT is an amalgamation of their most important contributions. Fullwiller (2012) MMT a debate (pdf) MMT and Austrian economics are mirror images. MMT wants soft money to redistribute wealth to middle class. Austrians want hard money to maintain value of middle class savings. Dominant capitals, in control of state, ignore both and switch between soft and hard to maintain power. (Ian Wright) Culbreath The common conservative objection to government spending is not only based on a faulty economic model, but it also serves as a mental obstacle to using government capacity for purposes conservatives are supposed to care about. Conservatives should re-examine the economics that underlie the tendency on the right to condemn any instance of large-scale government spending. The policy conversation could then shift away from whether government spending is economically justifiable and towards what kinds of spending are productive and worthwhile. According to MMT, no sovereign state that issues its own currency can ever run out of money. This is because it simply “prints” its money in order to pay for whatever operations it deems necessary. The government doesn’t need to go out and find the money it needs for such spending; it needs no fundraisers, no sales, and not even taxes, to fund its operations. It simply spends into existence the money that it needs for its operations. unlike any private entity, such as a household or a corporation, the government does not need to worry about its own solvency, since it can never go insolvent—it can never run out of money. Thus, the government does not need to take any of the measures that private entities take to accumulate savings or make a profit. It will always be able to “spend into existence” whatever money it needs to pay for its operations. The difference between the federal government and a household is thus not merely a difference in degree (one is bigger than the other). It is a difference in kind. It follows that taxes do not amount to the accumulation of wealth for the government. Rather, they amount to no more than the extinction of money from the economy. Like God, the government giveth, and the government taketh away; the government spends money into the economy by printing it and removes money from the economy by taxing it. In terms of the pure quantity of government spending, there is no limit except inflation to what the government may spend. Inflation only occurs, at least in any damaging degree, under certain conditions. Foremost among those conditions is full employment. When the economy is at full employment, which it rarely is, then the overall purchasing power within the economy may be considered to be at full capacity. At this point, an injection of more money into the economy might result in inflation, since it would likely push demand to outstrip supply, thereby causing prices to soar higher and purchasing power to decline at a dangerous rate. However, even this danger could be averted to a significant degree if the government aimed its spending not only at stimulating demand but also at stimulating the production of consumer goods. In fact, the U.S. government has a long history of doing exactly this, as the extensive research of economists like Mariana Mazzucato demonstrates quite compellingly. If there is a risk of inflation from government spending, it would not be because government spending automatically produces inflation. Rather, it might be due to the fact that the U.S. government has ever since the Reagan era failed to make a priority of stimulating both demand and production through the implementation of a deliberate industrial policy. This only highlights further the importance of well-planned spending by the government. Furthermore, the government has many other tools at its disposal for controlling inflation. Not only does the Federal Reserve play a central role here, but even taxes can be levied by the government as a way of controlling inflation. When demand begins to outstrip supply, the government may very well consider imposing taxes where this might effectively limit inflation. Of course, not just any taxation would do this; the government needs to ensure that its taxes target entities or classes who spend enough money to affect consumer prices. Taxing the extremely wealthy, for example, may not be the best way to control inflation, since wealthy people tend to save more than they spend. while there are many good reasons for the government to impose taxes, especially to control inflation, funding government expenditure is not one of those reasons. Indeed, the idea that taxes should fund the government’s expenditure by “plugging its deficits” could prove to have damaging effects on the economy as a whole, if actually put into practice. As MMT theorists are fond of pointing out, what we call the government’s deficit actually amounts to nothing other than the people’s surplus. As long as the government (public sector) spends more into the economy (private sector) than it taxes out (the definition of deficit spending), the economy itself will enjoy a money supply large enough to stimulate productivity and growth. By contrast, if the government were to tax more out of the economy than it spent into it, the economy would suffer from its own deficit, a risk of deflation, and a potential crisis of underconsumption. In other words, the deficit by itself is not a bad thing, and it is here to stay. Culbreath (2021) MMT for Conservatives 27.5 Milton’s Money Fix Like a good neoclassical economist, Friedman grounds his theory in an accounting identity — one that relates the quantity of money \\(M\\) to the average price level \\(P\\): \\[MV=PT\\] In this identity, \\(V\\) is the ‘velocity of money’ — the rate that money changes hands. And \\(T\\) is an index of the ‘real value’ of all transactions. The nice thing about this accounting identity is that it is true by definition. So if you tie a theory of inflation to it, your ‘predictions’ will always work. The problem, pointed out by critics, is that this identity tells us nothing about causation. It could be that printing too much money causes prices to rise. Or it could be that rising prices drive people to borrow (and hence ‘create’) more money. Fix (2021) The Truth About Inflation 27.6 The Dollar’s Imperial Circle Akinzi The importance of the U.S. dollar in the context of the international monetary system has been examined and studied extensively. In this post, we argue that the dollar is not only the dominant global currency but also a key variable affecting global economic conditions. We describe the mechanism through which the dollar acts as a procyclical force, generating what we dub the “Dollar’s Imperial Circle,” where swings in the dollar govern global macro developments. Fig: A Strengthening Dollar Is a Procyclical Force Governing Global Manufacturing and Trade A tightening of U.S. monetary policy sets the circle in motion, generating an appreciation of the dollar. Given the structural features of the global economy, tighter policy and an appreciation of the dollar lead to a contraction in manufacturing activity globally, led by a relatively larger decline in emerging market economies. The resulting contraction in global (ex-U.S.) manufacturing will spill back to the U.S. manufacturing sector due to the reduction in foreign final demand for U.S. goods. These same forces will also lead to a drop in commodity prices and world trade. In the final turn of our mechanism, given that the U.S. economy is relatively less exposed to global developments, the contraction of global manufacturing and global trade is associated with a further strengthening of the dollar, reinforcing the circle. Akinzi (2023) The Dollar’s Imperial Circle "],["markets.html", "28 Markets 28.1 Market Failure 28.2 The Rise and Fall of Market Societies 28.3 The Market Institution 28.4 Market Primacy 28.5 Markets as Entropy Maximisers 28.6 Competition Bargaining", " 28 Markets Markets are politically regulated institutional processes far removed from idealized, perfect competition, and prices are negotiated between small numbers of powerful brokers, involving hidden subsidies, e.g., for infrastructure and military technologies. Governments provide safety nets and corporate bailouts in times of crisis, offering a form of public insurance to facilitate private profit. (Beslik) Markets are always imperfect and incomplete and can thus never be characterised as Pareto-optimal. They are always in a situation where a government (a central planner) is able to improve a market outcome. The market economy has not always existed – indeed it has been around for only about 250 years. Before that there were feudal economies where peasants or serfs worked the land for their masters who consumed the produce. That system was around for over 1000 years. Before that there were slave economies where people captured in wars were forced to work for their slave owners – that system was around for thousands of years. A primary reason for the ongoing failure to respond to climate change is that markets are predominantly oligopolistic in nature (i.e., dominated by a small number of suppliers). In addition, competition in markets incentivizes pushing costs onto others. (Beslik) 28.1 Market Failure Climate change is not the result of a market failure but rather the outcome of a fully functioning capital-accumulating economy working hard to shift costs onto others, especially those who lack voice or power (such as the poor, future generations, children, and nonhumans). (Beslik) Reinert ‘Market failure’ is a term oft en used when actual developments fail to behave the way economic theory would predict. Cimoli, Dosi and Stiglitz (2009) acknowledge that ‘market failure’ is not a useful way to approach the problem of poverty. In fact, from a Schumpeterian angle, what we gener- ally refer to as ‘development’ is, in fact, a ‘market failure’ compared to the standard neoclassical model assuming perfect competition and diminishing returns. What all developed countries have in common is a large increasing returns sector that has created huge barriers to entry, imperfect competition, and a ‘rent’ that has been divided among capitalists (high profi ts), labour (high wages) and the government sector (larger tax base). The shock therapies of the Washington institutions—instant free trade and ‘structural adjustments’—sent poor countries, whose industrial sectors were not yet competitive on the world market, ‘from Boston to St. Louis’ in Carey’s scheme. Reinert (2011) The terrible simplifiers (pdf) 28.2 The Rise and Fall of Market Societies van Bavel Markets emerge in an equitable setting and grow by creation of institutions that secure easy access to broad groups. The opportunities of market exchange push up economic growth and well-being as long as the fruits of growth are fairly evenly distributed. As markets become dominant - especially the markets for land, labour and capital - inequality grows in a slow process as ownership of land and capital concentrates. As inequality grow, economic growth initially continues, but to a lesser degree get translated into broad well-being. Purchasing power stagnates for large parts of the population. Shrinking demand and declining profitability shift investments to financial markets away from productive sectors. Wealth is instead used to acquire political leverage through patronage and buying political positions. or by Through their dominance in financial markets and their role as creditors of the state the wealthy acquire key positions in the fiscal regime, bureaucracy and finance. Over time markets become less open and equitable, through both large wealth owners’ economic weight and their ability to skew the institutional organization. Productive investments declines and the economy stagnates. Economic inequality rises further, feeding into growing political inequity and social unrest, resulting in coercion, rebellion and breakdown. After breakdown the process starts to build up anew from a position of more equal distribution of wealth and power. Market economies were thus not the base of freedom and equity, as some theories would have it, but rather developed on the basis of earlier-won freedom and equity. The market subsequently replaced the associations and organizations of ordinary people as the allocation system, a process that sped up when market elites and state elites came to overlap and jointly, and often deliberately, marginal- ized these organizations. This reduced ordinary people’s opportunities to defend freedom, their access to decisionmaking power and their grip over land and resources. The allocation systems that prevailed before the rise of the market, whether the commons or other associations, had mostly included long- term security and environmental sustainability in their functioning, as ensured by their rules. But the market does not do so explicitly. 7 And in these other systems, cause and effect, and actor and affected person, were more closely linked, because of their smaller scale. In mar- kets they are less so. This poses a risk, since in a market economy, owners of land, capital and natural resources are often far detached from those affected by damage from exploiting resources. They also face fewer constraints on exploitation than systems with more divided property rights. van Bavel (2019) Power concentration and state capture: Insights f rom history on consequences of market dominance for inequality and environmenta l calamities (pdf) 28.3 The Market Institution Markets as one of the fundamental institutions of capitalism function precisely because fragmented actors come together to compete. However, these decentralized encounters are based on a (financial) infrastructure that must be as frictionless as possible, i.e. centralized. In received theory, the market is a flat institution, comprising numerous buyers and sellers whose actions are independent of each other and whose size is too small to individually alter the overall outcome. In the actual world, however, the market is anything but flat. What should the Biden administration have done to overwhelm supply chain bottlenecks early on in the crunch? What should the administration be doing now? I think many of us imagined that we live in a world where there’s a wizard behind the box. That there’s actually somebody in charge of all of this, and that that somebody must have made a mistake. And of course, it must be the president of the United States. But that’s not actually the world that we live in. It’s a market-based system. We’re lucky to live in an economy that’s built on the principles of free enterprise, and so while it’s easy to cast blame and point fingers at the administration, we have to recognize that they’re not really in charge of all of these things. They didn’t create this situation and I’m not 100% convinced that they’re the ones that are going to be best equipped to solve the problem… As much as we love the idea of a free enterprise system, the reality is that markets often fail…if you’re trying to address a market failure, you want to have a single person or team in charge that can dictate terms to all the different market actors. Markets: mechanisms for eliciting preferences and equating marginal rates of substitution among diverse agents with local information about production and consumption opportunities (Hayek) Roth (2019) How Market Design Emerged from Game Theory 28.4 Market Primacy Austin Today’s market primacy may be detrimental. With markets privileged – and non-market institutions discredited – government has been unable to correct the market’s omission of so many recognized externalities, let alone advance non-market regulations or prohibitions to protect our ecology. We conceived of Homo Economicus and built a logical model of the world around that conception and have ever since been trying to live up – live down, really – to that self-image. We have been striving to make our behaviour fit a simple model rather than adjusting our models to a new comprehension of our complex behaviour. Effectively, a subplot of our broader mind-culture co-evolution has been a ‘mind-market coevolution’, in which human minds have made markets have shaped minds. Central to that model – and to our current faith in markets – is the left-brain inspired idea that society can be reduced to rational individual ‘agents’ endowed with entirely independent preferences who exchange parts of the world in a market system that has the magical power to ‘add everything back up’ to arrive at the best of all possible worlds. It is a seductive vision – magically self-coherent and entirely insulated from any external limitations. It sounds exactly like the sort of place that the reductionist left brain would wish to inhabit. Two centuries after Carlyle, people formerly members of communities have become ‘human capital’ summable into ‘social capital’ wholes. To allow the market mechanism to be sole director of the fate of human beings and their natural environment… would result in the demolition of society. (Karl Polanyi) Where Einstein said you cannot solve problems with the same sort of thinking that created them, what McGilchrist effectively says is that you cannot solve problems with the same brain hemisphere that created them. What I suggest from a lowlier perch, and what Kumar invited his LSE hosts to consider, is that we are unlikely to solve problems that have arisen from economic primacy with thinking that upholds economic primacy. Our market-centric culture has granted primacy to the economist, not the ecologist, which may be the wrong way around. Of course, we do have to ‘manage our house’ (economics), but we must also be mindful of the state of Nature’s house (ecology) and of how the management of our house at the local level affects Nature’s house at the global level. Market advocates have sometimes likened the market to an ‘intelligence’. But if it is intelligent, it is very much intelligent in the way the left brain is intelligent: with a limited view of the world – a ‘part world’ – that it nonetheless believes is whole. McGilchrist has remarked: ‘the left brain doesn’t know what the left brain doesn’t know.’ By the same token, the market doesn’t know what the market doesn’t know, but we have told ourselves it is all-knowing: ‘markets are the solution, government is the problem’. Our challenge is not to build a sustainable economy but to develop a sustainable culture that has an economy. If a left-brained culture has enabled – and been reinforced by – the ascendancy of the private-enabling market over non-market institutions within human self-organization, then we might attain a more sustainable culture by revitalizing the institutions left behind by that ascent. Indeed, before the ‘neoliberalism’ that has been ascendant for the duration of the ‘Great Acceleration’, there was an ’embedded liberalism’ in which market and non-market institutions were more finely balanced. Critically, the potential benefit of such a rebalancing is to reinvigorate those institutions that can complement the markets by standing up to them! The real value of government is its potential not to amplify market forces but to modulate them. The market is not only self-regulating, but also susceptible to positive reinforcement loops that can become runaway problems. Austin (2021) The Matrix of the Emissary - Market Primacy and The Sustainability Crisis 28.5 Markets as Entropy Maximisers Markets are randomising machines, they maximise entropy, and this fact alone is sufficient to explain some of the inequality we observe. Market transactions involve a transfer of monetary value. After any transaction one party may have more or less money than before. It’s quite easy to write a short simulation program that takes a large collection of individuals that start with equal amounts of money. We then pick two individuals at random. One is the buyer. We randomly choose a proportion of their money to spend. The seller gets that money. We then repeat, and pick another two individuals at random. And we keep doing this forever. After a short period, we can then measure the distribution of money across individuals. And we find, once again, the exponential distribution. Most individuals have very little money, and a small number have a great deal. So the activity of market exchange is acting just like the cocktail shaker: its mixing everything up, randomising things, and maximising the entropy of the system. You might think that this model of money exchange is far too simple to tell us anything about real markets. But you’d be wrong. Remarkably, we observe the exponential distribution in actual economies. The exponential is a great fit to the bottom 80% of the wealth distribution, which is the vast majority of the population. And this holds true for whichever capitalist country we look at. The fact that 80% of the wealth distribution of actual economies follows an exponential law is a very astonishing regularity. We might think that differences in wealth must arise from accidents of birth or personal virtue. But the principle of entropy maximisation tells us there’s a much more important causal factor at work. We quickly get extreme income inequality even in an economy with identical individuals with identical initial endowments of money. The points is that markets are randomising machines, they maximise entropy, and this fact alone is sufficient to explain some of the inequality we observe. So the anarchy of the market is the primary and essential cause of economic inequality. But why doesn’t the exponential law fit the entire wealth distribution? What about the remaining 20% of rich people? 28.5.1 The social relations of production as constraints Ian Wright 28.6 Competition Bargaining Tankus The central question this paper tackles are: what institutional mechanisms coordinate between different market actors to produce market prices and how does law, particularly competition law, shape these institutional mechanisms? This issue is surprisingly neglected because of the intuition built up by textbook microeconomics that market actors will coordinate markets without explicit governance institutions. Supposedly, they’re to do this automatically: simply acting in their own narrow self-interest. We tackle this myth head on by tackling the markets which are often claimed as “real world approximations” of textbook “perfect competition” markets: chartered exchange markets. While these markets have many buyers and sellers, in most other ways they diverge wildly from textbook market behavior. The vast majority of those buyers and sellers are not producers or consumers of the product — if there is a currently produced product at all — but market specialists (e.g. brokers and dealers) who’s explicit purpose is to smooth out market volatility. Recognizing these markets as jointly governed by chartered exchanges and financial firms helps us realise that governance architecture reliably manages all types of markets. The main type of price market focused on our paper are prices which are administered by business enterprises i.e. administered prices. Activities that happen “within” businesses are often illegal when conducted outside of businesses. It may seem obvious that a firm can set the price on the product it sells. But don’t forget the same conduct, conducted by the same individuals, using the same tools would be illegal if shared. This is known in competition law as the “per se rule” against “price fixing”. This differential treatment between firms and non-firms — or larger firms and smaller firms — is the firm exemption. This differential treatment is worsened by the fact that current competition law gives a relatively free hand to large businesses dictating terms to their suppliers or customers. These are known as vertical restraints. As an aside, in “industrial organization”, businesses which you interact with but don’t undertake the same activities are referred to as “vertical” because they are either “behind” you, or in “front” of you. Meanwhile your direct competitors are “horizontal” to you — because you are in the same business. Ergo coordinating with your direct competitors is referred to as horizontal coordination. Not all markets are vertically governed. Meanwhile the vast, vast majority of markets are not completely owned by one firm. Thus, vertical restraints and the firm exemption cannot explain all market governance in contemporary markets. And if it can’t, we are still left with a missing explanation for how markets are governed horizontally when explicit horizontal coordination is illegal. Posed in these terms, the answer is obvious. Horizontal implicit coordination between direct competitors is legal. This type of market governance, known popularly as price leadership, requires sufficiently concentrated firms to make coordinating implicitly viable. Or at least make the appearance of implicit coordination plausible. Because only firms can be price leaders, and a number of relatively concentrated price followers are likely required to maintain market order, this form of market governance is an extension of the firm exemption — and built on top of it. Most obviously, a cartel of the same size and market share could not be a price leader in the world of the firm exemption. Price leadership exemption. This is a really important point to emphasize. Price leadership by concentrated, hierarchical business enterprises is the most common form of market governance. That’s because competition law is designed in such a way that no other form of market governance can be conducted legally. At the same time, the price leadership exemption facilitates explicit price coordination among large concentrated firms, since they are presumed to be engaging in price leadership. Monetary policy, and to a certain extent fiscal policy, is obsessed with price stability. But only the type of price stability that appears in aggregate price indices. Our competition laws don’t value price stability. Some might respond to this paper by arguing that cracking down on price leadership will finally produce “competitive prices” and “true competition”. I don’t think that is the case. I think that is simply a recipe for more price instability and uncertainty, without very many benefits. Tankus (2021) Competion as Collective Bargaining (paper pdf) "],["demand.html", "29 Demand 29.1 Theory 29.2 Human Needs 29.3 Aggregate Demand Function", " 29 Demand 29.1 Theory The way the economy works, the extraction limit is really an affordability issue. If the cost of extraction rises too high, relative to what people around the world have for spendable income, production will stop because demand (in terms of what people can afford) will drop too low. People will tend to cut back on discretionary spending, such as vacation travel and meals in restaurants, cutting back on demand for fossil fuels. [d] How “demand” works is poorly understood. Very often, researchers and the general public assume that demand for energy products will automatically remain high. A surprisingly large share of demand is tied to the need for food, water, and basic services such as schools, roads, and bus service. Poor people require these basics just as much as rich people do. There are literally billions of poor people in the world. If the wages of poor people fall too low relative to the wages of rich people, the system cannot work. Poor people find that they must spend nearly all their income on food, water and housing. As a result, they have little left to pay taxes to support basic governmental services. Without adequate demand from poor people, the prices of commodities tend to fall too low to encourage reinvestment. The majority of fossil fuel use is by commercial and industrial users. For example, natural gas is often used in making nitrogen fertilizer. If the price of natural gas is high, the price of fertilizer will rise higher than farmers are willing to pay for the fertilizer. Farmers will cut back on fertilizer use, reducing yields for their crops. The farmers’ own costs will be lower, but there will be less of the desired crops grown, perhaps indirectly raising overall food prices. This is not a connection that economic modelers build into their models. The lockdowns of 2020 show that governments can indeed ramp up demand (and thus prices) for energy products by sending out checks to citizens. We are now seeing that the approach seems to produce inflation rather than more energy production. Also, countries without energy resources of their own may see their currencies fall with respect to the US dollar. Tverberg (2022) Why No Politician Is Willing to Tell Us the Real Energy Story 29.2 Human Needs I remember being fascinated when I discovered Manfred Max-Neef’s matrix of human needs. It’s such a rich description of the human animal that defies reduction to a single dimension. It puts homo economicus to shame. (Blair Fix) Manfred Max-Neef (2007) Development and Human Needs (pdf) 29.3 Aggregate Demand Function Since 1976, Robert Lucas—he of the confidence that the “problem of depression prevention has been solved”—has dominated the development of mainstream macroeconomics with the proposition that good macroeconomic theory could only be developed from microeconomic foundations. Arguing that “the structure of an econometric model consists of optimal decision rules of economic agents” (Lucas, 1976, p. 13), Lucas insisted that to be valid, a macroeconomic model had to be derived from the microeconomic theory of the behaviour of utility-maximizing consumers and profit-maximizing firms. In fact, Lucas’s methodological precept—that macro level phenomena can and in fact must be derived from micro-level foundations—had been invalidated before he stated it. As long ago as 1953 (Gorman, 1953), mathematical economists posed the question of whether what microeconomic theory predicted about the behaviour of an isolated consumer applied at the level of the market. They concluded, reluctantly, that it did not: Market demand functions need not satisfy in any way the classical restrictions which characterize consumer demand functions… The importance of the above results is clear: strong restrictions are needed in order to justify the hypothesis that a market demand function has the characteristics of a consumer demand function. Only in special cases can an economy be expected to act as an ‘idealized consumer’. The utility hypothesis tells us nothing about market demand unless it is augmented by additional requirements.’ (Shafer and Sonnenschein, 1993, p. 671-72) What they showed was that if you took two or more consumers with different tastes and different income sources, consuming two or more goods whose relative consumption levels changed as incomes rose (because some goods are luxuries and others are necessities), then the resulting market demand curves could have almost any shape at all. They didn’t have to slope downwards, as economics textbooks asserted they did. This doesn’t mean that demand for an actual commodity in an actual economy will fall if its price falls, rather than rise. It means instead that this empirical regularity must be due to features that the model of a single consumer’s behaviour omits. The obvious candidate for the key missing feature is the distribution of income between consumers, which will change when prices change. The individual demand curve is derived by assuming that relative prices can change without affecting the consumer’s income. This assumption can’t be made when you consider all of society—which you must do when aggregating individual demand to derive a market demand curve—because changing relative prices will change relative incomes as well. Since changes in relative prices change the distribution of income, and therefore the distribution of demand between different markets, demand for a good may fall when its price falls, because the price fall reduces the income of its customers more than the lower relative price boosts demand. The sensible reaction to this discovery is that individual demand functions can be grouped only if changing relative prices won’t substantially change income distribution within the group. Alan Kirman proposed such a response almost 3 decades ago: If we are to progress further we may well be forced to theories in terms of groups who have collectively coherent behavior. Thus demand and expenditure functions if they are to be set against reality must be defined at some reasonably high level of aggregation. The idea that we should start at the level of the isolated individual is one which we may well have to abandon. (Kirman, 1989, p. 138) Unfortunately, the reaction of the mainstream was less enlightened: rather than accepting this discovery, they looked for conditions under which it could be ignored. These conditions are absurd—they amount to assuming that all individuals and all commodities are identical. But the desire to maintain the mainstream methodology of constructing macro-level models by simply extrapolating from individual level models won out over realism. Macroeconomics cannot be derived from microeconomics. Steve Keen "],["business-cycles.html", "30 Business Cycles 30.1 Real Buciness Cycle - RBC 30.2 Long Transistory Theory 30.3 Hybrid Theory 30.4 The magic of expectations management", " 30 Business Cycles 30.1 Real Buciness Cycle - RBC Smith Back when I was an economics graduate student, I learned about Real Business Cycle models. These are mathematical models of economic booms and recessions, and they rely on some fairly suspicious assumptions. In these models, recessions happen because of a slowdown in the rate of technological innovation — basically, scientists and engineers suddenly fail to make new discoveries at the same rate as before, or even forget some of what they knew. And in these models, unemployment happens because people know that technology will be advancing more slowly for a few quarters, leading to slower wage growth, and they decide to take a little time off. That is obviously wildly unrealistic. And even by the (necessarily) weak evidentiary standards of macroeconomics, Real Business Cycle models never did a good job of fitting the data. Nevertheless, the models won their creators a Nobel prize in 2004, and versions of them are still in use in many subfields of economics. When I asked a macroeconomist at the University of Michigan why RBC models were still so strangely popular, he shrugged and said “Politics!” RBC theory basically says that neither the Fed nor Congress can stabilize the real economy, so using things like fiscal stimulus or interest rate cuts to try to fight recessions is pointless. The macroeconomist I spoke to suggested that this feature of the models appealed to small-government libertarians, who — at least at that time — had a decent amount of clout in the economics field. If politics really explains a large portion of the popularity of RBC models, that’s not a good thing at all. In recent years, it has become fashionable in some circles to claim that because scientists are inevitably biased, attempts to be objective are doomed and therefore pointless. But as I argued in a post back in 2021, this is utterly wrongheaded. Smith (2023) Politicized science inevitably tends toward pseudoscience Smith The theory of Real Business Cycles (RBC) (wikipedia) is actually a lot more complex than the way I’m going to describe it right here, but I think this gets the most simple version across. Basically, in the context of this simple model, you can think of RBC as saying that aggregate supply moves around on its own — that no matter what happens to aggregate demand, the economy simply produces as much as it’s going to produce. In that case, the only thing that aggregate demand can do is to affect prices. In the words of Ed Prescott, the inventor of RBC theory, this means that monetary and fiscal policy are “as effective in bringing prosperity as rain dancing is in bringing rain” — you can print money and lend money and hand out government checks, but all it’ll do is pump up inflation. So the RBC story of 2021-2023 would be something like this: In 2020-21, the Fed lowered interest rates to zero and did a ton of quantitative easing and lent out a bunch of money, and the government also ran a huge deficit. But in 2022 it largely stopped doing those things. This created a transitory increase in inflation that eventually ended. But it basically did nothing to the real economy, because in RBC-world, monetary and fiscal policy never affect the real economy. In this explanation, the Fed made a big mistake — it should have simply sat there and let the free market do its thing, instead of pumping up inflation. The weakness of this theory is that while it fits the basic facts of 2021-23, it doesn’t fit past experience. Volcker’s interest rate hikes really did seem to raise unemployment to quite a high level. And lots of quantitative research has found that monetary and fiscal policy really do have an impact on the real economy. So if RBC explains 2021-2023, it’s a mystery as to why it worked this time when it hasn’t worked other times. Smith (2023) How did the U.S. achieve a soft landing? 30.2 Long Transistory Theory Smith This theory, endorsed by Paul Krugman and some other basically Keynesian economists, is that the inflation of 2021-22 was caused mainly by temporary supply shocks, which faded over time. Anyway, after the pandemic we had a bunch of snarled supply chains, and then in early 2022 we got a rapid rise in oil prices from Putin’s invasion of Ukraine. Then supply chain pressures started to ease in 2022 and were back to normal by the start of 2023 Basically, supply bounces back and forth and ends up where it was before. Inflation is temporarily higher and growth is temporarily lower, and then everything goes back to how it was. The problem with this theory is…growth didn’t really slow much. It wobbled for a couple of quarters in early 2022, but not enough for a recession to be called. But supply chains were very stressed out in 2021, and oil prices had already begun rising. Why was growth so strong in 2021? Even if you assume that aggregate demand is very inelastic (i.e. that the blue line on the diagram I drew goes straight up and down), it’s hard to explain why 2021 was such a boom year, if the only thing happening was a negative supply shock. The other problem with the Long Transitory theory is that it means the Fed’s power to affect either inflation or the real economy is very limited. If raising interest rates from 0% to 5% and massively increasing the federal budget deficit basically does nothing to aggregate demand, it calls into question the whole power of Keynesian stabilization policy. Long Transitory is basically a theory of Fed irrelevance. Smith (2023) How did the U.S. achieve a soft landing? 30.3 Hybrid Theory Smith The first two theories relied on the idea that only one important thing happened to the U.S. economy in 2021-23. But what if two important things happened? What if there was a transitory demand shock and a transitory supply shock? Under this “all of the above” explanation, the story goes like this: In 2020-21, the government printed a lot of money and lent a lot of money and borrowed a lot of money, pumping up aggregate demand. But in early 2022 this ended. In 2021-22, supply chains got stressed, and oil prices rose. But in late 2022 this ended. By 2023, both supply and demand were back to normal. Basically, inflation rises and then falls (leaving prices permanently higher than before), while growth isn’t really affected. That…kind of looks like what happened! And in fact, the differential timing of the demand and supply shocks can even explain why growth was strong in 2021 and stumbled a bit in early 2022 — the negative supply shocks came a little later than the positive demand shocks, so in early 2022 the economy was hit by oil prices even as government was no longer giving things a boost. So this theory is very good at explaining what happened over the last three years. The problem is that it’s not very parsimonious. The great scientist John von Neumann is said to have remarked “with four free parameters I can fit an elephant, with five I can wiggle his trunk”. We reward theories for being simple, because complex theories make things too easy. But that said, sometimes the real world just isn’t parsimonious. A macroeconomy is a very complex thing, with a lot of moving parts, and everything tends to happen all at once. So maybe 2021-23 just isn’t a simple story at all, much as we might prefer it to be one. Note that in this explanation, the Fed might have made a mistake in 2020-21. This hybrid theory holds that the Fed boosted growth at the price of causing more inflation, and whether that was a good tradeoff depends on which of those things you care about more. But in 2022, according to this theory, the Fed did exactly the right thing — it reduced aggregate demand just as aggregate supply was righting itself, leading to lower inflation without slower growth. 30.4 The magic of expectations management There’s one more theory I should mention here — the theory of expectations. Modern macroeconomic models aren’t usually as simple as the little AD-AS graphs I drew above. One way they’re more complex is that they allow for a big role for expectations. In these models, if people believe that Fed policy will be very dovish toward inflation in the future, they raise their prices today, and inflation goes up. But if people believe that the Fed will be hawkish in the future, they’ll expect lower inflation, and they won’t raise prices today, and inflation will go down. According to this theory — which macroeconomist Ricardo Reis used in September 2022 to successfully predict a fall in inflation — the Fed can get something close to immaculate disinflation if it can manage expectations effectively. And as a bonus, expectational effects happen fast — they don’t have to filter through a years-long chain of causality, from high rates to high unemployment to lower consumer spending to lower prices. In other words, according to expectations management theory, Fed rate hikes in 2022 convinced the country that the spirit of Paul Volcker still animates the institution, and that high inflation will simply not be allowed to persist, then perhaps the Fed beat inflation without having to raise rates so high that they threw people out of work. So in this story, as in the previous one, the Fed did a great job in 2022. How plausible is this story? We can observe the financial market’s inflation expectations directly, by looking at the 5-year breakeven. This shows that inflation expectations rose strongly in 2021, then spiked even higher in early 2022 before falling to only a little higher than their pre-pandemic average Reis has argued that the true impact of expectations was even bigger than what this graph might suggest, because it contained considerable skewness — there were a lot of people who were paying a lot of money to hedge against very high inflation. (There could be other reasons for that pattern, but it’s suggestive.) But although this pattern might seem roughly consistent with the expectations story, alternative explanations are also possible — for example, maybe expectations just follow actual inflation, and don’t matter much at all. As usual in macroeconomics, it’s pretty hard to prove what’s causing what. So anyway, those are the four basic simple theories of how the U.S. achieved a soft landing. You can choose for yourself which set of assumptions you find the most plausible here, and decide which theory is your favorite. As for me, I’m just glad it all worked out. Smith (2023) How did the U.S. achieve a soft landing? "],["competition.html", "31 Competition", " 31 Competition “Monopoly produces competition, competition produces monopoly” (Marx). If you want to capture lasting value, look to build a monopoly- competition is for losers. (Peter Thiel) Fix Concentration through acquisition At this point we’ve got some fairly incendiary evidence. The ‘crime’ of elite wealth concentration seems to be tied directly to corporate oligarchy. But before we put the case to rest, let’s consider the testimony of the defense’s expert witnesses. I’m talking, of course, about neoclassical economists. Ostensibly, neoclassical economists love competitive markets and hate monopoly. But beginning in the 1980s, a weird thing happened; economists at the University of Chicago started to argue that despite lacking competition, monopolies could still be ‘efficient’. Their reasoning was that if monopolists actually behaved badly, they would be undercut by competitors, and their monopoly would be undone. Therefore, if a monopoly exists, it must be because the monopolist is doing what the market wants. Now the logic here is torturous. We’re positing imaginary competition to justify a lack of real-world competition. But then again, neoclassical economists have never let the real world get in the way of their imaginations. And in this case, the goal of the imaginary theorizing was always obvious: it was designed get government out of the way and allow big corporations to purchase their way to power. Savvy corporations are always looking for a better route to power. And that better route is to buy instead of build. When you buy your competitor, you solve two problems at once: you accumulate power and reduce your competition. The difficulty, though, is that this buy-not-build tactic has the appearance of being a blatant power grab. So there’s the risk that an entrepreneurial government might get in the way. That’s where Chicago-school theorists come in. Starting in the 1980s, they successfully preached an ideology that got the government out of the way. The net result is the modern corporate landscape, forged in large part by a string of government-approved corporate acquisitions. The consolidated corporate landscape of the 21st century was forged by a massive, neoliberal wave of mergers and acquisitions. The buy-to-build ratio takes the value of corporate mergers and acquisitions and divides them by the value of greenfield investments. The greater this buy-to-build ratio, the more that corporations are buying (and not building) their way to power. The neoliberal era saw a massive wave of corporate mergers and acquisitions. As a result, from 1980 to 2000, the US buy-to-build ratio jumped nearly tenfold. And guess what accompanied this acquisition wave. That’s right … a sharp rise in corporate concentration. Rockefeller, did you know that he was one of the principle funders of the University of Chicago? Ironic, isn’t it. Rockefeller, like Thiel, spoke openly about his pursuit of power and personal enrichment. So if, during Rockefeller’s life, someone had connected elite wealth concentration to corporate consolidation, the reaction would have been “Well, that’s obvious.” Fast forward to the 1980s and the connection became not-so obvious, at least to economists. And that’s thanks in large part to Rockefeller’s Chicago-school investment, which pumped out decades worth of pro-oligarch propaganda. Fix (2023) Stocking Up on Wealth … Concentration "],["currencies.html", "32 Currencies 32.1 Exchange Rate 32.2 Hegomonic Currency", " 32 Currencies Roberts The US dollar is not being gradually replaced by the euro, or the yen, or even the Chinese renminbi, but by a batch of minor currencies. The share of reserves held in US dollars by central banks has dropped by 12 percentage points since the turn of the century, from 71 percent in 1999 to 59 percent in 2021. But this fall has been matched by a rise in the share of what the IMF calls ‘non-traditional reserve currencies’, defined as currencies other than the ‘big four’ of the US dollar, euro, Japanese yen and British pound sterling, namely such as the Australian dollar, Canadian dollar, Chinese renminbi, Korean won, Singapore dollar, and Swedish krona. All this suggests is that the shift in international currency strength after the Ukraine war will not be into some West-East bloc, as most argue, but instead towards a fragmentation of currency reserves. Roberts (2023) A multipolar World 32.1 Exchange Rate Tooze An increase in the price of oil - the lead commodity of the world economy - now improves the terms of trade of the United States. And the terms of trade are strongly correlated with the strength of the dollar. As the BIS team comments, whereas “in the past higher energy prices were associated with worsening US terms of trade and a weaker dollar, today these patterns are reversed. A similar positive relationship between the terms of trade and the exchange rate is commonly found for commodity exporters, such as Australia and Canada, but not for commodity importers (see Cashin (2004), Rees (2023)). This suggests that – unless the United States becomes a major net energy importer once more – the combination of higher commodity prices and a stronger US dollar could be more common in the future than it was in the past.” The result of this change in the “commodity price-dollar nexus” is extremely toxic for global growth: Movements in the US dollar now compound the effect of commodity prices changes on the global economy. Commodity price rises tend to stoke inflation and choke off growth in commodity-importing economies, while dollar appreciation tends to do the same outside the United States, especially in emerging market economies. The stagflationary effects of higher commodity prices exert themselves in part through higher consumer prices, which squeeze household incomes, and rising production costs for firms, which dampen investment. The stagflationary effects of a stronger dollar come through its dominant role in global trade and finance.2 The recent confluence of such incidents has significantly increased the risk that weak growth will coincide with high inflation. Consistent with this, inflation surged worldwide while growth fell in in 2022. The stagflationary impact of the dollar-commodity nexus is compounded by the similar impact of a strong dollar by way of the “credit channel”. A strengthening dollar tightens global dollar-credit and tends to strangle global investment and trade finance. What does it mean for the dollar system that the US is now a major energy exporter? One of the constants in the global economy we have known since the 1970s has been the waxing and waning of the flow of petrodollars generated by US oil imports. As the BIS comments: For decades, major oil exporters have priced their oil trades in US dollars. Often petrodollars have been recycled back into US Treasury debt and other US assets, reinforcing the dollar’s reserve currency status, as manifested in its dominant role in global trade and finance and its outsize impact on global economic and financial conditions. Less US oil imports could mean fewer petrodollars flowing into the global financial system. This could reduce liquidity and affect currency choices in trade invoicing and reserve management. Above all politics and geopolitics that are intruding into an otherwise well-balanced and well-defined system of global finance centered on the dollar. Those forces do, of course, impact global finance, but the dollar system that they are shaping is not in itself equal, balanced or unpolitical. Rather it is highly unequal, crisis-prone and constantly changing. It is by the interaction between macroeconomics, macrofinance and geoeconomics that the dollar system’s future will be decided. Tooze (2023) Chartbook #212 The end of the petrodollar? How macroeconomics may compound geopolitics in shaping the future of the dollar system. 32.1.1 Exchange Rate as Randow Walk Charles We analyze the efficiency market hypothesis [EMH thereafter] of major Euro exchange rates. The EMH states that, at any time, prices fully reflect all available and relevant information. Therefore, given only past price and return data, the current price is the best predictor of the future price, and the price change or return is expected to be zero (Fama, 1970; 1991). This is the essence of the weak-form EMH, which implies a random walk, and which has been the most commonly tested hypothesis in the empirical literature.  If the nominal exchange rate follows a random walk process, then the market is weak-form efficient, and therefore not predictable. This means that it is impossible for an exchange trader to generate excess returns over time through speculation. Alternately, if the nominal exchange rate is predictable, then the market is not weak-form efficient, which means that exchange traders can generate abnormal returns through speculation. The standard variance ratio [VR thereafter] test or its improved modifications have been used to test market efficiency. All studies on the random walk hypothesis [RWH thereafter] examine foreign exchange rates against the US dollar. We re-examine the random walk behavior of major Euro exchange rates in two ways. First, this study is based on an extensive sample. We study daily and weekly data for major Euro exchange rates over the 1999-2008 period. We analyze the weak-form from both data frequencies because a market can be considered to be perfectly weak-form efficient if it is found to behave randomly at any level of data frequency. We thus avoid the shortcomings of high and medium-low frequency data. Indeed, using high frequency data to estimate the market efficiency of developed markets allows us to take into account some of their characteristics, such as high trading volume and better information. However, it also implies biases for developing markets, such as non-trading and asynchronous prices, which are overcome with medium-low frequency data. We apply new VR tests proposed by Chen and Deo (2006) and Belaire-Franch and Contreras (2004), which are more powerful than those applied in previous studies, to examine the behavior of major Euro exchange rates. More precisely, Chen and Deo (2006) suggested a power transformation of the VR statistic, which corrects a well-known problem with the VR test, namely that the VR statistic is biased and right skewed in finite samples. Wright (2000) proposes an alternative to conventional asymptotic VR tests using rank tests, which are exact tests whose sampling distributions do not entail asymptotic approximations. Belaire-Franch and Contreras (2004) developed a multiple version of Wright’s (2000) rank tests that overcomes the problem of size distortions when applying individual VR tests for several holding periods. Conclusion This study employed new variance ratio tests to evaluate the random walk behavior of eleven major Euro exchange rates over the period of January 4, 1999, to May 30, 2008, using daily and weekly data. These tests, which are robust to heteroskedasticity and non-normality, are the Chen and Deo (2006) power-transformed tests and the multiple Belaire-Franch and Contreras (2004) rank and sign-based tests. The results suggest that Euro-based exchange rates for the major trading countries (Australia, Canada, Japan, UK, US, New Zealand, Korea and Switzerland) follow the random walk hypothesis at both data frequencies, and are therefore significantly and perfectly weak-form efficient, suggesting no excess returns over time through speculation. This outcome is not necessarily the case for non-major trading currencies, especially for the Swedish kroner, where the random walk hypothesis is rejected at daily and weekly frequencies. Finally, the weak-form efficiency is rejected for daily data but is not rejected for weekly data for Singapore and Norway, suggesting the possibility of abnormal returns through speculation on the short horizon. Charles (2009) Testing for random walk behavior in Euro exchange rates 32.2 Hegomonic Currency Feygin Is a trade deficit is necessary for a reserve currency to be a dominant currency? What makes a dominant currency is neither the composition of reserves nor invoicing but the denomination of private debt. Private debt is not defined by any kind of legal basis, but rather by issuing debt in money you cannot print. From this point of view, so-called “Eurobonds” issued by sovereigns are as private as corporate bonds. You had dominant currencies that were anchored by exporters: the United States after 1945 (I would go back further and say after 1916) and the British in the 19th Century. What allowed both these countries to be dominant currencies while being big exporters was that they managed a system through the capital account rather than the current account. In other words, they exported their savings aggressively to other countries while maintaining a trade surplus. The fact that the United States is a trade deficit hegemonic currency is not a rule but a special case.The system is possible. However, is it stable? The UK system was extremely unstable. It could operate for a very long time because it was a commodity currency (gold), and thus the UK would be forced to export capital. The 19th century UK was a very strange bird indeed. It ran big deficits vis-a-vis its colony – India – to feed itself (and starve others) because it was a very small, highly productive island. Think of it as Japan more than America. By the end of the pound system, it was losing the ability to be an exporter already but did not have the market depth to be a hegemonic importer. The American system is also strange. First, we can divide it into three periods. First is the pre-hegemonic period in the 19th Century, where the United States was a capital importer. Even then, the US is a massive domestic market relative to most other economies. Its import of capital results in a lot of financial instability but also very rapid technological installation, which, in turn, deepens the domestic market and leads to institutional innovation in the form of the modern corporation. In the second period, after WWI, the US was still a hegemonic exporter and captial exporter, and as Adam Tooze argues in The Deluge, this system worked for quite a bit: until it didn’t. That is how you got the Great Depression. A big part of that is that the US, as a hegemonic exporter, cannot manage the intricate political economy that is required to assure both general financial stability, growth, and the domestic politics of an export economy. After WWII the US is very much the most hegemonic exporter possible because the rest of the world is devastated by war. However, what we see after WWII is that through a series of intentional policies and improvisations, the United States slowly allowed the rest of the world to build up a deficit against it. It actively exported its capital to allow others to export goods back into it. It slowly but surely moved to a deficit hegemon position and then, with a lot of pain and struggle, rebuilt the monetary system into one that is relatively stable over the next fifty years. In Charles Kindleberger’s work, he defines the function of a monetary hegemon as playing a global coordinating role by running a deficit to provide markets for distressed goods and long-term liquidity, as well as creating both long-term credit taking in deposits and short term liquidity by lending into crises. This is a theoretical statement about a systemic stability condition based on a theory of how monetary relations interact with real ones. You can meet this stability condition as a hegemonic exporter if you are one because of your very high productivity relative to everyone else. But that productivity would have to be something truly revolutionary, like being a continent-sized economy that did not suffer the consequences of two world wars or unleashing the industrial revolution. So from the point of view of special conditions in the world of Kindleberger’s theory, we can have two special cases. Just as under Keynesian assumptions, we can have a world where Say’s Law holds; we can also have a world with a hegemonic exporter. However, from the point of view of political economy, the hegemonic importer is much more probable. This gets me into the issue of Chinese Belt and Road loans. I think China could be doing a lot in solidifying itself as a constructive force in the world by lending its massive surpluses to the developing world. However, I think it is doing it wrong either on purpose or because of, as Nils would argue Cold War-like assumptions. Most Chinese lending is still going to infrastructure. This is not a bad thing per se. Infrastructure is critical to development and has multipliers. But it doesn’t get you too far unless you can create a profit. Profits come from sales. In a Say’s Law version of the world, supply creates its own demand, so you will have profits from the act of industrializing. This is also the world of the Lewis development model, where you will have modernization and, thus, a release of labor into more productive industries. However, in a Keynesian world, demand is autonomous of supply and is very related to social structure. So you will have some level of demand created by an infrastructure multiplier, but unless it is managed in the right way, it will have diminishing returns. Demand has to come in from other sources. Either domestic consumption if you are a big market or from exports (aka importing other people’s demand). China’s lending, as far as I can tell, is not creating long-term demand for exports that is consistent with full employment in its borrower’s economy. Doing so would gradually turn China into a hegemonic importer, and that would create domestic issues. Here again, I used a theory (Kalecki and Harrod’s demand-side growth models) to inform the special cases possible under the theory. Feygin (2023) Theory and History "],["employment.html", "33 Employment", " 33 Employment Roberts Employment is a lagging indicator of activity in a capitalist economy. The lead indicator starts with profits, then investment and production, then employment and consumer spending. Roberts (2023) Navigating by the stars under cloudy skies – and holed below the water line "],["game-theory.html", "34 Game Theory 34.1 Nash Equilibrium 34.2 Mean-Field Game Theory (MFG) 34.3 Mechanism Design 34.4 Market Design", " 34 Game Theory Vintage game theorists is the best kind of economist (and some of the macroeconomists do even sometimes interpret their models as mean-field games) (Beatrice Cherrier) Plato: Game Theory Article 34.1 Nash Equilibrium Investopedia Nash equilibrium is a concept within game theory where the optimal outcome of a game is where there is no incentive to deviate from the initial strategy. More specifically, the Nash equilibrium is a concept of game theory where the optimal outcome of a game is one where no player has an incentive to deviate from their chosen strategy after considering an opponent’s choice.1 Overall, an individual can receive no incremental benefit from changing actions, assuming other players remain constant in their strategies. A game may have multiple Nash equilibria or none at all. Nash equilibrium is often compared alongside dominant strategy, both being strategies of game theory. The Nash equilibrium states that the optimal strategy for an actor is to stay the course of their initial strategy while knowing the opponent’s strategy and that all players maintain the same strategy, as long as all other players do not change their strategy. Dominant strategy asserts that the chosen strategy of an actor will lead to better results out of all the possible strategies that can be used, regardless of the strategy that the opponent uses. All models of game theory only work if the players involved are “rational agents,” meaning that they desire specific outcomes, operate in attempting to choose the most optimal outcome, incorporate uncertainty in their decisions, and are realistic in their options. Both the terms are similar but slightly different. Nash equilibrium states that nothing is gained if any of the players change their strategy if all other players maintain their strategy. Dominant strategy asserts that a player will choose a strategy that will lead to the best outcome regardless of the strategies that other plays have chosen. Dominant strategy can be included in Nash equilibrium whereas a Nash equilibrium may not be the best strategy in a game. The prisoner’s dilemma is a common situation analyzed in game theory that can employ the Nash equilibrium. In this game, two criminals are arrested and each is held in solitary confinement with no means of communicating with the other. The prosecutors do not have the evidence to convict the pair, so they offer each prisoner the opportunity to either betray the other by testifying that the other committed the crime or cooperate by remaining silent. If both prisoners betray each other, each serves five years in prison. If A betrays B but B remains silent, prisoner A is set free and prisoner B serves 10 years in prison or vice versa. If each remains silent, then each serves just one year in prison. The Nash equilibrium in this example is for both players to betray each other. Even though mutual cooperation leads to a better outcome if one prisoner chooses mutual cooperation and the other does not, one prisoner’s outcome is worse. The primary limitation of the Nash equilibrium is that it requires an individual to know their opponent’s strategy. A Nash equilibrium can only occur if a player chooses to remain with their current strategy if they know their opponent’s strategy. Investopedia 34.2 Mean-Field Game Theory (MFG) Kaust In the theory of mean-field games (MFG), the concept of Nash equilibrium and the rational expectation hypothesis are combined to produce mathematical models for large systems, with infinitely many indistinguishable rational players. While not all economists agree that the behavior of agents can be reduced to a precise mathematical formulation, utility maximization principles and game-theoretical equilibria explain, at least partially, many economic phenomena. The concept of competing agents is illustrated in the works of A. Cournot and L. Walras. Indeed, Walras, also known as the founder of the École de Lausanne, refers to Cournot in 1873, as the first au- thor to seriously recur to the mathematical formalism in investigating economic problems. Cournot duopoly model is one of the earliest for- mulations of a non-cooperative game. The term indistinguishable refers to a setting where agents share common structures of the model, though they are allowed to have heterogeneous states. In other terms, the MFG theory enables us to investigate the solution concept of Nash equilibrium, for a large population of heterogeneous agents, under the hypothesis of rational expectations. The mean-field game formalism was developed in a series of semi- nal papers by J.-M. Lasry and P.-L. Lions [118, 119, 120] and M. Huang, R. Malhamé and P. Caines [103, 106]. It comprises methods and techniques to study differential games with a large population of rational players. These agents have preferences not only about their state (e.g., wealth, capital) but also on the distribution of the remain- ing individuals in the population. Mean-field games theory studies generalized Nash equilibria for these systems. Typically, these mod- els are formulated in terms of partial differential equations, namely a transport or Fokker-Plank equation for the distribution of the agents coupled with a Hamilton-Jacobi equation. The mean-field game formalism was developed in a series of semi- nal papers by J.-M. Lasry and P.-L. Lions [118, 119, 120] and M. Huang, R. Malhamé and P. Caines [103, 106]. It comprises methods and techniques to study differential games with a large population of rational players. These agents have preferences not only about their state (e.g., wealth, capital) but also on the distribution of the remain- ing individuals in the population. Mean-field games theory studies generalized Nash equilibria for these systems. Typically, these mod- els are formulated in terms of partial differential equations, namely a transport or Fokker-Plank equation for the distribution of the agents coupled with a Hamilton-Jacobi equation. MFG methods are equilibrium models where all agents are rational. In simple problems, fundamental questions such as unique- ness, existence or stability were investigated extensively. However, many MFG problems arising in mathematical economics raise issues that cannot be dealt with the current results. In this book, we have attempted to illustrate the main techniques and methods in mean-field games theory, in several simplified models motivated by economic considerations. Kaust (2015) Economic Models and Mean-field Games (pdf) Economicshelp: Examples of Game Theory in Economics 34.3 Mechanism Design Investopedia Mechanism design is a branch of microeconomics that explores how businesses and institutions can achieve desirable social or economic outcomes given the constraints of individuals’ self-interest and incomplete information. When individuals act in their own self-interest, they may not be motivated to provide accurate information, creating principal-agent problems. Mechanism design theory generally takes a reverse approach to game theory. It studies a scenario by beginning with an outcome and understanding how entities work together to achieve a particular outcome. Both game theory and design theory look at the competing and cooperative influences of entities in the process towards an outcome. Mechanism design theory considers a particular outcome and what is done to achieve it. Game theory looks at how entities can potentially influence several outcomes. Investopedia Nobel 2007 Nobel (2007) mechnism Design 34.4 Market Design Roth Abstract We interview each other about how game theory and mechanism design evolved into practical market design. When we learned game theory, games were modeled either in terms of the strategies available to the players (“noncooperative games”) or the outcomes attainable by coalitions (“cooperative games”), and these were viewed as models for different kinds of games. The model itself was viewed as a mathematical object that could be examined in its entirety. Market design, however, has come to view these models as complementary approaches for examining different ways marketplaces operate within their economic environment. Because that environment can be complex, there will be unobservable aspects of the game. Mathematical models themselves play a less heroic, stand-alone role in market design than in the theoretical mechanism design literature. Other kinds of investigation, communication, and persuasion are important in crafting a workable design and helping it to be adopted, implemented, maintained, and adapted. Roth (2019) How Market Design Emerged from Game Theory "],["industrial-policy.html", "35 Industrial Policy", " 35 Industrial Policy Noah Smith Ultimately what is required for industrial policy to work is far less than a consistent ability to pick “winners.” In the presence of uncertainty, both about the effectiveness of policies and the location/magnitude of externalities, the ultimate test is not whether governments can pick “winners,” but whether they have (or can develop) the ability to let “losers” go. As with any portfolio decision, it would be an indication of sub-optimal policy if the government did not back some ventures that end up as failures ex post. In the U.S., Department of Energy loan guarantees to Solyndra, a solar cell manufacturer, failed miserably; but a similar loan guarantee to Tesla enabled the company to avert failure and become the behemoth it is today. In Chile, successes in four projects supported by Fundacion Chile – including most spectacularly salmon – is said to have paid the costs of all other ventures. Letting losers go may still be a hard task, in light of political pressures that inevitably develop…But it is far less demanding than governmental omniscience. There’s a fairly large literature showing a negative correlation between subsidies and productivity at the industry level. But Juhász, Lane, and Rodrik point out that while this could be because of inefficient policies and rent-seeking, it could also be that government is directing subsidies to the industries where market failures are the largest. After all, if you look at the correlation between hospital visits and deadly heart attacks, you’ll find a strong positive correlation…but this doesn’t mean that hospitals give people fatal heart attacks! The authors also warn about conflating industrial policy with protectionism, noting that recent industrial policies have often been aimed at increasing exports and international capital flows. The ascent of the East Asian Tiger economies belies a clean correspondence between protectionism and industrial policy. A whole lot of industrial policies are just rich countries trying to help their successful established exporters, and most of that assistance comes in the form of subsidies. The truth is that all of this still represents only a few tiny slivers of knowledge, and economists as a whole have only begun to scratch the surface of this incredibly important and very timely topic. But at least a few are trying. If and when the global rise of industrial policy eventually forces economists to study it en masse, Réka Juhász, Nathan Lane, and Dani Rodrik will be remembered as the pioneers who broke ground on the topic even before it was cool and sexy. Noah Smith (2023) A few economists are starting to think seriously about industrial policy Juhasz Abstract We discuss the considerable literature that has developed in recent years providing rigorous evidence on how industrial policies work. This literature is a signiﬁcant improvement over the earlier generatioon of empirical work, which was largely correlational and marred by interpretational problems. On the whole, the recent crop of papers oﬀers a more positive take on industrial policy. We review the standard rationales and critiques of industrial policy and provide a broad overview of new empirical approaches to measurement. We discuss how the recent literature, paying close atention to measurement, causal inference, and economic structure, is oﬀering a nuanced and contextual understanding of the eﬀects of industrial policy. We re-evaluate the East Asian experience with industrial policy in light of recent results. Finally, we conclude by reviewing how industrial policy is being reshaped by a new understanding of governance, a richer set of policy instruments beyond subsidies, and the reality of de- industrialization. [Juhasz (2023) The New Economics of Industrial Policy (pdf)] (pdf/Juhasz_2023_The_new_economics_of_industrial_policy.pdf) "],["inequality.html", "36 Inequality 36.1 US and UK are poor countries with rich people within 36.2 Poverty 36.3 Billionaire Concentration", " 36 Inequality Wright Stop talking about inequality, start talking about exploitation. Ian Wright (2017) The Social Architecture of Capitalism 36.1 US and UK are poor countries with rich people within Murdoch On comparison of income levels adjusting for inequality UK &amp; USA emerge as “poor societies” with rich people in them. Murdoch (2022) Twitter Thread (jburnmurdoch?) (2022) FT (paywall) Smith By focusing only on the rich and the poor, Murdoch leaves out something incredibly important — the middle class. Of course it’s important to uplift the people at the bottom of the socioeconomic ladder. But most people are not at the bottom of the ladder. That’s true by definition. And when we look at how Americans in the middle of the distribution are doing, we see that America is not a “poor society” at all — in fact, it’s one of the richest on Earth. America’s middle class has higher living standards than almost any other country’s middle class Noah Smith (2022) We’re a rich society with some very poor people. 36.2 Poverty Policy Tensor The dramatic reduction in extreme poverty hailed by the World Bank—and taken seriously by many serious people, including myself until he alerted me—just did not pass the laugh test. Specifically, researchers associated with the Bank and cognate institutions responsible for poverty alleviation have estimated that the rate of extreme poverty has declined from 25% of the world population in 2000 to around 8% by 2018. The percentage of population earning less than a dollar a day has allegedly fallen dramatically since the neoliberal counterrevolution of the late-1970s. Another index, based of the percentage of the global population that can afford to pay for basic needs like food, clothing and shelter, allegedly shows the same collapse. This simply does not compute. The global extreme poverty rate cannot be below 10%, simply because there are more people in extreme poverty in South Asia alone. According to statistics compiled by the Government of India, more than 30% of Indian kids are stunted, meaning that they are so malnutritioned that they’re significantly shorter than they should be (per height-for-age reference charts). Now, 30% of the Indian population is already 5.3% of the world population. They don’t have enough to eat, but they’re out of extreme poverty? So, the World Bank’s and OECD’s numbers don’t pass the laugh test. But the laugh test is a sanity check, not a formal proof that the poverty statistics are wrong. In what follows, we’ll marshal considerably more compelling evidence that the World Bank’s numbers don’t add up. We will document a systematic bias in the extreme poverty statistics for lower middle income countries over the past few decades. This group of nations accounts for 3.3 billion people, or 42% of the world’s population—and the vast bulk of the alleged reduction in extreme poverty. The idea for the test is very simple and intuitive: variation in poverty rates, if they are indeed kosher, should track variation in, say, life expectancy—or any other measure of general well-being of a populace. If they don’t, and do so in a persistently biased way, then something is wrong with their production process. We proceed as follows. We obtain data on extreme poverty rates from Clio-Infra compiled by Michail Moatsos and also published by the OECD. There are two metrics: (1) percent of country’s populace making less than a dollar a day; and (2) percent of the populace unable to afford basic necessities. The time period covered is 1820-2018. We shall restrict attention to the past century, when the data is much more reliable. We obtain life expectancy data from Our World in Data, which is ultimately based on UN estimates. We detrend the data by computing 5-year changes in life expectancy and the two poverty rates. We rotate the variables so that positive numbers are good (ie, we read a decline in poverty rates as positive and a rise as negative). This makes the results easier to interpret. Having detrended and rotated the variables, we carry out two panel regressions. In both, we stratify by World Bank income group. The response or dependent variable in both will be one of the two extreme poverty rates; the feature or predictor will be life expectancy. We admit both time fixed-effects and income group fixed-effects. The former is especially important because we need to rig the game against our result that the recent numbers are particularly biased. We cluster errors by income group. We have 1,797 country-period observations—more than enough for our purposes. Here’s what the data looks like: (table) Here’re the results of the first regression. We find the expected gradient (b = 0.1933, p &lt; 0.0001). (table) Here’s the main result. We look at the mean residuals by income group and time. This is contains information on the systematic bias of the poverty statistics—if any exists. If there were no systematic bias, then the residuals would be more or less random. In particular, we should not see persistent decades-long runs of over-prediction and under-prediction by our simple model. This is not what we find. Instead, we find that, for the critical lower middle income group, gains in life expectancy overpredict reductions in extreme poverty until 1975, when the precipitous decline in global rates is alleged to have begun in earnest (see the first two graphs in this note). Conversely, from 1975 on, and especially after the Millennium Development Goals were announced in 2000 to great fanfare, we find that gains in life expectancy systematically underpredict the alleged dramatic reductions in poverty rates. In other words, the dollar-a-day and cost-of-basic-needs poverty rates paint too rosy a picture of reductions in global poverty. In sum, we have shown that the World Bank’s extreme poverty statistics have been implausibly rosy for the crucial group of lower middle income countries—the principal site of the alleged dramatic reduction in global poverty—precisely in the period of the alleged dramatic reduction in extreme poverty. The chasm that has now opened up between the self-congratulatory discourse of the global development institutions and ground realities has become ridiculously large. [Policy Tensor (2023) Here’s Proof that Extreme Poverty Statistics are Unreliable](Here’s Proof that Extreme Poverty Statistics are Unreliable](https://policytensor.substack.com/p/heres-proof-that-extreme-poverty) 36.3 Billionaire Concentration Fix The billionaire headcount is determined in large part by a single quantity: a country’s average income - More money, more billionaires. Compared to poor countries, rich countries ought to have more billionaires. Fig: The concentration of Forbes billionaires across countries. This figure uses Forbes data to measure the concentration of billionaires among the world’s countries, circa 2021-2022. Each point indicates the average concentration of billionaires within the corresponding country. (Note that the horizontal axis uses a logarithmic scale.) To construct the billionaire concentration, I divide billionaire counts (measured daily in 2021/2022) by population (measured in 2021). Horizontal lines indicate the 95% range of variation in the billionaire concentration over the observed time interval. (Countries without error bars have only one billionaire observation.) Fig: As countries get richer, they accumulate billionaires. The horizontal axis shows countries’ average income in 2021, measured using GDP per capita. The vertical axis plots the number of Forbes billionaires per capita (measured in 2021–2022). Technically, GDP per capita measures a country’s average income (a flow), while the Forbes list measures billionaires’ wealth (a stock). For the über rich, income and wealth are two sides of the same coin. The capitalization ritual is based on two quantities that are undetermined. Future earnings are, by definition, unknown. And the choice of discount rate is a matter of taste. So we’re left where we started — with a capitalized value that is undefined. Not to worry. Capitalists solve the problem with customs. They agree to judge future income by looking at recent quarterly earnings. And they choose a discount rate by looking at what everyone else is doing. As a result of this herd behavior, ‘income’ and ‘wealth’ become (statistically) interchangeable. Billionaire headcount tends to increase with a country’s per capita income because income is what gets capitalized into wealth. When statistical agencies measure GDP, they capture (among other things) the annual profits of all the companies that reside in the given country. Investors, in turn, take these profits and capitalize them into market value. Finally, Forbes looks at this market value to judge the net worth of the billionaires on its list. The result is a closed loop between aggregate income and billionaire wealth. So as average income grows, countries accumulate more billionaires. Fig: The billionaire abundance ratio. The billionaire abundance ratio divides the actual billionaire density (based on Forbes data) by the expected billionaire density based on a country’s income per capita. Now in capitalism, we no longer have feudal despots. But there’s still plenty of hierarchy. (In fact, there’s more hierarchy.) And guess who sits at the top of this hierarchy. That would be business despots … otherwise known as billionaires. And if a society has a dearth of billionaires, you’d expect it to be more egalitarian. In short, the relative abundance of billionaires should be a canary for social inequality. Power Law Now in technical terms, a power-law exponent doesn’t capture ‘inequality’ so much as it quantifies the behavior of a distribution tail. At this point, I’m throwing around a lot of jargon, so let’s move down to earth by asking the following question: how many people have double your wealth? If you’re a member of the elite, we can predict how many people have double your net worth using a single parameter which we’ll call \\(alpha\\). If α=3, then people with double your wealth are 2³=8 times rarer than you. And if α=2 then people with double your wealth are \\(2^2 = 4\\) times rarer than you. And so on. Given α, people with double your wealth are \\(2^{\\alpha}\\) times rarer than you. It’s an empirical fact that among the elite, the distribution of wealth tends to follow a power law. And the properties of this power law can be summarized using a parameter called α - the exponent in the following equation: \\[ p(x)∼\\frac{1}{x^α}\\] Here, p(x) p(x) p(x) describes the probability of finding someone with net worth x. We call this relation a ‘power law’ because of its mathematical form — x raised to some power α. What’s odd about power laws is that they use grade-school math to describe complex, real-world outcomes. In the case of the US circa 2019, the power law has an exponent of α=2.3. So if you’re an American elite, someone with double your net worth is about \\(2^2.3 ≈ 4.9\\) times rarer than you. What the power-law exponent does is capture the shape of the wealth-distribution tail. A higher exponent indicates a thinner tail. And a lower exponent indicates a fatter tail. Fix (2023) Billionaires are so predictable "],["inflation-1.html", "37 Inflation 37.1 2023 like 1940s 37.2 Wage-Price Spiral 37.3 Phillips Curve 37.4 Stagflation 37.5 Inflation - Growth Tradeoff? 37.6 Fiscal Theory of Price Level 37.7 Price Control 37.8 Greedflation 37.9 Structural Reallocation 37.10 Global Inflation 37.11 Interest rate and inflation 37.12 Inflation as Redistribution 37.13 Return of Inflation 37.14 Productivity Failure behind Inflation 37.15 Energy squeeze inflation 37.16 Supply Side Pressure 37.17 Lack of Inflation Theory", " 37 Inflation Inflation is about power, not money. Contrary to the fantasy world of neoclassical economics — in which businesses ‘take’ prices from the market — in the real world, prices are always ‘set’. mainstream economists believe that elevated inflation is a result of “excess” aggregate demand. 37.1 2023 like 1940s Weber Weber (2023) tweet 37.2 Wage-Price Spiral Roberts A general rise in the rate of wages will result in a fall of the general rate of profit, but not affect the prices of commodities. In other words, wage rises are much more likely to lower the share of income going to profits and thus eventually lower the profitability of capital. And that is the reason capitalists and their economist prize-fighters oppose wage rises. The claim that there is a wage-price spiral and that wage rises cause price rises is an ideological smokescreen to protect profitability. The IMF has compiled a comprehensive data analysis (pdf) of the movement of wage and price rises that refutes Bailey and Furman. The IMF “address these questions by creating an empirical definition of a wage-price spiral and applying this on a cross-economy database of past episodes among advanced economies going back to the 1960s.” So over 60 years and in many countries. What did the IMF find: “Wage-price spirals, at least defined as a sustained acceleration of prices and wages, are hard to find in the recent historical record. Also, there appears to be no inverse correlation between changes in wages, prices and unemployment – this classic Keynesian Phillips curve that claimed this relation has been shown to be false. And the latest empirical estimates show the Phillips curve to be broadly flat – in other words, there is no correlation between wages, prices and unemployment. No wage-price spiral. Capitalists want ‘wage restraint’ in the face of spiralling inflation in order to protect and sustain profits. The real aim of interest-rate hikes is not to stop a wage-price spiral but to raise unemployment and weaken the bargaining power of labour. Alan Budd, then chief economic adviser to British PM Margaret Thatcher in the 1980s: “There may have been people making the actual policy decisions… who never believed for a moment that this was the correct way to bring down inflation. They did, however, see that [monetarism] would be a very, very good way to raise unemployment, and raising unemployment was an extremely desirable way of reducing the strength of the working classes.” Roberts (2022) The wage price spiral refuted 37.3 Phillips Curve Roberts Gavyn Davies, former chief economist at Goldman Sachs, once explained why the theory that inflation is caused by wage rises persists even though it has been discredited theoretically and empirically. Davies: “without the Phillips Curve, the whole complicated paraphernalia that underpins central bank policy suddenly looks very shaky. For this reason, the Phillips Curve will not be abandoned lightly by policy makers”. Neoliberal ascendancy, deregulation has allowed corporations to amass pricing power. The sharp rise in the prices of non-labor inputs that were “the likely culprits for the acceleration in inflation”. They rose because of the shutdown of key suppliers during COVID in China and other developing countries and from the loss of electronic components supply that went into the production of consumers goods and because the supply chain system was broken with the collapse of the just in time inventory methods over the last four decades. Prices in oligopolistic markets are likely to be higher than in more competitive markets “but it is not the case that this can explain the continuous rise in prices; that would require a change in the competitive conditions, something that is not clearly taken place in the last two years.” Higher inflation can occur both with fairly competitive or oligopolistic market structures. In the late 19th century, the so-called Gilded Age Era was characterized by the rise of cartels, but with deflation in prices; and the 1990s, often seen as a second Gilded Age with increasing market concentration, experienced a so-called Great Moderation in price inflation ie disinflation. Indeed, in the last big inflationary spiral of the 1970s, profits actually fell. According to Sylos-Labini, wiring then: “the decline of the share of profits in several capitalist countries can be attributed primarily to the persistent increase of direct costs in labor, raw materials, and energy”. This contradicts views according to which: “Companies with enough market power can also unilaterally raise prices in a quest for greater and greater profits” as MMT economist, Stephanie Kelton has argued. It all depends on the point in the cycle of expansion and contraction that a capitalist economy is undergoing, not on the ability of monopolies to ‘price gouge’ as such. The persistence of contractionary demand, mostly monetary, policy as the main tool to contain inflation seems to respond more to the prevailing prejudices and the ideological biases of the profession, than to the analysis of the real causes of inflation. However, it is not helpful that the main challenge to this consensus has been to blame corporations for increasing their profit margins, since this view also provides an incorrect explanation for the recent acceleration of inflation. The main culprit for the inflationary acceleration in the U.S. and most advanced economies is related to the supply side snags, and the shock to energy and food prices resulting from the pandemic and the war in the Ukraine. Michael Roberts (2023) 37.4 Stagflation Napier Stagflation is the combination of high inflation and high unemployment. That’s not what we have today, as we have record low unemployment. You get stagflation after years of badly misallocated capital, which tends to happen when the government interferes for too long in the allocation of capital. Napier (2022) We Will See the Return of Capital Investment on a Massive Scale Fix Inflation in the midst of stagnation is not an anomaly. If anything, it is the general rule. Stagflation is a business strategy The idea is that ‘stagflation’ — economic stagnation combined with high inflation — is not some exogenous ‘market shock’. According to Nitzan and Bichler, stagflation is a business strategy — one of two main routes to profit. The first route to profit is for businesses to hold prices steady while they try to sell more stuff. The second route is to jack up prices. Since this latter option requires restricting the flow of resources (stuff that flows freely cannot be dear), Nitzan and Bichler reason that when inflation rears its head, it ought to come with economic stagnation. In other words, stagflation is the norm. If this stagflation thesis is correct, then inflation ought to correlate negatively with economic growth. Looking at the United States, Nitzan and Bichler find evidence that it does. Here, I broaden their stagflation research by looking at all countries in the World Bank’s global development database. I find that both within and across countries, economic growth (measured in terms of energy use) tends to decline as inflation increases. So Nitzan and Bichler appear to be onto something. Over the last half century, stagflation is the general rule. The reason is that to many people, the word ‘inflation’ implies a decrease in the purchasing power of money. Although not wrong, the problem with this interpretation is that it is needlessly indirect. Framing inflation in terms of decreasing purchasing power is like discussing your child’s growth in terms of the ‘shrinking height capacity’ of your doors. Sure, it’s true in a sense. But it is also tediously circuitous. The fact remains that it is your child (not your doors) who changes. The same is true of inflation. When inflation rears its head, money appears to lose its value. But the reality is that it is prices (not the nature of money) that change. So if we want to understand the phenomenon of ‘inflation’, we should study prices directly. As a social species, humans have an intense desire to conform to social norms. Sometimes this desire leads to stability — as with religious traditions that last for centuries. Other times, though, conformity leads to social change. Fashion is a good example of both tendencies. As individuals, we like to dress the same as other people, leading to a (relative) uniformity in our attire. And yet over time, fashion changes — a herd behavior in which people conform to the new way of dressing. And so we get coordination (changing fashion) through conformity. Something similar happens with businesses. Often, businesses compete by cutting costs and increasing the amount of stuff they sell. But this is not the only mode of competition. Sometimes, a business raises its prices and its competitor responds by doing the same. If enough businesses join in, suddenly we have a herd behavior in which every business is attempting to raise prices. Coordination through conformity. Inflation! When inflation rears its head, it should be accompanied by economic stagnation — a combination that economists call ‘stagflation’. You can’t look at the scale of inflation variation (Figure Inflation by countries) and claim it all boils down to ‘supply-chain problems’. To predict whether a country will have high (or low) inflation, we need only rank its per capita income. If the country is poor, inflation will be high. But if the country is rich, inflation will be low. As energy growth rates increase, inflation tends to decline. Or put another way, energy stagnation is associated with greater inflation. Stagflation! Fix (2023) Is Stagflation the Norm? 37.5 Inflation - Growth Tradeoff? Fix According to standard theory, there is a trade off between low inflation and high economic growth. The idea is that you can have one or the other, but not both. So if you want to keep inflation low, you have to ‘cool off’ the economy by slowing economic growth. (Like many things in economics, this idea comes from the totem of supply and demand.) The trouble is, the empirical evidence shows that the opposite is true. Rather than being driven by ‘excessive’ economic growth, inflation tends to come during periods of stagnation. So despite what mainstream economists proclaim, there is little evidence for a ‘growth-inflation trade off’. Instead, ‘stagflation’ seems to be the norm. Soon after I published ‘Is Stagflation the Norm?’ several readers pointed out that I should take a look at causation. The idea is that we want to know what drives what. Does (low) inflation drive (high) economic growth? Or does (low) economic growth drive (high) inflation? Business sabotage, plain and simple. It’s inflation through enforced scarcity. Maintaining high prices requires restriction. Musk turned on the inflation dial by charging for blue checkmarks. De Beers (a diamond cartel) spent years buying up diamonds to purposefully keep them off the market. Like all savvy businesses, De Beers knew that enforced scarcity (aka sabotage) was the key to high prices. Nitzan and Bichler argue that this enforced scarcity tends to come in waves, largely because it is unstable. Neoclassical economists look at these dynamics and conclude that they will lead to market equilibrium. But that’s because economists suppose that businesses won’t coordinate. In the real world, though, businesses coordinate all the time. It’s called herd behavior. If the herd decides to restrict supply and hike prices, I’d best join in. The result will be an oscillation between periods of economic boom with low inflation, and periods of economic bust with high inflation. Across countries, economic growth (as measured by energy consumption) tends to be high when inflation is low (and vice versa). Causation What causes what? Does high inflation cause low economic growth? Or does low economic growth cause high inflation? If the sabotage thesis is correct, then the latter should be true. Purposeful restriction (by business) should lead to higher prices. While causation is difficult to establish, it is easier to rule out. That’s because if a hypothesized ‘cause’ comes after the hypothesized ‘effect’, the hypothesis is wrong. One way to test for causation (or to be more precise, eliminate non-causation) is to take time-series data and introduce leads and/or lags. So instead of correlating data observed in the same year, we correlate data in adjacent years. Inflation is measured in terms of the change in the consumer price index. I use the growth rate of energy consumption per capita as a measure of economic growth. Energy growth rates are a decent predictor of next-year’s inflation rates. But the reverse is not true. Price gouging is preceded by a period of energy restriction. What’s causing this restriction, though, is still unknown. For their part, mainstream economists will be happy to look at energy restriction and identify ‘exogenous shocks’ to the market. But by ‘exogenous’, economists really mean a cause that they don’t care to think about. War is a good example. For mainstream economists, war is simply not part of their theory. But for Nitzan and Bichler, war is the most extreme form of sabotage, frequently associated with price gouging and profiteering. In particular, the (differential) profitability of oil companies seems to be tightly related to war in the Middle East. On that front, it’s worth concluding with some history. If you lived through the 1970s, you’ll remember it as a period of rampant inflation. But do you recall the chain of events that led up to the crisis? Here’s what happened. In October 1973, Egypt and Syria attacked Israeli-occupied territory in the Sinai Peninsula and the Golan Heights. The United State rushed to back Israel, while the Soviet union rushed to back its Arab allies. As retribution, in December 1973, the Arab-dominated OPEC cartel announced an oil embargo against the United States. Oil prices skyrocketed. Generalized inflation soon followed. Now we can quibble about the details, but the point is that nothing in this story fits the standard theory of inflation. There was no growth-inflation trade off. Instead, there was war, followed by an embargo imposed by a business cartel, followed by energy shortages (in the United States), followed by rampant inflation. That pretty much fits the sabotage thesis advanced by Nitzan and Bichler. Interestingly, the wider evidence that I’ve reviewed here suggests that the last two steps of this causal chain are quite general — energy restriction precedes inflation. The harder part is to then determine what drives short-term energy restriction. As is the norm, empirical evidence answers some questions, but leads to others. I’m fine with that because it’s better than the alternative (namely, to stop asking questions and take Econ rituals as received wisdom). Fix (2023) The Cause of Stagflation 37.6 Fiscal Theory of Price Level Cochrane Abstract I introduce and summarize the fiscal theory of the price level. Fiscal theory states that the price level adjusts so that the real value of government debt equals the present value of real primary surpluses. Monetary policy remains important. The central bank can set an interest rate target, which determines expected inflation, and then innovations to the present value of surpluses pick unexpected inflation. Fiscal theory is a frictionless supply and demand foundation, on which we can add interesting in- gredients. Long-term debt is an important buffer and allows a higher interest rate to lower inflation without a fiscal shock. An s-shaped surplus process and time-varying interest rate are crucial to fitting data. One can easily integrate fiscal theory with standard new-Keynesian macroeconomic models. The models are observationally equivalent. That equivalence is a feature not a bug. It opens the door to easy transla- tion. It focuses our attention on direct information about government policy rather than statistical tests. It shows how to fix the current generation of fiscal theory mod- els to describe the whole sample, and better, not just periods of undesirable high inflation. Fiscal theory overturns many traditional doctrines of monetary policy. It accounts for the stability of inflation at the zero bound. Fiscal theory offers a warn- ing that containing a new inflation will be harder, as interest costs on a large debt and the fiscal costs of debt revaluation will be larger. Cochrane Memo Cochrane (2021) The Fiscal Theory of the Price Level: An Introduction and Overview (pdf) Smith on Ghana Case Ghana’s inflation, however, doesn’t appear to be due to an attempt to pay off external debt by printing local currency. In fact, the central bank has been raising interest rates very fast in order to fight the inflation from food and fuel prices, as well as in a (failed) attempt to halt currency depreciation. So this is not the typical “print money to pay off foreign debt” story we’re seeing. Higher interest rates, unfortunately, exacerbate the government’s debt burden. Ghana is not a country that can borrow cheaply in the first place, and global interest rates have been going up, so the central bank’s rate hikes have just added fuel to the fire. Interest on the government debt is now absorbing 70% of tax revenues, which is crowding out anything else useful the government might try to do, and which is obviously unsustainable. The huge debt burden, in turn, probably led to soaring inflation. Ghanaian businesspeople looked at the mountain of government debt and decided that eventually the government would reverse its pattern of rate hikes and resort to printing a ton of money to pay off its debt. So they got in ahead of the game and started raising prices, which made inflation a self-fulfilling prophecy. Economists call this kind of thing the “fiscal theory of the price level”. Noah Smith (2022) Ghana you were doing so well 37.7 Price Control Weber A critical factor that is driving up prices remains largely overlooked: an explosion in profits. Large corporations with market power have used supply problems as an opportunity to increase prices and scoop windfall profits. Today economists are divided into two camps on the inflation question: team Transitory argues we ought not to worry about inflation since it will soon go away. Team Stagflation urges for fiscal restraint and a raise in interest rates. But there is a third option: the government could target the specific prices that drive inflation instead of moving to austerity which risks a recession. As long as bottlenecks make it impossible for supply to meet demand, price controls for important goods should be continued to prevent prices from shooting up. The role of price controls would be “strategic”. It will not stop inflation, but it gains the time for the measures that do. Weber (2021) We have a powerful weapon to fight inflation: price controls. It’s time we consider it Krugman I am not a free-market zealot. But this is truly stupid. Krugman on Weber (Twitter Thread) Paul Krugman (paulkrugman?) Deleting, with extreme apologies, my tweet about Isabella Weber on price controls. No excuses. It’s always wrong to use that tone against anyone arguing in good faith, no matter how much you disagree — especially when there’s so much bad faith out there. Comment by Jonathan McCarty: (paulkrugman?) is against price controls for consumer goods, but supportive when it comes to the cost of labor, rent and money. So gov’t is too stupid to set the price of bread but smart enough to set the price of money? Bread shortages are not okay, but housing shortages are? Galbraith In The Guardian, Weber provides careful parallels to the spring of 1946, when Paul Samuelson – Krugman’s own chief mentor – signed a letter to The New York Times urging continued price controls, given ongoing bottlenecks and temporary shortages – precisely today’s situation… The point of strategic price control, then and now, was to prevent an outbreak of inflation, followed by loss of purchasing power and confidence. A further purpose now, not relevant yet in 1946, is to forestall counterproductive hikes in interest rates by the Federal Reserve… Krugman’s tweets, by contrast, are the trite repetition of textbook banalities. Kelton on behalf of Galbraith(Twitter Thread Galbraith 2001 So what is modern economics about? It seems to be, mainly, about itself Thirty years ago, Friedman-style monetarists wiped out all alternative theories of inflation. The ideas of “cost push” and “wage-price spirals,” on which the successful anti-inflation strategies of the 1960s had been based, disappeared. To this day, there exist no alternatives for fighting inflation, except higher interest rates, recession, and unemployment. These are the hard measures, the brutal measures, for which we have the monetarists to thank. Galbraith (2001) How the Economists Got It Wrong Smith Price controls: Simple theory If there’s one thing you should know about macroeconomics, it’s this: Convincing evidence is really really hard to come by, so people end up relying a lot on theory and making a lot of assumptions. Price controls are no different. So we can’t just point at evidence for whether price controls are good or bad; we have to think about how we believe the economy works. The basic theory of competitive supply and demand says that price ceilings cause shortages. Here’s the graph showing the theoretical gap between how much people want and how much they get when government caps the price of something: The basic logic here isn’t complicated. Government declares that milk shall be super-cheap. People say “Oooh, milk is super cheap!” and rush out to buy milk. The shelves empty out and there’s no more milk. The people who were late to the store can’t find any milk, and they get mad. The end. But this perfectly competitive model is often a bad description of reality. Sometimes, as we’ve seen with minimum wage, price controls don’t distort markets by a noticeable amount. In that case, the model we want to think about is more like a monopoly model. When there’s monopoly power in the economy, a price ceiling can actually move the price toward what it ought to be, and relieve shortages instead of exacerbating them. Monopolies make goods more expensive and limit the amount people can consume; a modest price ceiling can make goods less expensive while also making them more abundant. But does this make any sense when talking about inflation? Monopoly models like the one in the picture above are static, long-term equilibrium models; they don’t say much about the rate of change. It’s probably not plausible that monopoly power would change significantly in the course of one year due to supply bottlenecks. In other words, as Matt Bruenig points out, if powerful companies could have jacked up prices before now they would have done so; if their ability to jack up prices has increased, it’s probably not because they’ve suddenly become much more powerful. An economy with lots of monopoly power in various markets might have steeper supply curves in those markets, which in turn might make aggregate supply steeper, which would make inflation tend to be higher. But if this is how the economy works, would price controls in various markets reduce inflation at a time like now? Probably not, no. Go back and notice that in the monopoly model, the price ceiling doesn’t actually change the supply curve. Even if there are monopolies in each market, that doesn’t mean the macroeconomy overall acts like a monopolized market. There’s no one company that has a monopoly over aggregate production. So price controls, macroeconomically, are likely to reduce inflation only at the cost of causing a recession. That would be a bad idea; sure, we’d beat inflation, but we’d throw a ton of people out of work. If that’s what we want to do, we might as well use monetary policy. Simple theory suggests that that enacting economy-wide price controls just to bring inflation back down to 2% is not worth the damage it’ll cause. But simple theories like AD-AS aren’t always sufficient for determining policy. Real macroeconomies have a lot more going on. One possibility is that if price controls do cause empty shelves — as they will if they’re strong enough to overcome the amount of monopoly power in the economy — that this will cause people to engage in hoarding behavior. Hoarding could be especially bad. It would boost demand (because everyone is trying to hoard), which will lead to even more inflation, causing the government to respond with even more price controls, etc. That would be a very unpleasant spiral, even beyond the hardships and unfairness created by hoarding. This possibility of a price-control-inflation spiral has occurred to economists, but it’s very hard to measure. Many economists theorize that inflation is, at least sometimes, determined by people’s beliefs about monetary policy. If people think the government (especially the central bank) doesn’t care that much about fighting inflation, then they’ll raise prices now in anticipation of future cost increases, causing fear of inflation to become a self-fulfilling prophecy. This is one leading explanation for the high inflation of the 1970s — the oil shocks caused some prices to rise and the Fed didn’t respond, which convinced people that the Fed didn’t care that much about inflation, which caused inflation to spiral upward much more than the oil shocks should have caused just by themselves. So if price controls became the government’s primary tool for inflation-fighting — as Kelton suggests — it could send a very dangerous signal. It could convince the public that the government isn’t willing to use monetary policy to do the job. Evidence Argentina: price controls have only a small and temporary effect on inflation that reverses itself as soon as the controls are lifted. Second, contrary to common beliefs, we find that controlled goods are consistently available for sale. Third, firms compensate for price controls by introducing new product varieties at higher prices, thereby increasing price dispersion within narrow categories. Overall, our results show that targeted price controls are just as ineffective as more traditional forms of price controls in reducing aggregate inflation. Evicence Venezuela The utter failure of Venezuelan price controls should also serve as a reminder that there are real-world factors that don’t appear in macroeconomic models — for example, the black market. In the U.S., a vigorous, comprehensive regime of price controls would undoubtedly cause people to turn to cryptocurrency, and to technologies like the dark web, to evade the controls. History is hard to interpret, theory involves lots of assumptions, and macroeconomists have been largely derelict in their duty of studying inflation in recent decades (though I predict this will change quickly now). There are multiple obvious downsides and potentially catastrophic possible downsides. We don’t know for certain that price controls can’t work as an inflation-fighting tool. Smith (2021) Why price controls are a bad tool for fighting inflation Tooze As Eric Levitz makes clear in his excellent write-up of the debate, whether you find Weber’s op-ed convincing or not, there is a serious position to be argued with. The effort to assert the monopoly of conventional inflation-fighting disarms us. One could make a strong case for more stringent controls throughout the American health-care system. And price controls are themselves just one of many unorthodox approaches to inflation management. Reducing the monopoly power of price-gouging firms, channeling credit to sectors where demand outstrips supply, forcing (or strongly encouraging) workers to save a fraction of their paychecks, and direct public investment in expanded production are others. All of these measures have the potential for negative side effects and unintended consequences. But the same can be said of raising interest rates. If policymakers reflexively presume the wisdom of conventional tools, and dismiss the potential of unorthodox ones, we will all pay the price. I am impressed by recent BIS work which shows the common factor in recent price movements declining in significance. The question becomes which instruments might usefully address which drivers of which price increases. Weber starts by stressing rising profit margins as an important driver of general inflation. On that score I find the critique by BLS-economist and Substacker Joseph Politano wholly persuasive. It just isn’t likely that a general surge in profit margins is doing the damage here. Likewise, I find Politano’s breakdown of the sectoral logic of inflation highly persuasive as well as his skepticism towards price controls as a means of addressing inflation in energy prices, for instance. There is no doubt a case for driving down the price of pharmaceuticals in the US. Rent controls may be part of housing-policy trade-off in some cities. The meat lobby has an anti-trust case to answer. But I see little advantage in packing an array of discrete measures using existing instruments under the (deliberately) provocative rubric of “price controls”. I don’t think it is pejorative to describe the use of the term “price controls” as provocative. I take it to be the purpose of this language to provoke debate and break open the confines of conventional discourse. But as desirable as that kind of heterodox challenge may be in general terms, we will be kidding ourselves if we imagine that such measures are a “powerful weapon” to fight the spike in prices in 2022. Tooze (2022) Inflation &amp; Price Controls 37.8 Greedflation Fix big business is systematically benefiting from inflation, it implies that these big corporations are raising prices faster than everyone else. In other words, it is oligopolies that are driving inflation. Figure: Inflation benefits big business Inflation looks nothing like it does in economics textbooks. Yes, inflation is a ‘monetary phenomenon’ — as is anything to do with prices. But more importantly, inflation is a power struggle over who can raise prices the fastest. Fix (2022) The Truth About Inflation: Why Milton Friedman Was Wrong, Again 37.9 Structural Reallocation Guerreri Abstract We characterize optimal monetary policy in response to asymmetric shocks that shift demand from one sector to another, a condition arguably faced by many economies emerging from the Covid-19 crisis. We show that the asymmetry manifests itself as an endogenous cost-push shock, breaking divine coincidence, and resulting in inflation optimally exceeding its target despite elevated unemployment. In fact, there is no simple, possibly re-weighted, inflation index that can be used as the optimal target. When labor is mobile between sectors, monetary easing can have the additional benefit of inducing faster reallocation, by producing wage increases in the expanding sector. Guerreri Memo Uneven shocks pose important challenges to policy, given that different sectors can suffer from opposite problems. How should monetary policy respond to this type of situation? Is the optimal response to target economy-wide average measures of inflation and of the output gap, or do the asymmetries across sectors require a deviation from standard recommendations, in one direction or another? We address here how monetary policy interacts with the process of sectoral reallocation. When uneven shocks have a persistent nature, a natural concern is that the economy should readjust by moving productive resources from declining sectors, that suffer from insufficient demand, towards growing sectors where demand is expanding. Excessively easy monetary policy may hamper the reallocation process. Monetary policy must balance various goals. The macroeconomic literature on optimal monetary policy has developed insights into navigating these goals. An influential and celebrated idea provides conditions under which inflation targeting can obtain both price and employment stability—as in some situations there is no trade- off, and we have the so-called “divine coincidence.” It is well appreciated that we may have to depart from this benchmark. This paper explores scenarios that fall quite some distance away from divine coincidence. We build a stylized model that departs from workhorse macroeconomic models in important ways, incorporating realistic features such as multiple sectors, downward wage rigidities and costly labor reallocation. We then consider a reallocation shock and study optimal monetary policy. Monetary policy should not only be concerned with average inflation and the average output gap, but also with getting relative prices across sectors close to their frictionless level, so as to reduce inefficiencies in the composition of output. We build our analysis in a model with downward rigid nominal wages, which introduces non-linear Phillips curves at the sectoral level. The main implication of this difference, is that to get relative prices right it is easier to get inflation in the expanding sectors than to get deflation in the contracting sectors, imparting an inflationary bias to optimal policy. The main question of our paper which is whether reallocation objectives impart a contractionary or expansionary bias to monetary policy. Does the presence of sectoral mobility add a negative or a positive social benefit to increasing M? Tools that encourage labor mobility (or remove obstacles to mobility), allow the central bank, all else equal, to achieve a better mix of inflation and unemployment. Guerreri Conclusion The paper has explored the optimal conduct of monetary policy in the presence of asym- metric shocks, which can cause a permanent reallocation of resources among sectors. Asymmetric shocks require adjustments in relative prices across sectors, and in the presence of downward nominal rigidity, this may lead to a more expansionary monetary policy being optimal. Moreover, when labor can move across sectors, households do not internalize the benefits of labor reallocation towards the booming sectors, and incentivizing reallocation is desirable. Does easier monetary policy speed up or slow down such reallocation? We presented examples where both are possible. If the dominant effect of easier monetary policy is to improve employment prospects in the declining sector, reallocation tends to be slowed down; if instead easier monetary policy has sufficiently powerful effects on relative wages, reallocation is accelerated. An investigation into which of these two forces is empirically stronger, is a promising avenue for further research. Guerreri (2021) Monetary Policy in Times of Structural Reallocation (pdf) Krugman on Guerreri 37.10 Global Inflation PolicyTensor We’re shown previously that inflation in advanced open economies is not a function of domestic slack. Instead, it is a function of slack in the global production system servicing the Western consumer as a whole. We revisit the empirical evidence and ask whether there’s any reason to do a Bayesian update of our model of the inflation process. Fig: We obtain data on inflation and unemployment from the IMF. We restrict the sample to twenty advanced economies because EM inflation rates are confounded by unanchored inflation expectations. We isolate the global factor in inflation from our AE panel using latent factor analysis. As is clear from the graph, the global factor closely tracks median inflation in advanced economies. We can think of it as the rate of inflation in the global tradable sector jointly faced by all advanced economies. The global factor thus contains a very strong signal of slack in the global production system as a whole. We expect inflation in the advanced economies to be more sensitive to the global factor than domestic slack. Fig: We fit panel regressions with CPI as the response, and the global factor and domestic slack as features. We proxy domestic slack by the unemployment rate. We detrend by first-differencing and standardize all variables to have zero mean and unit variance. The slope coefficients therefore represent effect sizes measured in standard deviation units. In all our regressions, we control for the price of crude, dollar strength, country fixed-effects, and lagged CPI. Country fixed-effects and the persistence term are included but not shown. Our estimates show that AE inflation is a function of global slack, not domestic slack. The elasticity of inflation in an advanced economy against domestic slack is statistically indistinguishable from zero at the 10 percent level (P = 0.254). Meanwhile, the elasticity of inflation against the global factor is very large and statistically significant at the 1 percent level. n order to test the hypothesis of recent changes in the inflation process, we divide the data into pre-Covid and post-Covid samples, and compare elasticity estimates. We find that AE inflation has become more sensitive to the global factor since Covid, not less. Our point estimates suggest that the global factor is still six times as important as domestic slack in the determination of AE inflation. Blanchard may be “one of Europe’s most prominent economists” but he is simply mistaken about the inflation process. It’s hard to see how evidence against the Phillips curve theory of inflation can ever get him to lose his religion. But one poorly-informed technocrat is not so important. What we should be worried about is broader intellectual regression among economists. Inflation could come down rapidly in 2023, stay stubbornly high, or even start climbing again. Given the substantial uncertainty around the inflation outlook, it is extremely important for central bankers to refrain from imposing their theoretical priors on the data. Monetary policymakers should not base their expectations of the future path of inflation on the very poor signal in the tightness of domestic labor markets. Policytensor (2023) Blanchard is still wrong 37.11 Interest rate and inflation Fix In his article ‘Do Interest Rate Hikes Worsen Inflation?’ Tim Di Muzio claims that there is good reason for monetary orthodoxy to be wrong. The problem boils down to the ceteris paribus clause — the assumption that when we raise interest rates, nothing else changes. To restate orthodox reasoning, if I have a fixed budget to spend on servicing my debt, then it follows that when interest rates rise, I’ll borrow less money. But what if my debt-servicing budget is not fixed? Then orthodoxy breaks down. In the real world, Di Muzio observes, businesses don’t need to reduce borrowing in the face of higher interest rates. Why? Because when interest expenses increase, businesses can respond by trying to raise their income. In other words, businesses can maintain their debt levels by passing their greater debt-servicing costs along to customers. Let’s lay out the consequences of this heretical thinking. If businesses practice ‘cost-plus’ pricing — meaning they tack a fixed markup onto their current costs — then raising interest rates ought to stimulate inflation. How should we interpret the fact that higher interest rates are associated with higher inflation? The least painful option is to suppose that monetary policy is well-intentioned yet toothless. In other words, policy-makers consistently respond to higher inflation with higher interest rates. And yet equally consistently, these rate hikes fail to do their job. A more incendiary possibility is that monetary orthodoxy does the opposite of what it intends. As heretic Tim Di Muzio observes, if businesses practice cost-plus pricing (tacking a fixed markup onto existing costs), then higher interest rates should actually stoke inflation. Blasphemy, yes. But the idea is supported by the evidence. Monetary orthodoxy is true by definition. Interest rates everywhere and always down-regulate inflation. It’s just that in our imperfect world, there are ubiquitous distortions that hide this truth. By definition, monetary policy works the way it should. It’s just that we can never observe this canonical outcome, for it is hidden by a barrage of distortions. And yet we have faith. We have faith that the plane of economic truth is there, waiting to be imagined. Fix (2023) Do high interest rates reduce inflation 37.12 Inflation as Redistribution Bichler and Nitzan There is the much broader question of whether we should think of inflation as a stand-alone variable in the economist’s arsenal, or as a process that is deeply interlaced with and shapes the political-economic space. The former, conventional view sees various concepts and processes such as ‘technology’, ‘GDP’, ‘growth’, ‘income’, ‘government’, ‘capital’ and ‘inflation’ as distinct entities that interact with each other in an otherwise independent Newtonian space called ‘the market’. The latter view, which CasP prefers, is that the social space is not Newtonian, but Leibnitzian. It does not exist on its own, but rather is defined, bent and transformed by the entities that comprise it. Seen from this latter perspective, inflation is not separate from the strategic sabotage and stagnation that causes it or the constant redistribution of income and assets it engenders. Taken together, these conceptual quandaries, missing data and the perception of inflation as part and parcel of the changing structure of society, make the study of inflationary redistribution truly daunting. To engage in such research, you must come up with thoughtful simplifications that do not sacrifice reality, create clever constructs and indices to transcend the vast voids of missing data and rethink the ways in which price changes creorder the very structure of society. Bichler and Nitzan (2023) Inflation as Redistribution (pdf) (pdfbucket) 37.13 Return of Inflation Vernengo The Great Moderation, characterized by increasing stability in prices lasting from around 1980s to the recent inflation acceleration, and the ideological victory of Monetarism in the 1970s, led to a certain theoretical complacency, and a view of inflation as being fundamentally related to excess demand, a positive output gap, and the notion that central banks could manage the inflation expectations and target it at around 2 percent in advanced economies. The long period of price stability has been seen, by the mainstream, as resulting mainly from the good practices of central banks which included a clear mandate to maintain price stability as the hierarchical goal of monetary policy, political and operational (instrument) independence, in particular the adoption of inflation targeting, and the required credibility and persistence “to counter inflation psychology and anchor inflation expectations at a low level” (Bernanke, 2022: 43). In addition, structural reforms, and the spread of the market friendly policies of the Washington Consensus led to responsible fiscal policies, and these coupled with independent and inflation conscious central banks explained, in this view, the relatively low levels of inflation. The reality underpinning the Great Moderation are more complex. Most likely structural causes including falling commodity prices and reduced power of labor unions due to the recessions of the 1980s, including the debt crisis in the developing world, and the 1990s, the rise and expansion of globalization and the large increase in the supply of low-wage workers from China, East Asia and Eastern Europe into global markets. The challenge to conventional wisdom, and its emphasis on demand, has come from left of center authors, like Robert Reich, that suggest inflation is caused by greedy corporations that have increased their profit margins during a crisis. This has brought back the old debate about the relationship between administered prices and inflation, and the proposition that inflation is directly related to highly concentrated market structures, or what might be termed oligopolistic inflation. There is an ideological divide between those that blame inflation in an incompetent government and central bank reaction to the pandemic versus those that suggest that the real culprits are greedy corporations rising their mark up above their costs. While supply side factors are central for inflationary pressures, and while it is true that in advanced economies higher interest rates might not have a significant impact in the control of prices, the same is not true in peripheral countries. Central banks in peripheral countries reacted more promptly and with greater intensity to the rise in inflation than did developed central banks. The rise in interest rates in peripheral economies was aimed to a great extent at reducing the impact of depreciating national currencies on inflation and to reduce the possibility of capital outflows. If the risk in the center has been associated with an overreaction of central banks, and excessively contractionary monetary policy, in the periphery the risks are associated with a mild reaction and the possible inflationary impacts of depreciating currencies. The paper is divided in three sections, discussing the limitations of demand- pull and oligopolistic inflation in the center in the following one, an analysis of inflationary factors in some peripheral countries, particularly Latin American ones, and a brief conclusion: The return of inflation as a central macroeconomic problem after almost forty years has taken place at a time in which the belief in the self-adjusting nature of the economy has been under questioning, if not by the economic profession, at least by society at large. The notion that the economy has a strong tendency to be close to full employment should be at the top of the ideas to be debunked. A second one to be discredited should be the notion that inflation is a monetary phenomenon. If the economy is only rarely at full employment, it should be clear that inflation seldom is caused by excess demand. Surprisingly, most economists would agree that the economy is close to full employment in the U.S. and that inflation is essentially caused by the overreaction of government and excessive spending and monetary expansion during the pandemic. The conventional narrative about the so-called Great Inflation of the 1970s is, however, incredibly persistent and prevalent. In fact, the very term Great Inflation is supposed to suggest that this was a crisis of the same proportion as the Great Depression, and to some extent justified the neoliberal turn in economic policies around the globe. The persistence of contractionary demand, mostly monetary, policy as the main tool to contain inflation seems to respond more to the prevailing prejudices and the ideological biases of the profession, than to the analysis of the real causes of inflation. It is not helpful that the main challenge to this consensus has been to blame corporations for increasing their profit margins, since this view also provides an incorrect explanation for the recent acceleration of inflation. The main culprit for the inflationary acceleration in the U.S. and most advanced economies is related to the supply side snags, and the shock to energy and food prices resulting from the pandemic and the war in the Ukraine. The main effect of inflation is distributive, and its cost is the fall of real wages, or even more precisely of the wages of the groups at the bottom of the income distribution. Inflation has limited effects on growth even at relatively high levels Vernengo (2023) Return of Inflation (pdf) 37.14 Productivity Failure behind Inflation Roberts What is missing from all this [debate] is what caused inflation to rise in the first place and why it stays ’sticky’. The recovery in output globally has been weak since the end of the pandemic. Growth in the productivity of labour (output per worker) has been low. Indeed in value terms (ie hours of work) supply has been flat or falling. As a result any increase in spending or credit has ended up adding to price inflation. But nobody mentions that it is the failure of capitalist accumulation to boost the productivity of labour (and value creation) [that is behind inflation]; instead the argument is about whether labour or capital should take the hit; or whether inflation should be allowed to stay high or be driven down despite the risk of slump. The BoE data above reveal that the lower is productivity growth, the higher is the sticky’ core inflation rate. And as the BIS also said above, inflation won’t come down without a slump unless productivity growth rises sharply. The reason the US labour market is ‘tight’ is not because the economy is expanding at a fast rate and delivering well-paid jobs for all. It is because so many skilled people of working age have left the labour market since the pandemic. Also immigration, a key driver of labour supply has diminished as many countries apply yet more restrictions. And so far, AI technology is not delivering faster productivity growth from the existing workforce. Why is productivity growth not appearing? It’s because investment in technology is not picking up; instead companies prefer to find cheap labour even from a ‘tight’ labour market. Why is investment not picking up? It ’s because the profitability of capital is still low and has not seen any significant shift up – outside of the small group of mega companies in energy, food and tech. And while US real GDP has risen, that is not reflected in domestic income growth. There is a significant divergence between the gross domestic product (GDP) and gross domestic income (GDI). That divergence is due to both wages and profits (after inflation) falling. So, on a GDI basis, the US economy is already in recession. The next recession would not be triggered by a housing bust or a stock market bust, or even a financial crash, but by increasing corporate debt costs, driving sections of the corporate sector into bankruptcy – namely ‘fallen angels’ and ‘zombie companies’. Corporate debt is still at record highs and whereas the cost of servicing that debt was comfortable for most due to low interest rates, that Is no longer the case. Roberts (2023) From greedflation to stagflation to slumpflation 37.15 Energy squeeze inflation Tverberg Repayment of debt with interest acts like a Ponzi Scheme if there is inadequate growth in the energy supply. Most people today do not realize the extent to which the entire financial system is dependent on growing inexpensive-to-produce energy supply of the right kinds. It takes physical resources of the right kinds to produce goods and services. Resources such as fresh water, copper, lithium, and fossil fuels require more and more energy consumption to produce the same amount of supply because the easiest-to-extract resources are extracted first. When the economy is far from limits, adding more debt (or other types of promises, such as shares of stock) does seem to increase “demand” for finished goods and services, and this, in turn, tends to increase the production of fossil fuels and other commodities. Thus, for a while, increased debt does indeed increase energy supply. But when we start reaching extraction limits, instead of producing more fossil fuels and other commodities, higher debt tends to produce inflation. (In other words, more money plus practically the same amount of finished goods and services tends to lead to inflation.) This is the issue central banks are up against today. Central banks raise interest rates in response to the higher level of inflation, partly to compensate lenders for the inflation that is taking place, and partly to make their own economies more competitive in the world economy. The combination of higher interest rates and higher inflation is problematic in many ways: Ordinary citizens find that they must cut back on discretionary goods and services to balance their budgets. This tends to push economies in the direction of recession and debt defaults. Some citizens find they need to apply for government assistance programs for the first time. Businesses find it more difficult to operate profitably with higher interest rates and inflation. Businesses increasingly expand in programs supported by government subsidies, such as those for electric cars and batteries, as it becomes increasingly difficult to make a profit without a subsidy. In the US, defaults seem especially likely on commercial real estate loans. Governments become especially squeezed. Many of them find that their own tax revenue is falling at precisely the time when citizens need their programs most. Governments also find that with higher interest rates, interest costs on their own debt rises. Subsidized programs increasingly seem to be needed to keep the economy operating. The number of retirees also grows year after year. Government debt levels spiral upward, as shown for the US on Figure 6. With all these issues, the world becomes increasingly prone to war. Political parties, and even groups within political parties, find it increasingly difficult to agree on solutions to problems. The stage seems to be set for an array of worrisome outcomes, including major debt defaults, failing governments, and even widespread war. Tverberg (2023) Today’s energy bottleneck may bring down major governments 37.16 Supply Side Pressure Roberts Inflation rate is down, not really because of anything done by central banks, namely hiking interest rates to squeeze ‘aggregate demand’ and thus slow spending growth. The main reason for the slowing inflation rate is the fallback in energy and food prices – in other words, key raw material and commodity prices. Marxist theory suggests that there is tendency for prices to fall over time as productivity growth rises and costs per unit of production fall. Indeed, before the pandemic, durable-goods prices fell an average of 1.9% a year from 1995 to 2020 as globalisation shifted US manufacturing production to low-wage countries and productivity improvements lowered costs. But if production is disrupted, then the opposite will occur. Post-pandemic, product shortages snarled supply chains and a surge in demand from consumers flush with cash sent prices soaring in 2021 and 2022. Durable-goods inflation peaked at a 47-year high of 10.7% in February 2022. Research by Adam Shapiro, an economist at the San Francisco Fed, finds that supply disruptions such as those from closed factories or shipping backups accounted for roughly half the run-up in inflation in 2021 and 2022. Today, supply chains are running smoothly, according to a New York Fed index. Supply chains, either alone or interacted with slack, explain most of the excess core inflation from 2021-23. The continued levels of high interest rates imposed by the Federal Reserve did not get inflation rates down and won’t do so from hereon. So what central banks call ‘going the last mile’ to reach the Fed’s arbitrary 2% a year rate is not likely to be achieved by Fed policies. Indeed, there are signs that inflation may start to rise again. Roberts (2023) Goldilocks and the last mile 37.17 Lack of Inflation Theory Tooze So it certainly hasn’t been a glorious period for the Phillips curve theory in any of its forms. But I’m not sure we have a satisfactory alternative theory. The theory to which many economists are gravitating to is that the Phillips curve is basically flat, inflation is set by inflation expectations, and inflation expectations are set by the people who form inflation expectations. And that’s a little bit like the theory that the planets go around the universe because of the orbital force. It’s kind of a naming theory rather than an actual theory. So I think inflation theory is in very substantial disarray, both because of the Phillips curve problems and because we don’t have a hugely convincing successor to monetarist-type theory. Monetarist theory had an idea that the price level had to do with the quantity of paper versus the quantity of goods. But now that money pays interest, what the nominal quantity is, that is divided by a real quantity and sets the price level, is unclear. We know from extreme examples, like the monetary history of Argentina, that in some contexts a theory about the price level and nominal quantity of money becomes the right theory for thinking about inflation. But how one thinks about that in the context of relatively low inflation environments, I think economics is embarrassingly short on clear, operational theories. While I have Keynesian policy instincts, I have long been pretty unconvinced by new Keynesian models as an intricate structure. Tooze (2023) Larry Summers on the Phillips curve "],["innovation.html", "38 Innovation 38.1 US vs Scandinavia", " 38 Innovation Innovation is related to the manufacturing floor, it can’t recover if you’re losing that. 38.1 US vs Scandinavia Smith on Acemoglu In 2012, Daron Acemoglu, James Robinson, and Thierry Verdier came out with a paper about the different “varieties of capitalism”. The basic idea was that Scandinavia’s more safety net discouraged entrepreneurship, while America’s relative lack of government support forced people to be risk-takers, and that this explained America’s greater rate of innovation. This is the kind of theory economists tend to like, because it emphasizes tradeoffs, and because it tells a story that allows economists to place themselves in the political center, charting the optimal middle path between the kind-hearted Democrats who want to give out free stuff and the exacting Republicans who want to force people to work for their supper. But other economists and bloggers immediately started noting problems with the thesis — most importantly, the fact that the Nordic countries are generally more innovative than the U.S. by many measures. Those countries are small, so you don’t hear about their innovations as much, but they really punch above their weight. Acemoglu et al. were trying to explain a “fact” that didn’t really exist. Smith (2021) Cutthroat capitalism vs. cuddly capitalism "],["interest-rate.html", "39 Interest Rate 39.1 Natural Rate of Interest 39.2 Unpayable debt in a Stationary Economy", " 39 Interest Rate What debt does, no more and no less, is to establish a contractual agreement to tie an allocation of resources in the present to a mirroring reallocation of resources in the future. If they cannot be paid, the one thing we know about debts is that they should be written off. [Tooze (2022) Chartbook #181: Finance and the polycrisis (6): Africa’s debt crisis](https://adamtooze.substack.com/p/finance-and-the-polycrisis-6-africas 39.1 Natural Rate of Interest Roberts Indeed, ‘managing’ an anarchic capitalist economy is not easy – indeed impossible. Even worse, the navigation guide that Powell and mainstream economics are trying to use is the so-called ‘neutral policy rate’ that supposedly tells economists when demand and supply; or more accurately, aggregate savings and investment, are in balance.But this r*, as it named, is a nonsense concept that comes from the neoclassical equilibrium economics of Kurt Wicksell.. Many studies have shown up the myth of this theory. It’s less an astronomic navigation tool and more the astrology of the zodiac. Nevertheless, Powell referred to this ‘natural rate of interest’ theory as his policy foundation, but then dissed it by saying “we cannot identify with certainty the neutral rate of interest, and that assessment is further complicated by uncertainty about the duration of the lags with which monetary tightening affects economic activity and especially inflation.” Yes, indeed. Roberts (2023) Navigating by the stars under cloudy skies – and holed below the water line 39.1.1 From R* to r Roberts The Fed has been following a monetary policy theory that there is some ‘equilibrium’ rate of interest that can be identified that would be appropriate for an economy to be back at trend growth and full employment without serious inflation. The Fed calls this (imaginary) rate, R. This idea is based on the theory of the neo-classical economist Kurt Wicksell. The trouble is that it is nonsense – there is no equilibrium rate. Even worse, the Fed’s economists have no idea what it should be anyway. In their latest projection, they reckon R is anywhere between 1% and 5% for two years ahead, with a best guess at about 2%. The current Fed rate is 0.5%. R* is not really anywhere near as high as the Fed economists think. The major economies are in a state of ‘secular stagnation’ caused by ageing, slowing productivity growth, falling prices of investment goods, reductions in public investment, rising inequality, the “global savings glut” and shifting preferences for less risky assets. If we recognise that R* is really low, then we can adopt the policy of handing out cash to companies and individuals directly and combine that with more public spending (with larger government budget deficits) on investment projects. These answers are really an admission of the failure of monetarism and monetary policy. The capitalist economy does not respond to injections of money (or, for that matter, injections of government spending) but to the profitability of investment. The rate of profit on capital invested is the best indicator for investment and growth, not the rate of interest on borrowing. It is r, not R*, that matters. Roberts (2016) From R* to r 39.2 Unpayable debt in a Stationary Economy Hartley Abstract Under what circumstances are interest-bearing loans compatible with an economy without much growth? The question is becoming increasingly important given a tendency towards declining growth in industrialised economies and increasing evidence that continued growth is incompatible with environmental sustainability. Previous theoretical work suggests that when interest-bearing loans compound, this results in exponentially growing debts that are impossible to repay in the absence of economic growth. We here examine ten historical cases to assess support for this finding. We find that interest-bearing loans have typically resulted in unpayable debts in these non- and slow-growing economies. We further identify four broad category of measures to prevent or alleviate the problem of unpayable debts, and show how they have been employed in the past. Our Appendix compiles sources of debt regulation from across the world over five millennia. Hartley Memo Compound interest debt-based money is incompatible with a stationary economy but interest bearing debt-based money does not necessarily imply compound interest. Positive interest rates do not systematically lead to exponentially growing deposits, because taxation and consumption out of wealth and income can dampen the positive feedback loop of compound interest. Our starting point for this paper, then, is the longstanding body of literature which suggests that when interest compounds it can result in exponentially growing debts that are unpayable in the absence of economic growth. This body of theory has been developed to analyse modern economies, with the particular aim of better understanding what may happen if today’s economies stop growing. Rome, for example, had significant levels of financial intermediation and credit creation, with one recent comparative analysis concluding “that financial institutions in the early Roman Empire were better than those of eighteenth- century France and Holland. They were similar to those in eighteenth-century London and probably better than those available elsewhere in England” What particularly motivates us here is a desire to un­ derstand the consequences of positive interest in the absence of growth, and also to shed light on how these societies tried to mitigate the potential negative effects of interest-bearing loans. The charging of interest in the absence of substantial economic growth was accompanied by notable levels of unpayable debt in seven out of our ten cases. In these seven cases, there is evidence that in different periods this resulted in debtor dispossession and indenture, and at least some degree of social upheaval or revolt. The more extended lending is, the more individual problems of indebtedness are likely to translate into a bigger social problem. One might also argue that if lending markets worked efficiently, real interest rates in modern economies should converge towards the rate of real economic activity, which would suggest that real interest rates in a non-growing economy should tend towards zero. Current theories that suggest interest-bearing loans may become problematic in the absence of substantial growth have significant empirical support when tested against historical cases. Hartley and Kallis (2021) Interest-bearing loans and unpayable debts in slow-growing economies: Insights from ten historical cases (pdf) (SI pdf) "],["investment.html", "40 Investment", " 40 Investment “Now, capitalists do many things as a class, but they certainly do not invest as a class’ (Kalecki) "],["knowledge-economy---intangibles.html", "41 Knowledge Economy - Intangibles", " 41 Knowledge Economy - Intangibles Roberts There is a new book out called Capitalism without capital – the rise of the intangible economy. The authors, by Jonathan Haskel of Imperial College and Stian Westlake of Nesta, are out to emphasise a big change in the nature of modern capital accumulation – namely that increasingly investment by large and small companies is not in what are called tangible assets, machines, factories, offices etc but in ‘intangibles’, research and development, software, databases, branding and design. This is where investment is rising fast relative to investment in material items. The authors call this capitalism without capital. But of course, this is using ‘capital’ in its physicalist sense, not as a mode of production and social relation, as Marxist theory uses the word. For Marxist theory what matters is the exploitive relation between the owners of the means of production (tangible and intangible) and the producers of value, whether they are manual or ‘mental’ workers. As G Carchedi has explained, there is no fundamental distinction between manual and mental labour in explaining exploitation under capitalism. Capitalism cannot be without capital in that sense. Knowledge is produced by mental labour but this is not ultimately different from manual labour. Both entail expenditure of human energy. The human brain, we are told, consumes 20% of all the energy we derive from nourishment and the development of knowledge in the brain produces material changes in the nervous system and synaptic changes which can be measured. Once the material nature of knowledge is established, the material nature of mental work follows. Productive labour (whether manual or mental) transforms existing use-values into new use-values (realised in exchange value). Mental labour is labour transforming mental use values into new mental use values. Manual labour consists of objective transformations of the world outside us; mental labour of transformations of our perception and knowledge of that world. But both are material. The point is that discoveries, generally now made by teams of mental workers, are appropriated by capital and controlled by patents, by intellectual property or similar means. Production of knowledge is directed towards profit. And they reckon this is changing the nature of modern capitalism. Indeed, it could expose the uselessness of the so-called market economy. The argument is that an intangible asset (like a piece of software) can be used over and over again at low cost and allow a business to grow very fast. That’s an exaggeration, of course, because tangible assets like machines can also be used over again, but it’s true that they have ‘wear and tear’ and depreciation. But then software also gets out of date and also becomes ‘tired’ for the continually changing purposes required. Indeed, the ‘moral depreciation’ of intangibles is probably even greater than tangibles and so increases the contradictions of capitalist accumulation. For an individual capitalist, protecting profit gained from a new piece of research or software, or the branding of a company, becomes much more difficult when software can easily be replicated and brands copied. Capitalism is continually facing a dynamic tension between the underlying forces of competition and monopoly. That’s why companies are keen on intellectual property rights (IPR). But IPR is actually inefficient in developing production. ‘Spillover’, as the authors call it, where the benefit of any new discovery is shared in the community, is more productive, but by definition almost, is only possible outside capitalism and private profit – in other words rather than capitalism without capital; it becomes capital without capitalism. As Martin Wolf of the FT concludes in his analysis of the rise of ‘intangibles’, “intangibles exhibit synergies. This goes against the spillovers. Synergies encourage inter-firm co-operation (or outright mergers), while spillovers are likely to discourage it. Who really wants to give a free lunch to competitors?” So “Taken together, these features explain two other core features of the intangible economy: uncertainty and contestedness. The market economy ceases to function in the familiar ways.” Under capitalism, the rise of intangible investment is leading to increased inequality between capitalists. The leading companies are controlling the development of ideas, research and design and blocking ‘spillover’ to others. The FANGs are gaining monopoly rents as a result, but at the expense of the profitability of others. Indeed, the control of intangibles by a small number of mega companies could well be weakening the ability to find new ideas and develop them. we have the position where the new leading sectors are increasingly investing in intangibles while investment overall falls along with productivity and profitability. Marx’s law of profitability is not modified but intensified. The rise of intangibles means the increased concentration and centralisation of capital. Capital without capitalism becomes a socialist imperative. Roberts (2017) Capitalism without capital – or capital without capitalism? "],["phillips-curve-1.html", "42 Phillips Curve 42.1 Unemployment and interest rates", " 42 Phillips Curve Ratner Abstract Is the Phillips curve dead? If so, who killed it? Conventional wisdom has it that the sound monetary policy since the 1980s not only conquered the Great Inflation, but also buried the Phillips curve itself. This paper provides an alternative explanation: labor market policies that have eroded worker bargaining power might have been the source of the demise of the Phillips curve. We develop what we call the “Kaleckian Phillips curve”, the slope of which is determined by the bargaining power of trade unions. We show that a nearly 90 percent reduction in inflation volatility is possible even without any changes in monetary policy when the economy transitions from equal shares of power between workers and firms to a new balance in which firms dominate. In addition, we show that the decline of trade union power reduces the share of monopoly rents appropriated by workers, and thus helps explain the secular decline of labor share, and the rise of profit share. We provide time series and cross sectional evidence. Ratner (2022) Who killed the Phillips Curve (pdf) (pdf Slides) Seccareccia on Rattner A repeat of the anti-inflation policy scenario of the early 1980s of sharply raising central bank interest rates might prove inappropriate, if not catastrophic, as solution to dealing with the current inflationary environment. Seccareccia (2022) The Fed Tackles Kalecki 42.1 Unemployment and interest rates Fix Prior to 1970, there was essentially no connection between unemployment and bond yields. But from 1980 to the late 2000s, there was a one-to-one connection. In other words, just as bond yields started to move with unemployment, economists began to connect unemployment with the rate of interest. Given economists’ penchant for reactionary fads, it seems plausible that the interest-rate-unemployment nexus is not a general truth. Instead, it may have been a theoretical reaction to a transient period in US history. Fig: The rise of the interest-rate-unemployment nexus. This figure traces the connection between interest rates and unemployment to a particular moment in US history. Panel A shows the frequency of the phrase ‘interest rates and unemployment’ in the Google English corpus. (I’ve included in this measurement the frequency for the conjugate phrase ‘unemployment and interest rates’.) The phrase exploded in popularity during the 1980s. Around the same time, there was a shift in how US interest rates related to unemployment. Panel B quantifies this shift by plotting the slope of the trailing 30-year regression between unemployment and the US bond yield. When this slope is zero, bond yields don’t respond to unemployment. But when this slope is one, bond yields show a one-to-one response to unemployment. When it comes to monetary policy, economists have been taught that the effects come with lags that are ‘long and variable’. So if I don’t do a lag analysis, I’ll get an endless stream of requests to ‘lag the data’. Let me preempt that torture. Fig: Changes in the US bond yield predict jumps in next-year’s unemployment. The relation between changes in the US bond yield (horizontal axis) and changes in next-year’s unemployment (vertical axis). The red line shows the smoothed trend. The red shaded region shows the associated uncertainty in the trend, which is significant. The caveat here is that the lagged trend is produced in large part by a few outlier years, all of which are in the late 1970s and the early 1980s. It’s dubious to take a pattern from that decade and pronounce it a ‘general tendency’. Things get worse when we realize that a lagged effect doesn’t mean much on its own. That’s because when we’re dealing with cyclical data, we’ll inevitably find that an observation today predicts an observation later. n the case of a one year lag, it’s fairly easy to understand what we’ll observe. In the US, unemployment oscillates with a roughly 8-year cycle. In that context, a one year lag represents an eighth of a cycle. If we do the math, we find that an uptick in this year’s unemployment should be followed by another uptick next year. In other words, changes in unemployment this year ought to correlate positively with unemployment changes next year. And indeed they do. I have to admit that I find these results disappointing. Although I’ve learned to take economists’ pontifications with a boulder of salt, my intuition was that interest rates would connect with unemployment And yet the evidence suggests otherwise. That said, there are ways to connect interest income to unemployment — ways that are better supported by evidence. In my next post, I’ll discuss Jonathan Nitzan and Shimshon Bichler’s concept of the ‘maturity of capitalism’, which pits interest income against profit income. It turns out that unlike interest rates, the interest-to-profit income ratio is related to unemployment. The point is that measurements must take into account the social order that they are quantifying. On that front, if we divide society into the ‘employed’ and the ‘unemployed’, we’re excluding a third category: the shittily employed. As big corporations increasingly turn to ‘flexible’ labor to do their bidding, we’d best pay attention to this third category. Fix (2023) Interest Rates and Unemployment: An Underwhelming Relation "],["productivity.html", "43 Productivity 43.1 Productivity-Pay Gap 43.2 IPR Stagnation 43.3 TFP", " 43 Productivity 43.1 Productivity-Pay Gap Using prices to aggregate ‘output’ leads to bizarre problems. On the one hand, it causes ‘productivity’ to be equivalent to average hourly income. This means that any connection between ‘productivity’ and wages is circular. On the other hand, the same decision causes ‘productivity’ to be ambiguous. Our measure of ‘productivity’ depends on arbitrary choices about how to adjust for price change. As a result, productivity trends (like the one in Figure 1) are riddled with uncertainty. ‘Productivity’ is used by both major schools of economic thought. Neoclassical economists use productivity to claim that the distribution of income is just. They argue that in a competitive economy, workers get what they produce. Marxists, in contrast, use productivity to claim that the distribution of income is unjust. They argue that in a capitalist economy, workers receive less than they produce (because capitalists extract a surplus). What’s interesting is that these two opposing theories commit the same sin. They define productivity in terms of income. Neoclassical economists do so explicitly, as I’ve described in this post. Marxists do so implicitly because they haven’t developed their own system of national accounts. Instead, Marxists who do empirical work use neoclassical measures of productivity. The result of this circular definition is that the analysis of productivity is a sleight of hand. ‘Productivity’ is just income relabelled. The ‘productivity-pay gap’ is a textbook example of this relabelling. It claims to show a growing gap between what workers ‘produce’ and what they get paid. But workers’ ‘productivity’ is actually measured in terms of income — the average hourly income. Blair Fix: Debunking Productivity Blair Fix: Productivity does not explain income Productive individuals, productive society? In the 1990s, geneticist William Muir conducted experiments on chickens to see what would improve egg-laying productivity. In one trial, he did exactly what the eugenicists recommend – he let only the most productive hens reproduce. The results were disastrous. Egg-laying productivity did not increase. It plummeted. Why? Because the resulting breed of hens was psychopathic. Instead of producing eggs, these “uber-hens” fought amongst themselves, sometimes to the death. The reason this experiment did not work is that egg-laying productivity is not an isolated property of the individual hen. It is a joint property of the hen and her social environment. In Muir’s experiment, the most productive hens laid more eggs not because they were innately more productive, but because they suppressed the productivity of less dominant chickens. By selecting for individual productivity, Muir had inadvertently bred for social dominance. The result was a breed of bully chicken that could not tolerate others. The lesson here is that in social animals, traits that can be measured among individuals (like productivity) may not actually be traits of the individual. Instead, they are joint traits of both the individual and their social environment. Here is evolutionary biologist David Sloan Wilson reflecting on this fact: “Muir’s experiments … challenge what it means for a trait to be regarded as an individual trait. If by ‘individual trait’ we mean a trait that can be measured in an individual, then egg productivity in hens qualifies. You just count the number of eggs that emerge from the hind end of a hen. If by “individual trait” we mean the process that resulted in the trait, then egg productivity in hens does not qualify. Instead, it is a social trait that depends not only on the properties of the individual hen but also on the properties of the hen’s social environment”. Blair Fix: Human Capital Theory RWER95 (pdf) 43.2 IPR Stagnation Schwartz Abstract Explanations for slow global growth (secular stagnation) correctly focus on income inequality and wage formation but are incomplete. They ignore the source of wages and fail to ask why a rising profit share has not produced more investment. Older but essential insights on stagnation from Keynes, Schumpeter and Veblen complement orthodox and post-Keynesian analyses to generate a more robust explanation based on the distributional conflict over profit among firms. These thinkers highlight the importance of corporate profit strategy and organizational structure for investment behavior. A politically mediated process of strategic interaction has transformed the old Fordist dual industrial structure into a tripartite structure composed of high profit volume firms with monopolies based on intellectual property rights (IPRs), physical capital-intensive firms protected by an investment barrier to entry, and low profit volume labor-intensive firms. Profit data from Compustat and Orbis show that IPR-based firms have a lower marginal propensity to invest. Other firms with smaller profit volumes forego investment from fear of creating excess capacity in a slow growth environment. High profit firms also tend to pay higher wages, creating income inequality. Changes in antitrust, employment and intellectual property law can remedy this situation. Schwartz (2021) Global secular stagnation and the rise of intellectual property monopoly (Paywall) 43.3 TFP Each period of productivity growth is bit slower than the last, meaning that the exponential growth rate is slowing down. Adjusting for the changing utilitization of capital and labor the growth rate of “true” TFP has stagnated even more than the above graph would suggest. In other words, we’ve been using our machines and buildings and stuff more intensively, disguising some of the true TFP slowdown. TFP is not the same thing as technology. The word “technology”, as we commonly understand it, includes stuff like computer chips, car engines, and procedures for making cement. Economists would broaden that definition to include things like business management techniques. But even with that broad definition, there’s plenty of stuff that can affect TFP that most of us would agree does not represent actual technology. For example: If the government adds a bunch of burdensome regulations or taxes, that reduces TFP. If people’s education level stops increasing, that lowers TFP growth. If the population gets older, that can reduce TFP (since older workers are, on average, less productive). If people spend more time goofing off at work, that can reduce measured TFP (since we’re overestimating the amount of labor input). If people stop moving from less productive places to more productive places (for example, if housing restrictions drive up rents and keep workers away from superstar cities), that reduces TFP. If a few big companies become more dominant, that can lower TFP, either via monopoly/monopsony power, or just by reducing dynamism in the economy. If demand shifts from sectors where technology is progressing rapidly (for example, manufacturing) to sectors where it’s progressing slowly (e.g. services), that can reduce TFP growth, even if the rate of technological innovation in each sector remains exactly the same. Basically we’re seeing a whole lot of things happen that tend to reduce TFP growth but that have nothing to do with slowing technological progress! We can invent economically useful stuff just as brilliantly as in the past, but if the above stuff happens, TFP will still slow down. In fact, in a recent book called “Fully Grown: Why a Stagnant Economy Is a Sign of Success”, the brilliant growth economist Dietrich Vollrath — whose excellent blog you should absolutely read — argues that most of the slowdown in TFP comes from slowing educational attainment, lower geographic mobility and economic dynamism, and and the shift from goods to services. Tyler Cowen’s 2011 book The Great Stagnation (the most subtle and circumspect of the stagnationist books), he identifies non-technological factors as contributing to the stagnation, and he predicts that both technology and productivity growth will bounce back. Stagnationists would be well-advised to read that book. Noah Smith "],["economic-regulation.html", "44 Economic Regulation 44.1 Climate protection impact on economic growth", " 44 Economic Regulation 44.1 Climate protection impact on economic growth Mudge (This article is part of a series in which DW is debunking myths surrounding climate change. Read also: Part 1 — Is global warming merely a natural cycle? Part 2 — Is half a degree of warming really such a big deal? Part 3 — Is China the main climate change culprit? Part 4 — Climate protection: Can I make a difference? ) The first major environmental protection rules hark back to the 1970s. Since then, a debate has raged about their potentially damaging impact on economic growth and competitiveness. One train of thought (Dechezleprêtre (2017)) says countries that adhere less stringently to environmental policies have a production and trade advantage over those nations that are taking climate action measures to reduce emissions. The concern in those countries is that their own emission-heavy industries will be put at a competitive disadvantage. This so-called pollution haven hypothesis predicts that if competing companies diverge only regarding the severity of environmental regulations they face, then those that are bound by relatively stricter measures will lose competitiveness. On the other hand, the so-called Porter hypothesis concludes that more stringent climate rules should encourage investment in developing new pollution-saving technologies. If these technologies lead to energy savings, they may help in turn to offset some of the climate protection costs. Then, there is also the issue of how much it might cost if we fail to mitigate climate impacts. Is GDP the only valid indicator? At first glance, using GDP as a measurement tool is an obvious choice to provide a cost-benefit analysis. The question is to what extent it provides an adequate measure of growth and prosperity. “It is the most developed indicator. I wouldn’t say that we should move away from that, but many of these damages that are associated with climate change are not internalized. This means that we as a global society will probably have costs due to lost biodiversity, for example, which are not directly reflected in the GDP,” Wilfried Rickels, director of the Global Commons and Climate Research Center at the Kiel Institute for the World Economy, told DW. Figure: From OECD - Obviously this is once again Nordhaus unvalid calculations (DH) Environmental protection itself contributes to economic growth As modern economies move toward a so-called resource-efficient and circular economy (RE-CE), there are concerns that — in the short term, at least — jobs will be lost across various sectors of the economy and that job creation will be minimal. However, an OECD report notes that it is important to distinguish between different sectors. Most jobs over the next two decades are projected to be created within the construction industry, and renewable power generation and services; while manufacturing sectors, agriculture, food production and fossil-fuel based power are expected to record job losses. The overriding question is how to balance economic growth with cutting carbon emissions, and ultimately, achieving climate neutrality. At this year’s World Economic Forum in Davos, Johan Rockström, director of the Potsdam Institute for Climate Impact Research, pointed to that contradiction. “It’s difficult to see if the current GDP-based model of economic growth can go hand in hand with rapid cutting of emissions,” he said. Mudge (2021) Does climate protection stifle economic growth? (Deutsche Welle) Dechezleprêtre Ever since the first major environmental regulations were enacted in the 1970s, there has been much debate about their potential impacts on the competitiveness of affected firms. Businesses and policy makers fear that in a world that is increasingly characterized by the integration of trade and capital flows, large asymmetries in the stringency of environmental policies could shift pollution-intensive production capacity toward countries or regions with less stringent regula- tion, altering the spatial distribution of industrial production and the subsequent international trade flows. This has caused concern, particularly among countries that are leading the action against climate change, because their efforts to achieve deep emission reductions could put their own pollution-intensive producers at a competitive disadvantage in the global economy. There are two different views in the environmental economics literature on the effects of asymmetric policies on the performance of companies competing in the same market: the pollution haven hypothesis and the Porter hypothesis. The pollution haven hypothesis, which is based on trade theory, predicts that more stringent environmental policies will increase compliance costs and, over time, shift pollution-intensive production toward low abatement cost regions, creating pollution havens and causing policy-induced pollution leakage. This is a particularly troubling problem for global pollutants such as carbon dioxide, because it means that on top of the economic impacts on domestic firms, abatement efforts will be offset to some extent by increasing emissions in other regions. In contrast, the Porter hypothesis (Porter and van der Linde (1995) Toward a new conception of the environment–competitiveness relationship. Journal of Economic Perspectives 9(4):97–118 ) argues that more stringent environmental policies can actually have a net positive effect on the competitiveness of regulated firms because such policies promote cost-cutting efficiency improvements, which in turn reduce or completely offset regulatory costs, and foster innovation in new technologies that may help firms achieve international technological leadership and expand market share. Some 20 years ago, in their review of the literature on the competitiveness impacts of envi- ronmental regulation in the United States, Jaffe et al. (1995) concluded that “there is relatively little evidence to support the hypothesis that environmental regulations have had a large adverse effect on competitiveness.” Since then, through hundreds of studies that have used ever larger datasets with increasingly fine levels of disaggregation, employing up-to-date econometric techniques, and covering a wider set of countries, this conclusion has only become more robust. This article has reviewed the recent empirical literature on the impacts of environmental regulations on firms’ competitiveness, as measured by trade, industry location, employment, productivity, and innovation. The cost burden of environmental policies has often been found to be very small. The recent evidence shows that taking the lead in implementing ambitious environmental policies can lead to small, statistically significant adverse effects on trade, employment, plant location, and productivity in the short run, particularly in pollu- tion- and energy-intensive sectors. However, the scale of these impacts is small compared with other determinants of trade and investment location choices such as transport costs, proximity to demand, quality of local workers, availability of raw materials, sunk capital costs, and agglomeration. Moreover, the effects tend to be concentrated on a subset of sectors for which environmental and energy regulatory costs are significant—a small group of basic industrial sectors characterized by very energy-intensive production processes, limited ability to fully pass through pollution abatement costs to consumers (whether due to regulation or international competition), and a lack of innovation and investment capacity to advance new production processes (Sato et al., 2015a). For these subsectors, where pollution leakage and competitiveness issues represent a genuine risk, a critical avenue for future research is to assess and evaluate the various policy options available to prevent adverse impacts on trade and investment without dampening the incentives to develop cleaner processes and products. This article has also shown that there is strong evidence that environmental regulations induce innovation activity in cleaner technologies. Thus far the benefits from these innova- tions do not appear to be large enough to outweigh the costs of regulations for the regulated entities. Of course, this does not preclude the ability of environmental regulations to foster the development of global leaders in innovation, but it does suggest that the evidence for the most controversial interpretation of the Porter hypothesis (i.e., that environmental regula- tions can lead to an increase in firms’ competitiveness) is lacking. As regulatory designs and combinations continue to be explored, further research will be needed to identify the com- binations of research and development and environmental policies that best encourage in- novation in green technologies. This review raises the question of why the effects of environmental regulations on inter- national industry relocation have been found to be so small and narrow given the strong concerns about competitiveness in public policy circles. One explanation could be that reg- ulated companies have an incentive to overstate the potential competitiveness impacts of regulations as a strategy to lobby against stringent policies by attributing unpopular off- shoring decisions to public policy rather than to underlying economic factors such as the shifting locus of supply and demand in global manufacturing or decreasing transport costs. An alternative explanation for the lack of empirical support for the large pollution haven effects discussed in the literature is that environmental policy is endogenous, i.e., governments strategically set stringency levels to be low (high) where there is a high (low) risk of competitiveness distortions. This argument suggests that competitiveness concerns could trigger a “race to the bottom” in global environmental protection efforts. To avoid such an outcome, further research is needed to accurately measure and monitor the competitiveness effects of environmental regulations to help ensure that policy is based on robust evidence. Dechezleprêtre (2017) The Impacts of Environmental Regulations on Competitiveness (pdf) "],["savings.html", "45 Savings", " 45 Savings Stropoli While many economists think more saving leads to productive investment, Sufi, Princeton’s Atif Mian, and Harvard’s Ludwig Straub make a different argument. They find that these savings are largely unproductive, being remade by the financial system into household and government debt. And their research outlines a cycle whereby the savings of the top 1 percent fuel the debt and dissavings of the lower 90 percent, which in turn leads to more savings at the top. From the 1980s through 2007, the top 1 percent financed a large portion of the overall rise in household debt for the lower 90 percent, according to the researchers. And as the rich have accumulated capital, the less wealthy have accumulated fewer assets, which means they experience less financial stability overall. Thus, the work argues, the savings glut of the rich, and its role in financing unproductive debt and dissavings of the nonrich, leads to instability not only for the less economically privileged but also for the broad economy. Mian and Sufi argue in 2018 research that a rapid flow of foreign funds into the US triggered a credit-supply expansion that boosted household debt, which they say was a major factor in igniting the financial crisis. From 1982 to 2016, the glut of the US rich was, on average, 60–75 percent of the size of the global glut. And at times in the 1990s and 2010s, the amount rich Americans put away even exceeded the global glut. Credit Suisse’s 2020 wealth report finds that the US has about 20 million millionaires, 40 percent of the global total. Meanwhile, the Billionaire Census 2020 from Wealth-X, which provides information and insight on the world’s wealthiest individuals, finds the US has about 28 percent of the world’s billionaires, who hold a 36 percent share of global billionaire wealth. The world now has a record 2,755 billionaires, according to Forbes. The top 1 percent of households in the US have just as much influence as emerging-market economies in fueling the debt of the bottom 90 percent. More savings, less investment Ideally, all those savings would be channeled into productive investments such as research and development, or practical equipment, or new roads, or even new yachts—investments that would promote growth in the economy. However, from 2000 through 2016, the average annual savings of the top 1 percent exceeded average annual net domestic investment as a percentage of GDP. While the rich saved more, investment in productive assets declined. Those savings were put to use financing both household and government debt. Between 2000 and 2016, they find that claims on household and government debt account for nearly two-thirds of the rise in asset accumulation of the top 1 percent in the US. In the 25 years leading up to the 2008–09 financial crisis the top 1 percent financed almost a third of the rise in household debt owed by the bottom 90 percent. In the years since the crisis, and since the housing bubble burst, the savings of the rich have gone more toward government debt Not all household and government debt is unproductive, of course. More than one entrepreneur has financed a startup on a credit card or with a personal loan. However, much household debt goes toward instruments such as mortgages and home equity loans, which can be used speculatively, in which case they are less productive than, say, investments in manufacturing plants or technology. Thus, the researchers argue that mortgages, while enabling homeownership, can also help perpetuate a cycle of wealth inequality. The rich are seeking returns on their excess savings because, as Sufi says, many of them “just cannot spend all the money they make.” The US government, by providing tax breaks on debt interest, and by encouraging banks to lend via debt financing, promotes less-productive investment Nonfinancial corporations have increased their holdings of money market funds and time deposits by 10 percentage points of national income since 1995. Since the early 2000s, the amount of corporate saving not invested in new capital has increasingly accumulated as cash. Say a corporation issues equity to the wealthy, but instead of spending the proceeds on research or equipment, puts that money into a time deposit at a bank, which in turn uses it to fund a mortgage for a less-affluent household. This is how the rich become lenders. Some politicians, economists, and pundits say that people are borrowing (and consuming) irresponsibly. But banks with all that cash on hand work to expand the credit market and realize returns on the savings glut of the rich. The bottom 90 percent are being convinced to borrow more and more, through lower interest rates, easier credit, and more advertising. From the 1980s through 2007, the net amount of household debt that the top 1 percent held as a financial asset rose by 15 percentage points of national income, while at the same time the amount of household debt that the bottom 90 percent owed as a liability rose by 40 percentage points. The so-called accumulated dissavings of the bottom 90 percent from 1983 to 2015, relative to the average level from 1973 to 1982, was over twice the national income, the researchers say. The debt trap As the savings of the rich go toward the borrowing of the nonrich, there may be a GDP boost in the short run, as it does encourage consumption. But the debt becomes a drag on future demand. The cycle of unproductive debt makes it hard for consumer demand to support full employment in the economy, and it ultimately forces central banks to lower interest rates. While lower rates may strengthen demand for a time, consistently low rates may be problematic. Persistent low demand can foster a high-debt liquidity trap—or debt trap—in which economies are stuck in long periods of sluggish growth. Stropoli (2021) How the 1 percent’s savings buried the middle class in debt Inequality Wealth among the superrich has been fueled by a marriage of in-demand skills, globalization, and technology—the combination of which are allowing businesses to scale up as never before. Skills, say many economists, are critical to the modern economy. As the US economy grows, jobs are going unfilled as companies scramble to find skilled people to hire. There’s a flip side to this: as certain skills have become scarce, this has raised the amount companies are willing to pay people who have them. The situation has similarly raised the amount of profits skilled company owners can make, and technology and globalization are further magnifying the value of in-demand skills. If this is true, the 0.01 percent are most likely benefiting from what economists call “skill-biased technological change”—the increasing return on certain skills in an economy driven by technology and globalization Under this well-established theory, a shortage of in-demand skills raises the value of those skills in rapidly expanding markets, and new technology helps some workers’ productivity grow much more than others’, exacerbating inequality. In the Information Age, the change has been particularly pronounced. “In business, you can use technology to do things you couldn’t do 30 years ago,” says Steve Kaplan. “You can scale your business using technology, and you can use people in India and China and all over the world—you couldn’t do that as effectively 30 years ago.” This, he argues, has been spectacularly positive for poorer people in developing countries. In 1990, the World Bank estimated that roughly 35 percent of the world lived in extreme poverty. Today, less than 11 percent of the world’s population is so impoverished. And it has been good for wealthy residents of developed countries. For them, the result has taken the form of the “superstar” or “winner-take-all” phenomenon, first identified in a landmark 1981 paper by the late Sherwin Rosen, who taught at the University of Chicago. “In certain kinds of economic activity there is concentration of output among a few individuals,” wrote Rosen. “Relatively small numbers of people earn enormous amounts of money and dominate the activities in which they engage.” Technology allows a hedge fund to be able to manage $20 billion and invest it,” says Steve Kaplan. “I don’t think people had the systems and information to do that 20 to 30 years ago. Now they have the systems and the information to do that. That technological change is here and is not going away. If anything, it’s getting stronger.” Gold (2021) The 000.1 Pct "],["technology-policy.html", "46 Technology Policy", " 46 Technology Policy Farooqi The hourglass occupational structure may be a consequence of the information technology revolution. The technology enabled routinization of production processes of both goods and services. Both men and machines were automated. Machines were automated with code. Workers were automated through routinization of tasks, thereby increasing the scale of operations that could be brought under the precise control of highly-skilled managers. An unanticipated consequence of this development in technics was the vanishing of middle-skilled occupations and the systematic deskilling of the American labor force. As we argued above, this is the correct etiology of the crisis of the American working-class. The intellectual problem here is the implicit assumption that technology is an exogenous force and not susceptible to policy solutions. This premise is wrong. Technology is endogenous to our institutional arrangements. We can design them to incentivize certain classes of technologies and discourage others, for instance, through favorable or unfavorable tax treatment. A carefully-designed technology policy ought to be one of the pillars of open-ended American developmentalism. A rather direct form of attack would be to force the pace of the automation of all routine tasks. This would evacuate routine tasks in job task-bundles, thereby reducing the incidence of deskilled jobs. Human workers are great bundles of skill. Using them as automata is a poor use of their human capital. If we can force the pace of automation, we can get rid of deskilled dead-end jobs sooner and reallocate our tremendous human capital towards more productive activities. This policy also works with the grain of technology and markets—the automation of routine tasks in both blue-collar and white-collar occupations is already well underway. The idea that somehow human jobs will vanish due to competition from robots and AI is a luddite fallacy. Automation does not cause aggregate job losses. Rather, automation reconfigures what tasks are bundled together as jobs, increasing their creative content, reducing drudgery, and making them more rewarding for workers. Policies aimed at reversing occupational polarization will require time to work. Policy formulation and administrative implementation takes time; once there is some clarity over policy, firms will need some time to solve new problems, innovate, and re-bundle jobs; workers will also need time to reskill and master new bundles of responsibilities. So, we’re looking at least a decade long process. We should aim to reskill one hundred million Americans over the next decade. As part of a broader vision of open-ended American developmentalism, we should be constantly refining our strategy to upgrade the skill-set of the American populace—for it is human capital that is the true font of productivity. Farooqi (2023) Notes on US China Policy "],["trade.html", "47 Trade 47.1 Absolute Advantage 47.2 Supply Lines 47.3 Trading Partners 47.4 China is changing trade partners 47.5 Trade and Savings 47.6 Trade and Productivity 47.7 Scumpeterian Growth Theory", " 47 Trade 47.1 Absolute Advantage Welsh Everyone knows about comparative advantage, but what doesn’t get talked about is absolute advantage. In absolute advantage you have something people need that they can only get from you. This can be weapons. It can be jets. It can be advanced computers. It can be the capital equipment (like lithography machines used to create semiconductors) used to make other things. Sometimes it can be a resource, like oil. Welsh (2023) Climate Change and Environmental Collapse 47.2 Supply Lines Tverberg Countries are now actively trying to bring supply lines back closer to home. Figure: Trade as a percentage of GDP based on World Bank data for the World, the United States, and China. Tverberg (2023) Today’s energy bottleneck may bring down major governments 47.3 Trading Partners Welsh Figure: Trading networks 1990 Figure: Trading networks 2020 This sort of thing is a leading indicator. Countries who were dominant are able to control more of the world’s resources than they deserve. The US has LOST its dominance already. It’s all over except for the shooting. It doesn’t look or feel like that because of generations of accumulation and because the dollar is used for most world trade. But those are lagging indicators. Britain’s pound was the main instrument of trade for decades after the US had overtaken it industrially. Since China now offers almost everything the West does, at better terms, they will come to command those resources. Within the West we are already seeing the US cannibalizing its satrapies. The main industry the West seems to still have a dominant lead in is biotech, but the Chinese will get there. Japan and South Korea will do better because both are keeping up in scientific innovation, but my bet is that South Korea will peel off into the Chinese sphere at some point, economically they already have. I’m less sure about Japan, but they’d be wise to do so. As for Europe, well, for twenty years I’ve been warning them they had to regain their independence and forge their own path. Most of Eastern Europe should never have been allowed into the EU or NATO. The EU should have built its own army and left NATO. And yeah, they should have done everything necessary to keep good ties with Russia, which would have been easy, because until fairly recently Russia wanted to be a European nation. They did none of this, their rate of scientific advancement is abysmal outside of a few areas and they’re toast. Welsh (2023) Notes On The Structure Of American Imperial Collapse 47.4 China is changing trade partners 47.5 Trade and Savings Farooqi Trade imbalances are a mathematical function of relative savings rates. The flipside of current account surpluses are savings outflows. Countries that save more than others must necessarily export their savings; conversely, countries that save less must necessarily import the savings of others. So, the reason for the Chinese trade surplus and American deficit is not Chinese duplicity or that of US firms engaged in outsourcing, but the simple fact that the Chinese savings rate averaged 46 percent since 2000, while the US averaged 17 percent over the same period. The US has a massive trade deficit with Germany for the same reason: Germany’s savings rate has averaged 26 percent since 2000. There is an even stronger and more obvious reason to believe that trade imbalances are a total red herring when it comes to understanding deindustrialization. Employment in manufacturing has been in secular decline since Woodstock. In 1948-1969, manufacturing employed about 23 percent of the civilian labor force. It has been steadily declining ever since: manufacturing employment averaged 20 percent in the 1970s, 16 percent in the 1980s, 13 percent in the 1990s, 10 percent in the 2000s, and 8 percent in the 2010s. Even if deindustrialization is ultimately responsible for the ‘deaths of despair’ and 2016, import competition from China in the 2000s could not possibly have been responsible because causes must necessarily precede effects.[16] In fact, deindustrialization may itself be a red herring. The structural break for deaths of despair ­— deaths due to alcohol poisoning, drug overdose and suicide — occurs is the early-1990s.[17] In other words, the structural break occurs decades into the decline of manufacturing and a decade before the China Shock. So, deindustrialization per se cannot be held responsible for the crisis of the American working class either. Farooqi (2023) Notes on US China Policy 47.6 Trade and Productivity Farooqi The causal channel through which trade enhances productivity growth has been well understood since Melitz (2003), an academic paper that’s been cited 18,586 times. The Heckscher-Ohlin class of Ricardian models explained inter-industry between countries; they could not be modified to explain the dominant pattern of trade in the core of the world economy: trade within the same industry between countries with similar factor endowments. New trade theory, beginning with the pioneering work of Krugman and Helpman, highlighted the importance of product differentiation and increasing returns to scale to explain why intra-industry trade was dominant and why most trade takes place between countries with very similar factor endowments. Melitz extended the new trade theory to explain the logic of precisely how trade enhances productivity growth. In the modern understanding, trade plays a dynamic role is fostering productivity because exposure to the world market forces the least productive firms to exit and the most productive firms to innovate harder to escape competition. This Schumpeterian logic of creative destruction and the unrelenting disciplinary force of the world market is the key to productivity growth in the advanced industrial core of the modern world economy. There are two distinct components of intensified import competition that induce adjustments in opposite directions.[27] The first component is vertical: firms using imported intermediate goods and services in their own production processes. The response induced by the vertical component is always positive and works very much like an exogenous productivity shock. All firms that use the more efficient intermediates become more productive and profitable as a result. Market competition then ensures that most of these gains are passed on to consumers in the form of larger consumer surpluses and to workers in the form of higher wages. The second component is horizontal: it affects firms whose products are directly exposed to import competition. For low productivity firms, the horizontal component induces a negative response. Low productivity firms that find it hard to compete with imports lose market share and may be forced to exit. The response of high productivity survivors of the onslaught is usually positive. The intensified discipline of the world market incentivizes the survivors to innovate harder. Both components raise average productivity in the industry. It may seem that the horizontal component is bad, and the vertical component is good for the home country facing stiffer import competition. But that is a myopic view of the dynamics. By driving the least productive firms to shrink or exit altogether, stiffer import competition frees up factors of production for redeployment towards more productive firms. Financial resources, embodied capital, managerial skill, and above all, the skills and knowhow of the workforce are thereby put to more productive use. This raises aggregate productivity in the industry and for the home country at large. The dynamic welfare gains from Schumpeterian creative destruction are reckoned to be three times larger than the Ricardian gains from trade. 47.7 Scumpeterian Growth Theory One of the main findings of what’s called Schumpeterian growth theory is of particular importance to the countries of the advanced industrial core. This is the finding that “openness is particularly growth-enhancing in countries that are closer to the technological frontier.”[36] Conversely, tariff walls and non-tariff trade barriers pose special risks for the most advanced economies. Specifically, the risk posed by protectionism is that it would undermine the competitiveness of the center countries. In terms of trade policy, we must try to reclaim our role as the agenda-setter. This involves shouldering the responsibility of articulating arrangements for the community of nations as a whole. This would require some tolerance of what may look like the loss of strategic sectors. Sheltering incompetent American automakers against Chinese competition is going to make them fatter and less competitive rather than more. At any rate, if we change the rules of the game mid-play because we start losing some valuable pieces, we shall not be able to reclaim our role as the agenda-setter. Trying to prevent China from catching up in computing by garrisoning the world economy is a harebrained idea. It has almost no chance of success. In fact, our chips escalation has already failed. It was designed to prevent Chinese firms from producing chips less than fourteen nanometers thick. They’re already making chips narrower than seven nanometers. We just should let it go. All we can buy with this strangulation plan is the enduring enmity of the Chinese. We can’t actually prevent them from catching up at all. We can only run harder ourselves. That is where the focus of our policies ought to be. Farooqi (2023) Notes on US China Policy "],["wealth.html", "48 Wealth", " 48 Wealth Fix There’s a straight line between wealth concentration, corporate consolidation, and the strategy of ‘buying, not building’. In short, Peter Thiel is correct when he says that ‘competition is for losers’. Speaking of competition and losers, Ronald Reagan set the tone of the neoliberal era when, in 1981, he fired 11,000 striking air-traffic controllers. The message? Workers were losers who would be subjected to the discipline of competition. Reagan called it ‘morning in America’. But really, it was ‘morning for American big business’. Today, we are well into the next-day’s hangover, and we know how the party played out. For workers, it was a disaster. But for the rich, it was an incredible boon. Wealth didn’t trickle down so much as it got catapulted up. The result, as Figure 1A shows, was a relentless rise in the concentration of American wealth. As wealth got catapulted from the poor to the rich, it also got transported from the mega rich to the supremely rich. Even among the upper crust of elites wealth has grown more concentrated. The culprit seems to be the stock market. The world’s billionaires have a median wealth of about $2.4 billion. Compared to the $240B wealth of the world’s richest man, Elon Musk, $2.4B is chump change. The fit between the S&amp;P 500 and billionaire wealth concentration is so tight. S&amp;P 500 tracks the total market capitalization of the 500 largest US firms. S&amp;P 500 sums the market capitalization of the 500 largest US firms. To unwrap our stock-market puzzle, we need to review some math. In general, measures of spread are unrelated to measures of central tendency. There is, however, an exception. It happens when growth is driven by inequality. ‘Growth through inequality’ — explains our stock-market results. The S&amp;P 500 index (an average) is connected to levels of elite wealth concentration (a form of spread). But this connection only makes sense if the S&amp;P 500 is an (unwitting) indicator of stock-market inequality. What appears as stock-market ‘growth’ is in part, an artifact of rising stock-market concentration. As their stock rises, the richest firms will buoy the whole S&amp;P 500 index. But this buoyancy isn’t really ‘growth’; it’s an artifact of corporate concentration — rich firms getting richer. The ‘crime’ of elite wealth concentration seems to be tied directly to corporate oligarchy. Savvy corporations are always looking for a better route to power. And that better route is to buy instead of build. Google’s success stories (its ad-tech stack, its mobile platform, its collaborative office suite, its server-management tech, its video platform …) are all acquisitions. Billionaires like Peter Thiel are so hubristic that they speak brazenly about their pursuit of power, laying bare their inner robber baron. The upshot to this plute bravado is that few people will be surprised by the straight line that connects corporate oligarchy with the concentration of elite wealth. Fix (2023) Stocking Up on Wealth … Concentration "],["climate-economics.html", "49 Climate Economics 49.1 A Blocking Neoclassical Framework 49.2 Long-term Economic effects of Climate Change 49.3 Carbon Tax 49.4 Finnish Carbon Tax 49.5 Instrument Choice Delays 49.6 SSPs 49.7 Guard Rail Economics 49.8 Tipping Points 49.9 Keynesian Decarbonization 49.10 Discount Rate 49.11 Geoff Man on Nordhaus 49.12 Persistence of Temperature Effects 49.13 Climate economics has let us down", " 49 Climate Economics 49.1 A Blocking Neoclassical Framework Brookes and Wagner With its fixation on equilibrium thinking and an exclusive focus on market factors that can be precisely measured, the neoclassical orthodoxy in economics is fundamentally unequipped to deal with today’s biggest problems. Change within the discipline is underway, but it cannot come fast enough. The economics discipline has failed to understand the climate crisis – let alone provide effective policy solutions for it – because most economists tend to divide problems into small, manageable pieces. Rational people, they are wont to say, think at the margin. What matters is not the average or totality of one’s actions but rather the very next step, weighed against the immediate alternatives.Such thinking is indeed rational for small discrete problems. Compartmentalization is necessary for managing competing demands on one’s time and attention. But marginal thinking is inadequate for an all-consuming problem touching every aspect of society.Economists also tend to equate rationality with precision. The discipline’s power over public discourse and policymaking lies in its implicit claim that those who cannot compute precise benefits and costs are somehow irrational. This allows economists – and their models – to ignore pervasive climate risks and uncertainties, including the possibility of climatic tipping points and societal responses to them. And when one considers economists’ fixation with equilibrium models, the mismatch between the climate challenge and the discipline’s current tools becomes too glaring to ignore.Yes, a return to equilibrium – getting “back to normal” – is an all-too-human preference. But it is precisely the opposite of what is needed – rapidly phasing out fossil fuels – to stabilize the world’s climate.These limitations are reflected in benefit-cost analyses of cutting emissions of carbon dioxide and other greenhouse gases. The traditional thinking suggests a go-slow path for cutting CO2. The logic seems compelling: the cost of damage caused by climate change, after all, is incurred in the future, while the costs of climate action occur today. The Nobel prize-winning verdict is that we should delay necessary investment in a low-carbon economy to avoid hurting the current high-carbon economy. The very structure of academic economics all but guarantees that marginal thinking continues to dominate. The most effective way to introduce new ideas into the peer-reviewed academic literature is to follow something akin to an 80/20-rule: stick to the established script for the most part; but try to push the envelope by probing one dubious assumption at a time. Needless to say, this makes it extremely difficult to change the overall frame of reference, even when those who helped establish the standard view are looking well beyond it themselves. Because equilibrium thinking underpins the traditional climate-economic models that were developed in the 1990s, these models assume that there are tradeoffs between climate action and economic growth. They imagine a world where the economy simply glides along a Panglossian path of progress. Climate policy might still be worthwhile, but only if we are willing to accept costs that will throw the economy off its chosen path.Against the backdrop of this traditional view, recent pronouncements by the International Monetary Fund and the International Energy Agency are nothing short of revolutionary. Both institutions have now concluded that ambitious climate action leads to higher growth and more jobs even in the near term.The logic is straightforward: climate policies create many more jobs in clean-energy sectors than are lost in fossil-fuel sectors, reminding us that investment is the flipside of cost. That is why the proposal for a $2 trillion infrastructure package in the United States could be expected to spur higher net economic activity and employment. Perhaps more surprising is the finding that carbon pricing alone appears to reduce emissions without hurting jobs or overall economic growth. The problem with carbon taxes or emissions trading is that real-world policies are not reducing emissions fast enough and therefore will need to be buttressed by regulation. The framework of neoclassical economics is still blocking progress. The discipline is long overdue for its own tipping point toward new modes of thinking commensurate with the climate challenge. Brookes and Wagner (2021) Economics needs a Climate Revolution 49.2 Long-term Economic effects of Climate Change Kahn Abstract We study the long-term impact of climate change on economic activity across countries, using a stochast ic growth model where labour productivity is affected by country-specific climate variables—defined as deviations of temperature and precipitation from their historical norms. Using a panel data set of 174 countries over the years 1960 to 2014, we find that per-capita real output growth is adversely affected by persistent changes in the temperature above or below its historical norm, but we do not obtain any statistically significant effects for changes in precipitation. Our counterfactual analysis suggests th at a persistent increase in average global temperature by 0.04°C per year, in the absence of mitigation policies, reduces world real GDP per capita by 7.22 percent by 2100. On the other hand, abiding by the Paris Agreement, thereby limiting the temperature increase to 0.01°C per annum, reduces the loss subst antially to 1.07 percent. These effects vary significantly across countries. We also provide supplement ary evidence using data on a sample of 48 U.S. states between 1963 and 2016, and show that climate chan ge has a long-lasting adverse impact on real output in various states and economic sectors, and on labo r productivity and employment. Kahn Memo By using deviations of climate variables from their respective historical norms, while allowing for nonlinearity, we avoid the econometric pitfalls associated with the use of trended variables, such as temperature, in output growth equations. As it is well known, and is also documented in our paper, temperature has been trending upward strongly in almost all countries in the world, and its use as a regressor in a growth regression can lead to spurious estimates. To measure the damage caused by climate change, economists have sought to quantify how aggregate economic growth is being a¤ected by rising temperatures and changes in rainfall patterns; see a recent survey by Dell et al. (2014) The literature which attempts to quantify the e¤ects of climate change (temperature, pre- cipitation, storms, and other aspects of the weather) on economic performance (agricultural production, labour productivity, commodity prices, health, con‡ict, and economic growth) is relatively recent and mainly concerned with short-run e¤ects Moreover, there are a number of grounds on which the econometric evidence of the e¤ects of climate change on growth may be questioned. Firstly, the literature relies primarily on the cross-sectional approach and as such does not take into account the time dimension of the data (i.e., assumes that the observed relationship across countries holds over time as well) and is also subject to the endogeneity (reverse causality) problem given the possible feedback e¤ects from changes in output growth onto the climate variable. Secondly, the …xed e¤ects (FE) estimators used in more recent panel-data studies im- plicitly assume that climate variables are strictly exogenous, and thus rule out any reverse causality from economic growth to rising average temperatures. n his computable general equilibrium work, Nordhaus accounts for the fact that faster economic activity increases the stock of greenhouse gas (GHG) emis- sions and thereby the average temperature. At the same time, rising average temperature could reduce real economic activity. This equilibrium approach has important implications for the econometric speci…cation of climate change–economic growth relationship. In fact, recent studies on climate science provide strong evidence that the main cause of contemporary global warming is the release of greenhouse gases to the atmosphere by human activities. Consequently, when estimating the impact of climate change on economic growth, temperature (T it ) may not be considered as strictly exogenous, but merely weakly exogenous/predetermined to income growth; in other words economic growth in the past might have feedback e¤ects on future temperature. While it is well known that the FE estimator su¤ers from small-T bias in dynamic panels with N (the cross-section dimension) larger than T (the time series dimension). This bias exists regardless of whether the lags of the dependent variable are included or not, so long as one or more regressor is not strictly exogenous. In such cases, inference based on the standard FE estimator will be invalid and can result in large size distortions unless N=T ! 0, as N; T ! 1 jointly. Therefore, caution must be exercised when interpreting the results from studies that use the standard FE estimators in the climate change–economic growth literature given that N is often larger than T . Thirdly, econometric speci…cations of the climate change–macroeconomic relation are often written in terms of real GDP per capita growth and the level of temperature, T it , and in some cases also T it 2 ; see, for instance, Dell et al. (2012) and Burke et al. (2015). But if T it is trended, which is the case in almost all countries in the world (see Section 3.1), inclusion of T it in the regression will induce a quadratic trend in equilibrium log per capita output (or equivalently a linear trend in per capita output growth) which is not desirable and can bias the estimates of the growth–climate change equation. Finally, another major drawback of this literature is that the econometric speci…cations of the climate change–growth relation are generally not derived from or based on a theoretical growth model. Either an ad hoc approach is used, where real income growth is regressed on a number of arbitrarily–chosen variables, or a theoretical model is developed but not put to a rigorous empirical test. We contribute to the climate change–economic growth literature along the following di- mensions. Firstly, we extend the stochastic single-country growth models of Merton (1975), Brock and Mirman (1972), and Binder and Pesaran (1999) to N countries sharing a common technology but di¤erent climate conditions. Our theoretical model postulates that labour productivity in each country is a¤ected by a common technological factor and country- specific climate variables, which we take to be average temperature, T it , and precipitation, P it , in addition to other country-specific idiosyncratic shocks. As long as T it and P it remain close to their respective historical norms (regarded as technologically neutral), they are not expected to a¤ect labour productivity. However, if climate variables deviate from their his- torical norms, the e¤ects on labour productivity could be positive or negative, depending on the region under consideration. For example, in a historically cold region, a rise in temper- ature above its historical norm might result in higher labour productivity, whilst for a dry region, a fall in precipitation below its historical norms is likely to have adverse e¤ects on labour productivity. 2 Secondly, contrary to much of the literature which is mainly concerned with short-term growth e¤ects, we explicitly model and test the long-run growth e¤ects of persistent increases in temperature. Thirdly, we use the half-panel Jackknife FE (HPJ-FE) estimator proposed in Chudik et al. (2018) to deal with the possible bias and size distortion of the commonly-used FE estimator (given that T it is weakly exogenous). When the time imension of the panel is moderate relative to N , the HPJ-FE estimator e¤ectively corrects the Nickel-type bias if regressors are weakly exogenous, and is robust to possible feedback e¤ects from aggregate economic activity to the climate variables. Our results suggest that a persistent change in the climate has a long-term negative e¤ect on per capita GDP growth. Our empirical findings apply equally to poor or rich, and hot or cold countries. We show that an increase in average global temperature of 0:04 C per year— corresponding to the Repre- sentative Concentration Pathway (RCP) 8.5 scenario (see Figure 1), which assumes higher greenhouse gas emissions in the absence of mitigation policies— reduces world’s real GDP per capita by 7:22 percent by 2100. Limiting the increase to 0.01 C per annum, which corre- sponds to the December 2015 Paris Agreement, reduces the output loss substantially to 1:07 percent. To put our results into perspective, the conclusions one might draw from most of the existing climate change–macroeconomy literature are the following: (i) when a poor (hot) country is 1 C warmer than usual, its income growth falls by 1–2 percentage points in the short- to medium-term; (ii) when a rich (temperate) country is 1 C warmer than usual, there is little impact on its economic activity; and (iii) the GDP e¤ect of increases in average temperatures (with or without adaptation and/or mitigation policies) is relatively small— a few percent decline in the level of GDP per capita over the next century (see, Figure 2). In contrast, our counterfactual estimates suggest that all regions (cold or hot, and rich or poor) would experience a relatively large fall in GDP per capita by 2100 in the absence of climate change policies (i.e., the RCP 8.5 scenario). However, the size of these income e¤ects varies across countries depending on the projected paths of temperatures. Burke et al. (2015) consider an alternative panel specification that adds quadratic climate variables to the equation and detect: (i) non-linearity in the relationship; (ii) di¤erential impact on rich versus poor countries; and (iii) noisy medium-term growth e¤ects— their higher lag order (between 1 and 5) estimates reported in Supplementary Table S2, show that only 3 out of 18 estimates are statistically significant. Overall, apart from the econometric shortcomings of existing studies, robust evidence for the long-run growth e¤ects of climate change are nonexistent in the literature. However, our results show that an increase in temperature above its historical norm is associated with lower economic growth in the long run— suggesting that the welfare e¤ects of climate change are signi…cantly underestimated in the literature. Therefore, our findings call for a more forceful policy response to climate change. Kahn (2019) LONG-TERM MACROECONOMIC EFFECTS OF CLIMATE CHANGE: A CROSS-COUNTRY ANALYSIS (pdf) Dell Abstract A rapidly growing body of research applies panel methods to examine how temperature, precipitation, and windstorms influence economic outcomes. These studies focus on changes in weather realizations over time within a given spatial area and demonstrate impacts on agricultural output, industrial output, labor productivity, energy demand, health, conflict, and economic growth, among other outcomes. By harnessing exogenous variation over time within a given spatial unit, these studies help credibly identify (i) the breadth of channels linking weather and the economy, (ii) heterogeneous treatment effects across different types of locations, and (iii) nonlinear effects of weather variables. This paper reviews the new literature with two purposes. First, we summarize recent work, providing a guide to its methodologies, datasets, and findings. Second, we consider applications of the new literature, including insights for the “damage function” within models that seek to assess the potential economic effects of future climate change. Dell Memo The difficulty in identifying causative effects from cross-sectional evidence has posed substantial and long-standing challenges for understanding the historical, contemporary, and future economic consequences of climate and climate change. In the last few years, there has been a wave of new empirical research that takes a different approach. These new studies use panel methodologies, exploiting high-frequency (e.g., year-to-year) changes in temperature, precipitation, and other climatic variables to identify these variables’ economic effects. As nomenclature, this new literature uses “weather variation” to describe shorter-run temporal variation. The word climate is reserved for the distribution of outcomes, which may be summarized by averages over several decades, while weather describes a particular realization from that distribution and can provide substantial variability. The primary advantage of the new lit- erature is identification. By exploiting exogenous variation in weather outcomes over time within a given spatial area, these methods can causatively identify effects of temperature, precipitation, and windstorm variation on numerous outcomes, including agricultural output, energy demand, labor productivity, mortality, industrial output, exports, conflict, migration, and economic growth. This literature has thus provided a host of new results about the ways in which the realizations of temperature, precipita- tion, storms, and other aspects of the weather affect the economy. This literature has important implications for the “damage function” in climate change models. The opportunity here is to bring causative identification to the damage functions, elucidating the set of important climate–economy channels and their functional forms. The challenge lies in bridging from the evidentiary basis of short-run weather effects to thinking about longer-run effects of changes in the distribution of weather, which may be either larger (e.g., due to intensification effects) or smaller (e.g., due to adaptation) than the short-run impacts. While certain climate change aspects are difficult to assess, we examine a number of empirical methodologies that can help bridge toward longer-run effects while maintaining careful identification. climate studies often seek to estimate the contemporaneous effect of temperature on economic activity for the purpose of assessing the potential impacts of forecasted temperature changes over the next several decades. The ­ cross-sectional relationship, which represents a very long-run equilibrium, may incorporate processes that are too slow to accurately inform the time scale of interest, or it may include historical processes (such as colonialism) that will not repeat themselves in modern times. To the extent that one is interested in iso- lating the impact of climatic variables such as temperature—apart from the many other factors that they are correlated with and have influenced over the very long run—a different approach is to use longitudinal data to investigate the effects of weather shocks. This approach, which is the focus of this review, has emerged in recent years and emphasizes variation over time within a given spatial entity. The literature uses a nomenclature of “weather varia- tion” for shorter-run temporal variation, as opposed to “climate variation,” where the word climate is used to describe the distribution of outcomes while weather refers to a particular realization from that distribution. A related issue is the inclusion of lags of the dependent variable, ​ y​ it ​ . Including these lags biases coefficient estimates in short panel models, 4 yet excluding the lagged dependent variable may also bias the estimates if it is an important part of the data-generating pro- cess. While what comprises a “short” panel will depend on the data-generating process, Monte Carlo experiments suggest that the bias can be nonnegligible with panel lengths of T = 10 or even T = 15. A further implementation question involves the appropriate functional form for the weather variables. One common approach measures ​ C ​ it ​ in “levels” (e.g., degrees Celsius for temperature or millime- ters for precipitation). In the panel set up, the identification thus comes from devia- tions in levels from the mean. 7 Another common approach, aimed at revealing nonlinear effects, considers the frequencies at which the weather realizations fall into different bins. A different approach emphasizes “anom- alies,” where the weather variable is cal- culated as its level difference from the within-spatial-area mean and divided by ­ the within-spatial-area standard deviation. The first part—the difference in mean—is already captured in a broad sense by the panel model. The second part—scaling by the standard deviation—takes a particular view of the underlying climate–economy model where level changes matter not in an absolute sense but in proportion to an area’s usual variation. Alternatively, outcome-specific approaches may be preferred where existing research provides guidance. For example, knowledge of biological processes in agriculture sug- gest refined temperature measures such as “degree-days” for crop growth, possibly with crop-specific thresholds. s a general rule, imposing specific func- tional forms on the data, such as crop degree- days, is useful to the extent that one has confidence in the specific model of the pro- cess that translates weather to economic out- comes. The more agnostic about the model, the more general the researcher would like to be about the functional form. There are two notable interpretative issues with the panel models that, while not calling into question the experimental validity of the regression design, do raise questions about their external validity for processes such as global warming. One interpretive challenge is whether and how the effects of medium- or long-run changes in climatic variables will differ from the effects of short-run fluctuations. A second issue is that panel models, in focusing on idiosyncratic local variation, also neutral- ize broader variation that may be of poten- tial interest, including general equilibrium effects that spill across spatial borders or are global in nature, like effects on commodity prices. Data There are currently four principal types of weather data: ground station data, gridded data, satellite data, and reanalysis data. The most basic type of data are from ground stations, which typically directly observe tem- perature, precipitation, and other weather variables such as wind speed and direction, humidity, and barometric pressure. Gridded data provide more complete coverage by interpolating station information over a grid. Satellite data use satellite-based readings to infer various weather variables. Finally, reanalysis data combine information from ground stations, satellites, weather balloons, and other inputs with a climate model to estimate weather variables across a grid. Different interpolation schemes can produce different estimates, particularly in short time periods and particularly for precipitation. Precipitation has a far greater spatial variation than temperature, especially in rugged areas, and thus is more difficult to interpolate. While satellite data can provide important weather information for areas with a limited ground network, satellite data are not necessarily a panacea. Satellites were launched relatively recently, so their data does not extend back nearly as far historically as other ­datasets. Furthermore, an individual ground station is more accurate than the satellite data for that particular location, in part because satellites do not directly measure temperature or precipitation, but rather make inferences from electromag- netic reflectivity in various wavelength bands. Lastly, a s ­ atellite-based series is not drawn from a single satellite, but rather from a series of satellites. Sensors have changed subtly over the years and, within a particular satellite, corrections are needed due to subtle changes in the satellite’s orbit over time and other factors. The key difference between reanalysis and gridded data is that, rather than use a statistical procedure to interpolate between observations, a climate model is used. One approach is to aggregate spatially; that is, to overlay administrative or other boundaries with the gridded weather dataset and take a simple area-weighted average of weather variables within the administrative unit, which can be done easily using GIS soft- ware. However, this approach will lead large areas with little economic activity and sparse populations (such as deserts, rain forests, or the Arctic) to dominate the weather aver- ages of large spatial units such as the United States, Russia, and Brazil. A second approach is, therefore, to aggregate using a fixed set of population weights, so that the relevant concept is the average weather experienced by a person in the administrative area, not the average weather experienced by a place. Overall, the studies discussed in this sec- tion document that temperature, precipi- tation, and extreme weather events exert economically meaningful and statistically significant influences on a variety of eco- nomic outcomes. These impacts illustrate the multifaceted nature of the ­ weather– economy relationship, with numerous appli- cations for understanding historical, present, and future economic outcomes and possible policy responses. For example, the effects of weather variables on mortality rates, labor productivity, energy demand, and agricul- tural output can inform investments and policy design around public health, air-con- ditioning, energy infrastructure, and agricul- tural technologies. Moreover, these studies can help inform classic issues of economic development, especially the role of geo- graphic features in influencing development paths. Finally, these analyses may inform estimates of the economic costs of future climatic change. The possibility of future climatic change has been a primary motive for the recent, rapid growth of this literature. Results Cross-country empirical analyses show a strong negative relationship between hot cli- mates and income per capita. Panel studies exploit the exogeneity of cross-time weather variation, allowing for causative identification. In a world sample from 1950 to 2003, Dell, Jones, and Olken (2012) examine how annual variation in temperature and precipitation affects per capita income. They show that being 1°C warmer in a given year reduces per capita income by 1.4 percent, but only in poor countries. Moreover, estimating a model with lags of temperature, they find that this large effect is not reversed once the temperature shock is over, suggesting that temperature is affecting growth rates, not just income levels. 22 Growth effects, which compound over time, have potentially first-order consequences for the scale of eco- nomic damages over the longer run, greatly exceeding level effects on income, and are thus an important area for further modeling and research. While the production function is often calibrated through the use of experimental data, it has been criticized for not realistically modeling real farmer behavior in real settings. For example, many studies do not allow farmers to adopt new crops when the temperature input into the production function changes, nor do they allow farmers to switch their cultivated land to livestock or nonfarm use. To address these concerns, Mendelsohn, Nordhaus, and Shaw (1994) developed a second approach, which they called the Ricardian approach, that instead used cross-sectional regressions with land values to recover the net impacts of climate on agri- cultural productivity. By analyzing farm land prices as a function of climate and a host of other characteristics, they estimated that the impacts of climate change would be much smaller than those estimated by the production function approach and might even be positive. In estimating a cross-sectional relationship like equation (2) for irrigated areas, which transport water from other locations, the localized climate is not the key determinant of production. Understanding nonlinearities becomes important when con- sidering the impact of global climate change because a right-shift in the distribution of average temperature causes a disproportionate increase in the number of very hot days. The possibility of adaptation was a major argument for the approach of Mendelsohn, Nordhaus, and Shaw (1994), since presumably, changes in land values would incorporate future adaptation effects. Modern lab experiments have investi- gated the impact of temperature on pro- ductivity. Subjects are typically randomly assigned to rooms of varying temperatures and asked to perform cognitive and physical tasks. Examples of tasks shown to respond adversely to hot temperatures in laboratory settings include estimation of time, vigilance, and higher cognitive functions, such as men- tal arithmetic and simulated flight. Observational and experimental studies also show a strong relationship between tem- perature and the productivity of factory, call center, and office workers, as well as students. Within the range of temperatures from 22–29oC, each additional oC is associated with a reduc- tion of about 1.8 percent in labor produc- tivity. The relationship is complex and find that other aspects (e.g., humidity, amount of outdoor air, carbon dioxide levels) have complex inter- actions with ­temperature. A meta-analysis of these studies concludes that increasing temperature from 23 to 30oC reduces productivity by about 9 percent. Industrial output using aggregated data center approximately on a 2 percent output loss per 1°C. Large effects of windstorms on industrial production. Effects of precipitation on industrial output appear slight, although only one study looks at extremely heavy precipitation and in that case finds modest negative effects. Energy The literature has looked extensively at how climatic variables, in particular temper- ature, influence energy consumption. This relationship, which has received renewed attention in light of potential climate change, has long been important for the design of electricity systems, where demand varies with climate and weather. Understanding temperature effects matters for the energy consequences per se and for potential feed- back loops, incorporated into some climatic models, where energy demand influences greenhouse gas ­emissions, which in turn affects future energy demand. A clear U-shape relationship between energy demand and temperature, with an extra day below 10oF or above 90oF raising annual energy demand by 0.3–0.4 percent. These panel-data papers, in using tempera- ture bins, depart from a prior practice of using “heating degree days” (HDD) and “cool- ing degree days” (CDD), which count the number of days below and above a threshold temperature, with each day weighted by its temperature difference from the threshold. This degree-days approach misses the con- vexity found in the ­nonparametric approach, where extreme temperatures provoke much stronger energy demand increases. The con- vexity of the U-shape appears important both in getting the energy demand estimation cor- rect and in light of climate change models, which show an increasing number of very hot days. Partly for this reason, Deschênes and Greenstone (2011) and Auffhammer and Aroonruengsawat (2011) find that the net effect of warming over the twenty-first century is likely to increase energy demand substantially, ceteris paribus, with these studies estimating 11 percent and 3 percent demand increases respectively. Trade and Innovation Trade can, in principle, dampen or exacerbate local effects of productivity losses. Another potentially first-order adaptation mechanism is innovation. The unusual identification opportunity provided by weather shocks has allowed a rigorous analysis of weather–economy link- ages, and implications for breadth, hetero- geneity, and functional forms. While much work remains in developing a detailed under- standing of the underlying mechanisms, especially for macroeconomic and politi- cal economy outcomes, the new literature shows that weather variation has substantive effects in contemporary periods. This begins to suggest policy targets, whether the goal is preventing substantial economic damages or protecting public health and security. From short to long run: Econometrics While tem- perature changes over the next thirty years will plausibly be within this range (recall the IPCC middle estimates were between 1.8–3.1oC by 2100), the ninety-fifth percentile estimate is warming of 7oC by 2100. If the impacts of climatic variables are linear throughout this range, then extrapolation is not an issue per se. However, if there are nonlinearities that are different from those operating within historical experience, one cannot directly extrapolate from equation (3) to climate scenarios far outside this range. This issue suggests a limited capacity for panel models to provide quantitative estimates of damages from extreme warming. hese issues highlight that, even though panel models of the form of equation (3) correctly identify the causal effect of weather shocks on contemporaneous economic out- comes, they may not estimate the structural equation of interest for understanding the likely effects of future global climate change. Moreover, even leaving aside the potential of catastrophic climate scenarios, such as rapid sea-level rise or the release of methane from melting permafrost that could greatly increase global temperature, the panel esti- mates are neither obviously an upper bound nor a lower bound for the effect of climate change. If the adaptation force dominates, then the effects of weather shocks will tend to be larger than the effects of climate change; if the intensification force dominates, then the effects of weather shocks will tend to be smaller than the effects of climate change. Longer-difference estimates are perhaps the closest empirical analogue to the structural equation of interest for climate change. To the extent that adaptation requires forward-looking investments, adap- tation choices will depend not only on the underlying damage functions and adaptation possibilities, but also on agents’ expectations. Responses will depend on whether agents both were aware of the change in average temperature, and whether they perceived it to be a permanent change or just an accumulation of idiosyncratic shocks. The challenge is that economies are chang- ing and the longer the time difference taken in (8), the further back in time the analysis goes (by necessity), and the further removed from present-day economic conditions the analysis becomes. To the extent that differ- ent economies presented very different standards of living, technologies, and institutions through the twentieth century, one may still make headway by examining historical heterogeneous treatment effects along various dimensions of economic development. On the other hand, the future presumably prom- ises new technologies and other features that may pull economies outside the range of historical experiences, calling for caution in drawing sharp conclusions from increasingly historical studies. Long-run studies illustrate that factor reallocation may be an important mechanism. IAMs Our focus is on the damage function, the component of IAMs that specifies how increased temperatures affect economic activity. IAMs used for economic policy analysis typi- cally include four broad components: 1) a model projecting the path for greenhouse gas (GHG) emissions; 2) a model mapping GHG emissions into climatic change; 3) a damage function that calculates the economic costs of climatic change, and; 4) a social welfare function for aggregating damages over time and potentially across space. All IAMs must make a wide variety of modeling choices, with large uncertainties remaining across each component. The possibility of positive feedback loops implies that mod- eled climate change predictions are right- skewed; in other words, there are “fat tail” probabilities for massive climatic change in the next century. IAMs must specify a social welfare function that discounts the future path of consumption. The concavity of the utility function. This property influences not only how one weighs future versus current generations, but also how one weighs rich versus poor economies at a single point in time. Different IAMs model the climate-dam- age function in somewhat different ways. For example, the DICE/RICE models use a Cobb–Douglas production function with capital and labor as inputs, multi- plied by TFP, which grows at a constant, exogenously specified rate. Output is then reduced by the climate-damage function. For example, in the DICE model, the damage function is \\[D(T) = frac{1}{1+Pi_1 T + Pi-2 T^2}\\] DICE calibrates the π parameters to match cross-sectional estimates of climate damages reviewed in Tol (2009). In the FUND model, rather than spec- ify an aggregate damage function directly, climate damages are calculated at the region-by-sector level and aggregated up; ­ that is, FUND posits separate models for agriculture, forestry, energy consumption, and health (deaths from infectious, cardio- vascular, and respiratory disease), while also considering water resources, extreme storm damage, sea level rise, and the value for eco- systems, with potentially separate regional parameters for each of these models. An important challenge with the current damage functions is that, for the most part, they do not incorporate the type of rigor- ous empirical evidence on climate damages reviewed here. In a recent review of IAMs, when discussing the calibration of the D(T) function, Pindyck (2013) writes “the choice of values for these parameters is essentially guesswork. The usual approach is to select values such that [D(T)] for T in the range of 2°C to 4°C is consistent with common wis- dom regarding the damages that are likely to occur for small to moderate increases in temperature. . . . The bottom line here is that the damage functions used in most IAMs are completely made up, with no theoretical or empirical foundation.” The implications of the econometric evidence discussed here can be thought of in two respects: how we model and calibrate the climate-damage function at a point in time, and how the climate-damage function evolves over time. A key modeling choice for the dam- age function is whether climate affects the level of output or the growth path of output. The main IAMs assume that the impact of climate is on the level of output only with the growth of total-factor productivity continuing exogenously. Because growth effects, even small ones, will ultimately dominate even large-level effects, ruling out growth effects substantially limits the possible economic damages these models allow. An alternative way of specifying the dam- age function is to allow climate to affect the long-run growth rate directly. Understanding the functional form through which climate affects economic output is critical. While it is hard to know definitively the correct functional form for the loss func- tion, even small impacts on productivity growth could, over time, swamp effects on the level of output. Building IAMs is a challenging exercise with enormous uncertainty. We are optimistic that the damage function can be substantially informed by the recent wave of new empirical research, which has begun to provide key insights. Integrating across the many studies reviewed, several broad themes emerge. First, there is a wide range of channels through which weather shocks affect eco- nomic outcomes. Shocks, especially temper- ature, affect agricultural output, industrial output, energy demand, labor productivity, health, conflict, political stability, and eco- nomic growth. Labor productivity effects alone may suggest potentially economywide mechanisms. Moreover, the magnitudes of the effects are often substantive. An inter- esting linkage appears across studies of labor productivity, industrial output, and economic growth, where estimates converge around a 1–2 percent loss per 1°C in poor countries. Second, the panel studies provide an emerging set of key insights about functional forms. Effects are often not simple linear functions independent of context. High sensitivity to extreme temperatures, but little or no sensitivity to temperature changes within moderate temperature ranges. International and internal trade effects, including studies of how integrated mar- kets both mute and transmit shocks. Panel methodologies can also study medium-run and longer-run changes directly. Keeping in mind that countries have warmed substan- tially on average in the last several decades, with substantial variance within and across countries, there is ample capacity to study medium-run changes. The recent warm- ing rate is also very similar to that predicted by many climate models through at least the middle of the current century. Noting that climate change is not about a perma- nent climate shock, but rather about a sto- chastic warming process along an upward trend, recent historical experience, which has occurred on such a stochastic warming trajectory, provides a highly relevant set- ting to understand warming effects. Dell (2014) What Do We Learn from the Weather? The New Climate–Economy Literature (pdf) 49.3 Carbon Tax Roberts Carbon taxes are an almost perfectly terrible policy from the perspective of political economy. They make costs visible to everyone, while the benefits are diffuse and indirect. They create many enemies, but have almost no support outside the climate movement itself. 49.3.1 Fee and Dividend More to the point, because there have been so few fee-and-dividend policies implemented in the real world, there’s been very little field testing of the public’s actual response to it. A new paper in the journal Nature Climate Change by political scientists Matto Mildenberger look at public opinion in the places where carbon fee-and-dividend policies have been implemented. It turns out there are only two. Switzerland established a rebate program in 2008. The carbon tax reached 96 Swiss francs (about $105) per tonne in 2018; about two-thirds of the revenue is rebated on a per-capita basis, with everyone (including children) receiving an equal share. Canada established a rebate program in 2019 as part of its national carbon-pricing strategy. So far, the scheme covers four of 10 provinces, with more than half of the national population. The price was initially set at 20 Canadian dollars (about $16 U.S.) a tonne, rising to CA$50 by 2022; recently the government released a new schedule that would target CA$170 by 2030. The refund, or Climate Action Incentive Payment, is based on the number of adults and children in the household, with a 10 percent boost for rural households. It is highly progressive; 80 percent of households get more back than they pay. The Nature Climate Change paper looks at public opinion in both countries. In Canada, it draws on a longitudinal study, which surveyed the same residents — “from five provinces, two subject to the federal carbon tax (Saskatchewan and Ontario), one with provincial emissions trading (Quebec), and two with provincial carbon taxes (British Columbia and Alberta)” — five times from February 2019 through May 2020, during which time the scheme was proposed, debated, passed, and implemented. In Switzerland, the paper draws on a survey of 1,050 Swiss residents in December 2019. Only 12 percent of Swiss respondents know that part of the carbon revenue is refunded; 85 percent did not know they’d gotten a refund at all. Canadians remain confused and in many cases ignorant about carbon refunds. You might think, well, the problem is how these countries administer their refunds. In Canada, it’s a line on your tax return. In Switzerland, it’s a discount on your health insurance premiums. Both are clearly marked, but lots of people don’t exactly scrutinize those documents and keep track of every line item. In short, the available evidence suggests that carbon refunds don’t do much to reshape public opinion on carbon taxes, even among voters with accurate information about the refund they receive. Roberts (2022) Do dividends make carbon taxes more popular? Apparently not 49.4 Finnish Carbon Tax Mideksa Abstract Finland introduced the planet’s first carbon tax in 1990 to experiment with, to most economists, the best policy to reverse carbon emissions. I estimate the causal effect of taxing carbon on Finnish emissions using the Synthetic Control Approach (Abadie, 2021). The results suggest that taxing carbon reduces emissions by big margins. Finnish emissions are 16% lower in 1995, 25% lower in 2000, and 30% lower in 2004 than emissions in the counterfactual consistent with carbon taxes whose value increasing by 20 fold in 1990 - 2005. The estimates suggest that the carbon tax’s abatement elasticity is about 9%. Mideksa Memo Despite the conceptual foundation behind using a carbon tax to reverse carbon emissions being strong, its empirical foundation remains arguably weaker. The supporting evidence for the effectiveness of taxing carbon is missing, first, because few countries have taxed carbon: even fewer empirical studies of the causal effect on emissions. Besides, in the countries that have taxed carbon, the policy-induced data generating process has been too complex to lend itself to causal identification. Syntetic Control Approach What is the causal effect on CO 2 emissions of the Finnish taxes on carbon? One can think about this question and identify the causal effect by conducting a randomized control trial: some regions, chosen randomly, tax carbon while the remaining regions serve as a control group. Yet, countries tax carbon in all regions let alone with randomization. One thus needs a second-best alternative to randomization: the synthetic control approach to estimate the causal effect of the Finnish carbon tax since 1990. The synthetic control method adopts a data-driven approach in choosing the best comparison unit and allows falsification tests in assessing its sensitivity In estimating the effect of the Finnish carbon tax using synthetic control, I focus on emissions from the transportation sector for the following reasons. First, there is a problem of ruined-control in the countries without a carbon tax. The problem arises when Finland imposes the carbon tax, some inputs can be imported from (or exported to) other countries. In other words, the aggregate emissions in nations without a carbon tax could be affected by the Finnish carbon tax when such countries trade with Finland. This is a concrete problem, for example, when it comes to per capita emissions, which is a contaminated measure for a small open economy like Finland. However, transportation services are internationally non-tradable, the ruined-control effect due to international trade is limited. The structure of energy production and use in transport activities is similar across countries. This eases the task of constructing a valid comparison unit to Finland from the set of other countries. I focus on the transport sector due to the availability of data for predictors that allow comparability across countries. The estimated gap between the actual and the counterfactual emissions implies that the carbon tax reduces emissions considerably. Finnish emissions are 16% lower in 1995, 25% lower in 2000, and 31% lower in 2005 relative to the counterfactual. Rising impact over time goes in line with the increasing intensity of the CO 2 tax per ton of CO 2 over time (i.e., increased by 20 fold in the treatment period). The estimated emissions reductions came from stabilizing the Finnish emissions at the 1990 level relative to sharply rising emissions in the countries lacking a carbon tax. The estimated emissions reductions are consistent with the decline in Finnish gasoline, and diesel, consumption after 1990. Moreover, Finnish passenger transport activities and the number of vehicles have decreased to a new trend after 1990. I estimate the carbon tax elasticity of emissions reductions directly by using the real carbon tax data and its estimated impact. The geometric mean of the annual carbon tax elasticity of emissions reduction values is −9%. The countries in the donor pool adopt arguably similar transportation technologies as Finland. Limiting the donor pool to countries whose emissions are driven by a similar structural process as that of Finland serves as a reasonable potential comparison unit. The counterfactual trajectory of emissions emerges as a convex combination of emissions of six countries. These countries, with corresponding weight in a bracket, are the United Kingdom (43.20%), Turkey (18.40%), New Zealand (15.90%), Luxembourg (10.20%), Switzerland (9.40%), and the United States of America (2.90%). A useful parameter to summarize the effect of carbon tax is elasticity. Since the number of observations is very small, the OLS based estimation of elasticity is sensitive and its application is conceptually questionable. The initial values of annual elasticity are higher consistent with abundant possibilities of low-hanging abatement options. While the values of annual elasticity oscillate, the arithmetic and geometric mean values are 0.099 and 0.086 respectively. eople expect taxes to be permanent and long term while variations in fuel prices could be short-term. In response, people adjust both on the intensive and the extensive margins when facing carbon taxes whereas mostly on intensive margins for temporary fluctuations in gasoline prices. Third, carbon taxes carry a signal from a society that urge for reducing carbon emissions, a signal absent in temporary variations in gasoline prices. The identifying assumption underlying the synthetic control approach is that emissions in synthetic Finland serve as a valid counterfactual. For example, if countries price carbon indirectly through energy and fuel taxes, the identifying assumption calls for a similar evolution of such variables both in Finland and synthetic Finland. To take into account implicit carbon prices, I exclude countries that have raised their fuels taxes from the donor pool. I perform a back-dating test by introducing hypothetical carbon taxes in 1986, 1987, and 1988. Besides, I drop one predictor at a time to re-estimate emissions in synthetic Finland. While one recognizes that the Finnish recession in 1990 – 1993 could have played some role in reducing emissions, it is unclear if the recession is the central explanation for the observed emissions reductions in 1993 – 2005. Similarly, the high elasticity estimates may reflect the low-hanging fruits. The case for a carbon tax, when compared with auctioned quotas, is far from unanimous. The reservation goes back, at least, to Buchanan (1969, p. 175) – who aimed at contributing the project of “dismantling of the Pigovian tradition in applied economics, defined here as the emphasis on internalizing externalities through the imposition of corrective taxes and subsidies.” The current concerns include the difficulty of meeting a given emissions target (Harris and Pizer, 2020) and the ramifications for risk externality (Mideksa, 2020), the political difficulty of imposing a tax and changing it over time (Slemord and Bakija, 2017), the challenge of enforcing a tax and avoiding evasion in countries with a weak fiscal capacity (Acemoglu, 2005; Besley and Persson, 2009), the ease for allowing exceptions and loopholes and for undermining a tax through subsidies to complementary inputs, and other factors covered in Sterner and Coria (2012) and Stavins (2020). These factors, in addition to carbon leakage, can explain why there was no break in the trend of Finnish industrial CO 2 emissions around 1990. Focusing on political economy considerations, Brooks and Keohane (2020, p. 20) explain the point as follows. “In general, the environmental community is focused on ensuring emissions reductions, while the regulated industry is focused on limiting costs. The former’s strong preference for such environmental certainty helps to explain why existing market-based climate policies are overwhelmingly quantity-based; the latter’s insistence on some degree of cost certainty helps to explain why these policies generally also include price containment mechanisms.” 10 Emissions target can be unmet due to the uncertainty from business cycles or MIT shocks like pandemics. Other things being the same, carbon taxes deliver higher abatements during recessions and lower abatements during booms, relative to auctioned quotas. Since economies tend to have more periods of a boom than periods of a recession (e.g., see Figure 2 in Rebelo, 2005), some stakeholders (e.g., environmentalists that Brooks and Keohane (2020) refer to) worry that taxes can fail to meet emissions targets. Nevertheless, the Finnish experience in 1990 – 2005 suggests something else: a well-crafted carbon tax induces meaningful emissions reductions both in booms and recessions in the early phases of decarbonization. Thus, the Finnish experience does not support the idea that carbon taxes are excuses to continue emitting more. Mideksa (2021) Finnish Carbon Tax (pdf) 49.5 Instrument Choice Delays Roberts The capture of the climate policy debate by carbon-price-obsessed economists in the late 20th century helped send national and international climate policy down a multi-decade cul-de-sac in which very little was accomplished and much precious time was wasted. Roberts (2021) A rant about economist pundits, and other things, but mostly economist pundits Boyd Using the case of emissions trading, this Article investigates how the instrument choice debate has impoverished our conception of government and limited our capacity to respond to the climate crisis. The central claim is that the overly abstract theory of instrument choice that has underwritten widespread enthusiasm for emissions trading and other forms of carbon pricing over the last three decades has led to a sharply diminished view of public engagement and government problem solving. Boyd (2021) The Poverty of Theory: Public Problems, Instrument Choice, and the Climate Emergency 49.6 SSPs Buhaug Abstract The recently developed Shared Socioeconomic Pathways (SSPs) have enabled researchers to explore coupled human–nature dynamics in new and more complex ways. Despite their wide applicability and unquestionable advantage over earlier scenarios, the utility of the SSPs for conducting societal impact assessments is impaired by shortcomings in the underlying economic growth projections. In particular, the assumed economic convergence and absence of major growth disruptions break with historical growth trajectories in the developing world. The consequence is that the SSP portfolio becomes too narrow, with an overly optimistic lower band of growth projections. This is not a trivial concern, since resulting impact assessments are likely to underestimate the full human and material costs of climate change, especially for the poorest and most vulnerable societies. In response, we propose that future quantifications of the SSPs should incorporate the likelihood of growth disruptions, informed by scenarios of the relevant political contexts that historically have been important in curbing growth. How will climate change shape societies in coming decades, and what steps could be taken to avoid the gravest consequences? The recently developed Shared Socioeconomic Pathways (SSP) framework, which plays an integral role in the ongoing Intergovernmental Panel on Climate Change (IPCC) Sixth Assessment cycle, constitutes the most comprehensive attempt to date to model societal development consistent with different climate change scenarios (O’Neill et al. 2014; Riahi et al. 2017). The SSPs span a range of alternative futures, determined by assumptions about challenges to climate change mitigation and adaptation. Four pathways (SSP1, SSP3–SSP5) capture the four possible combinations of low versus high barriers to adaptation and mitigation, whereas the fifth (SSP2) represents a middle-of-the-road pathway. Central drivers of these challenges include changes in demographic, economic, technological, social, political, and environmental factors. The SSPs serve two key functions: to provide “a basis for integrated scenarios of emissions and land use” and to facilitate “climate impact, adaptation and vulnerability analyses” (O’Neill et al. 2017, 169). There is some tension between these functions, since the former is determined mostly by the development trajectories of large economies and major greenhouse gas (GHG) emitters, whereas the latter is much more sensitive to future development in low-income countries and the world’s poor. In other words, there is little overlap between the countries that contribute the most to anthropogenic climate change and those that are the most vulnerable to its impacts (Althor et al. 2016). Presently, the SSP framework appears better suited to fulfilling the first task than the second. In this research note, we show that existing quantifications of the SSPs, despite their wide applicability and unquestionable advantage over earlier scenario exercises, have clear limitations for researchers seeking to conduct societal adaptation and impact assessments because of shortcomings in the economic growth models underlying the SSPs. In particular, the assumption of growth convergence, whereby poorer countries gradually catch up with wealthy economies as long as educational attainment improves, and the related assumption of a future without major growth disruptions break with historical development trajectories. The result is an overly narrow and optimistic range of projected development outcomes. In response, we encourage revising or expanding the SSPs to incorporate growth projections that are sensitive to the underlying political and security contexts. Assumptions about such conditions are already embedded in the narratives that accompany the quantified SSPs (O’Neill et al. 2014, 2017), but presently, they exist in isolation from the growth projections. By bringing the political context explicitly into the quantitative scenarios, the SSP modeling community would help the IPCC get one step closer to achieving its objective: “to provide governments at all levels with scientific information that they can use to develop climate policies.” Buhaug Memo The quantiﬁcation of the SSPs consists of end-of-century population and urbanization projections, including changes in fertility and education ( Jiang and O’Neill 2017; KC and Lutz 2017), as well as three alternative projections of growth in gross domestic product (GDP), developed by modeling teams at the Organisation for Economic Co-operation and Development (OECD) (Dellink et al. 2017), the International Institute for Applied Systems Analysis (IIASA) (Cuaresma 2017), and the Potsdam Institute for Climate Impact Re- search (PIK) (Leimbach et al. 2017), respectively. Governance and security de- velopments are not part of the quantitative scenarios. Instead, aggregate descriptions of the regional and global political contexts are embedded in the qualitative storylines that accompany the SSPs. All three SSP teams modeling future economic growth adopted the aug- mented Solow growth model. A central feature of the Solow model is the convergence mechanism: that development is associated with diminishing marginal returns on invest- ments, such that it is cheaper and more viable for less advanced societies to absorb inventions from the technology frontier than for advanced societies to develop new technology. The augmented model assumes that the rate of convergence is conditional on human capital. If the convergence mechanism is a major driving factor of growth, we should observe a steady narrowing of the income gap between developed and developing countries as time pro- gresses, assuming that the populations in developing countries become increas- ingly educated. In the real world, economic convergence has been much less pronounced, despite signiﬁcant educational improvements in poor countries. Despite the historical prevalence of growth disruptions and a good scientiﬁc un- derstanding of important structural drivers, the economic growth models in the SSP portfolio abstain from incorporating negative shocks. For most countries, most of the time, this is not a problem, and the augmented Solow model has been shown to perform well in predicting welfare growth for the countries that account for the vast majority of global GDP (Mankiw et al. 1992). For the same reason, the SSPs are well suited to evaluating implications of alternative societal development trajectories for global GHG emissions and climate change mitiga- tion challenges. However, such models tend to return overly optimistic projec- tions in the long term, especially for countries at greater risk of experiencing growth disruptions. Known barriers to growth, such as resource dependence, war, and lack of good governance structures—all of which are part of the SSP narratives are unaccounted for in the GDP projections. The consequence is that the range of futures provided through the quanti- ﬁed SSP framework becomes too narrow and covers too small of an area of the conceivable probability space due to an overly optimistic lower band of growth projections. Of particular concern is the fact that the coun- tries for which the GDP projections will ﬁt the least well (i.e., developing countries with a history of recurring growth disruptions) are the very same countries where vulnerability to climate change is considered the highest and for whom sound adaptation and impact assessments may be most in demand. [Attempts to use the SSPs] are thus at risk of overestimating future production and security improvements and underestimating the relative cost of choosing a development pathway akin to regional rivalry (SSP3) or inequality (SSP4) over sustainable development (SSP1). Although modelers need high- growth scenarios, and there are reasons to remain optimistic about long-term development in many of today’s poor countries, sound and comprehensive impact assessments also require projections that are at least as pessimistic as the recent past. Buhaug (2019) On Growth Projections in the Shared Socioeconomic Pathways (pdf) 49.7 Guard Rail Economics Stern We risk loss of life in the hundreds of millions or billions; because we do not know what the “carrying capacity” of a world of 4 or 5 o C might be. It could be much lower than the 9-10 billion or so expected towards the end of the century. It is hard to understand or put numbers on the potential devastation and agony around the process of loss of life that could be involved. It is difficult, in particular, to argue that an expected utility approach captures the issues at stake in a plausible way. In my view, a direct risk-assessment looking across possible consequences and a guard-rail approach is more thoughtful, reasoned, broad-ranging and robust. And it is clearly seen as a reasonable and rational approach by the body-politic. A new form of Growth The necessary rapid change across the whole system, just described, can be a story of growth, indeed the only sustainable story of growth. In the shorter term, the necessary investments can boost demand in a world where planned savings exceed planned investments (with sluggish demand and low real interest rates). In the short and medium term it is full of innovation, investment, discovery, and new ways of doing things. It can be more efficient; and much cleaner. It can create cities where we can move and breathe, and ecosystems which are robust and fruitful. It is potentially a very attractive, different way of doing things, relative to past dirty models, with so many gains across the different dimensions of well-being. But that does not mean that it is easy. It does mean that it is sensible, it does mean that it is attractive, and it is within our grasp. We have to change radically and, particularly, invest and innovate strongly to get there. That is the challenge. But there can be a real payoff in terms of a much better form of growth. Can it be done? The answer is ‘yes’ and in particular there are four forces at this current moment which are particularly favourable to moving quickly and on scale: low interest rates, rapid technological change (see section 2.4), international understandings coming together (including the UNFCCC, COP21, the Paris agreement of 2015 and more than 100 countries covering 61% of emissions committing to net- zero by mid-century (Black, et al., 2021)), and pressure from the young people of the world to change (for example, Fridays for the Future and strong activity in the universities of the world). Investment Strong, internationally coordinated investment should be at centre stage, right through from recovery from the COVID pandemic to transformational growth and the drive to a net-zero economy. What kind of orders of magnitude of investment do we need to make? To bring through the new ways of doing things and the new technologies required to make that happen, we have to increase investment by around 2-3 percentage points of GDP across the world, relative to the previous decade. Policy These increases in investment, will require strong policy and a positive investment climate, including the functioning of relevant governmental institutions. Further, the many relevant market failures (see section 7b) and the urgency of change indicate the necessity of a whole range of policy instruments. Carbon pricing will be important, but alone it will not be enough. Complementary policies, including city design, regulation and standards, and investments in R&amp;D, will also be needed. Finance Getting the right kind of finance, in the right place, at the right time is not easy. Mobilising private sector finance, at scale, will be critical. But there will also be a need for development finance and concessional finance to support the activities that do not quickly generate strong revenue streams or have high risks. The international financial institutions, especially the multilateral development banks, and including the IMF, have a crucial role to play. How economics must change An assessment of what the current situation demands of us, particularly for this decade, was set out. That requires changing our ways of producing and consuming, rapidly and fundamentally, and creating the investment, innovation, sets of policies, and the finance that could foster and support the change. How can we bring our economics to bear in a way that informs those very real and urgent problems? How can we use economic analysis to tell us as much as it possibly can about why to do this, how to do this, and the methods and policy instruments we should use? I will focus, in terms of broad analytical approaches, on where we are in the economics discipline on climate change and argue that it is time for change in the economics of climate change and, in some respects, economics generally. Our subject does have much to offer in applying our existing tools and in developing new perspectives and analyses, but we must be innovative and, as a profession, engage much more strongly on this, the biggest issue of our times. A starting point is the important set of insights of economists Alfred Marshall and Arthur Pigou. At the end of the 19 th century, Marshall (Marshall, 1890) drew attention to the potential difference between marginal private cost and marginal social cost. Thirty years later, Pigou (Pigou, 1920) argued for a tax, equal to the difference between the marginal private cost and the marginal social cost, to correct for an externality, where that is the source of the difference 11 . Around 60 or 70 years ago, Ronald Coase began considering these concepts in a different way, emphasising institutional arrangements (Coase, 1960). He spoke of allocating property rights and establishing markets so that there could be trade in externalities. James Meade - his work ‘Trade and Welfare’ (Meade, 1955) was a landmark - also wrote very insightfully about the theory of externalities, including integrating externalities into the theory of reform, bringing in distributional issues and looking at general equilibrium in multi-good models. Coming forward further, and looking at applications 30 or so years ago, David Pearce, for example, was writing ‘Blueprint for a Green Economy’, emphasising how the Pigouvian idea could be implemented (Pearce et al., 1989). The modelling of climate change began with Bill Nordhaus’ important and admirable paper ‘To slow or not to slow?’, published in the Economic Journal in 1991 (Nordhaus, 1991) and Bill Cline published his book ‘The Economics of Global Warming’ in 1992 (Cline, 1992). Nordhaus’s question, recognising that there could be potential dangers from climate change and that emissions arose from activities around producing and consuming, was ‘should we grow a little less fast than we might have envisaged before we thought about climate change?’. He proceeded in a sensible way, taking an emerging problem and applying the standard tools of economics: first the Pigouvian story of marginal social costs, marginal private costs, and taxing for the externality; second on growth, he used the framework of a standard exogenous growth model and considered the impact of climate change largely in terms of small perturbations around the underlying growth path(s). That was a sensible early contribution for the economics of climate change. Over the following 10-15 years, it became more and more clear that climate change is not a marginal problem. We are dealing with a challenge involving huge potential disruptions, which requires very radical changes in our production systems and ways of consuming. That simply cannot be picked up by assuming a fairly standard underlying model of exogenous growth and, within that model, portraying climate change in terms of marginal damages of just a few percent of GDP. Nordhaus’ DICE model launched a major literature on integrated assessment models (IAMs), and their scope has been expanded. But the basic underlying features of optimisation of explicit, calibrated social welfare functions, underlying exogenous growth and aggregation (usually to one good) impose severe limitation on their ability to illuminate two basic questions. The first is how to approach analytically the challenge of managing immense risk, which could involve loss of life on a massive scale. The second is how to chart and guide a response to this challenge which will involve fundamental structural change across a whole complex economy. These two issues are at the core of economic policy on climate. The basic structure of IAMs, I shall argue, even with the many advances and mutations that have been offered, is not of a form which can tackle these two questions in any satisfactory way. There is a problem in the profession, which goes beyond the way IAMs are structured and specified, associated with an inability or unwillingness to move much beyond the static Pigouvian or 20 th century approach to externalities in analysing the challenges of climate change. Many discussions of policy suggest that “economic theory says” that policy should be overwhelmingly about a carbon price. That “theory says” that the carbon price is the most effective route is simply wrong and involves a number of mistakes. The first mistake is that there is a whole collection of market failures and market absences of great relevance beyond the greenhouse gas externality (see section 7). The second is that under the temperature target or guardrail approach (see section1), the choice of carbon prices is focused on its role, in combination with other policies, in incentivising paths which achieve the overall target (such as net-zero emissions by mid-century to fit with the temperature target) with as much economic advantage as possible. Such prices are not simply the marginal social cost as in Pigou (see discussion of Stern- Stiglitz Commission below, this section). Third, where the risks of moving too slowly are potentially very large and there are increasing returns to scale and fixed costs in key industries, then regulations can help reduce uncertainty and bring down costs (e.g. Weitzman, 1974). Fourth, many consumers, producers, cities, and countries, recognise the obligation to act, and are not blinkered, narrow optimisers with a view of utility focused only on their own consumption. Fifth, much of the challenge of action is how to promote collaboration and act together. This poses a whole set of important questions around institutions and actions for mutual support. This is an immense challenge concerning risk, values, dynamics and collaboration, and the narrow Pigouvian model, useful though it is,is very far from the whole story. Failure of IAMs There is an underlying one-good growth model where emissions depend on output, accumulated emissions cause temperature increase and climate change, and emissions can be reduced by incurring costs. However, much of this literature, which has dominated so much work on the economics of climate change, has been misleading and biased against strong action, because climate damage specifications are implausibly low and costs of action implausibly high, and subject to diminishing returns. Most standard IAMs also embody diminishing returns to scale and increasing marginal costs of action to reduce emissions, plus modest rates of technical progress (relative to those experienced in the last decade or so). These features are very problematic because we have already seen how important increasing returns to scale and very rapid change in technology are in this context. By embodying diminishing returns and modest technical progress, the IAMs systematically overstate the costs of climate action. Further, they distort the theory of policy which is much more complex when we have increasing returns to scale; particularly in the context of risk. Standard optimising policy models which focus on “marginal cost equals marginal benefit” are far more tractable with diminishing returns and increasing marginal costs to action, but by choosing model assumptions primarily for tractability and convenience, we risk severely disturbing the policy discussion at issue. There are deeper problems with the general approach of maximising a social welfare function (for example, based on expected utility) in the presence of extreme risk, which cannot be corrected by adjusting functions and parameters. Standard utility or welfare functions at the heart of the IAMs cannot capture adequately the nature and scale of the risks from climate change. Impacts which can involve deaths of billions are not easily captured in the standard social welfare functions, which we used in most IAMs (and more broadly), involving aggregation of individual utility functions. Indeed, as Weitzman argued (Weitzman, 2009, 2012) standard approaches quickly run into problems of utility functions going to minus infinity. There can be arbitrary “fixes”, for example by putting bounds on utility, but it is an indication that the model has lost touch with the problem. Just as with the social welfare function aspect of IAMs, there is a deeper question on the production side of the modelling. The policy challenge, as we have seen, involves generating rapid and major change in key complex systems, including energy, transport, cities and land, over a very short period. Simple “cost” functions for emissions reductions, even if made more realistic, do not get to grips with the real policy challenges of how to make these changes. We are in a world with many market imperfections, with major risks, requiring fundamental systemic change, and where optimisation is difficult to define, let alone achieve. There is no serious ethical argument in favour of pure-time discounting. There is little point in looking for ethical values relevant to social discounting in capital markets, because capital markets: (i) do not reflect ethical social decisions; (ii) they embody expectations and views about risk that are hard to identify; and (iii) they involve many imperfections. Nevertheless, one often seems to hear the mistaken argument that social preferences can be derived from these markets. Economic analyses of climate change must first capture extreme risk, including possible large-scale and unforeseeable consequences. Second, they should recognise that many key markets have critically important failures (beyond that of the GHG externality), that crucial markets may be absent, and that there are limits on the ability of government to “correct” these market failures or absences. Third, they should embody rapid technical and systemic change, often in very large and complex systems such as cities, energy, transport, and land use, and allow for increasing returns to scale. Fourth, they should examine rapid changes in (endogenously determined) beliefs and preferences; and fifth, take into account distributive impacts and risks, both at a moment in time and over time, and including those associated with structural change. All of this will unavoidably involve explicit analysis and discussion of value judgements. These components, or sets of questions, are difficult to incorporate in standard integrated assessment modelling, but are at the core of the issues around understanding policy towards climate change. We must deepen our economic analysis to incorporate them. We should also recognise that questions embodied in, or similar to, these components arise in many other parts of economics, where major risks and fundamental change are at the core of the challenge under examination. Given that governments are made up of complex compromises and coalitions, are limited in information and capabilities, and are not necessarily long lasting, we must recognise in our analysis that there are limits on their ability or willingness to “correct” for market failures and absent markets. Governments cannot fully commit to future actions in a credible way. The GHG failure is top of our list of market failures. And carbon pricing has a critical role to play in tackling that market failure. However, we can see, from thinking about different aspects of market and government failures, that the policy question is much richer than carbon pricing alone. Regulatory policies, alongside carbon pricing, could be more efficient and effective than carbon pricing alone. The need for new approaches to economic analysis of climate change raises an enormously rich research agenda. At the same time, action on scale is urgent. The necessary transformation of the economy relies critically on changing key systems: energy, cities, transport, land use. These large and complex systems cannot be changed by fiddling with just one parameter, a whole set of policies will be required to foster change. For example, you would not sensibly attempt to redesign a city to reduce congestion and pollution just via a carbon price. Most elements of economics come into the challenge of climate change. It is time for change in economics. Stern (2021) A time for action on climate change and a time for change in economics (pdf) 49.8 Tipping Points Dietz Dietz (2021) Economic impacts of tipping points in the climate system (pdf) [SI (pdf)(pdf/Dietz_2021_Economic_Tipping_SI.pdf) Keen on Dietz This was an invited talk to the Oxford Department of International Development “Climate Change and the Challenges of Development Lecture Series”, on my criticisms of the application of neoclassical economics to climate change. I focused on the new paper by Dietz et al. that allegedly calculates the economic costs of tipping points: Dietz, S., J. Rising, T. Stoerk and G. Wagner (2021). “Economic impacts of tipping points in the climate system.” Proceedings of the National Academy of Sciences 118(34): e2103081118. Upon closer examination, this papers fails to consider tipping points in any credible way, and this is obvious in its incredible claim (in the original sense of the “not credible”), that: “Tipping points reduce global consumption per capita by around 1% upon 3°C warming and by around 1.4% upon 6°C warming&quot; This is ridiculous: the tipping points they consider are: Arctic summer sea ice, The Greenland Ice Sheet, The West Antarctic Ice Sheet, The Atlantic Meridional Overturning Circulation (“Gulf Stream”), The Amazon Rainforest, The Indian Monsoon, Permafrost, and Ocean methane hydrates. If all 8 of these tripped–especially with a temperature 3-6°C above pre-industrial levels–we would be experiencing a climate utterly unlike anything Earth has seen for tens of millions of years. The thought that this would just reduce global consumption by just 1.4%–compared to what it would be if none of these tipping points were triggered–doesn’t pass what Nobel Laureate Robert Solow once called “the smell test”: “every proposition has to pass a smell test: Does it really make sense?”. I show why this paper stinks in Solow’s sense (slides here). Keen (2021) From Economic Fantasy to Ecological Reality on Climate Change 49.9 Keynesian Decarbonization Mason In the Keynesian vision, the economy is imagined as aa system of monetary production rather than real exchange, with the binding constraints being not scarce resources, but demand and, more broadly, coordination. Economic activity is coordination- and demand-constrained, not real resource-constrained. Production is an active, transformative process, not just a combining of existing resources or factors. Money is a distinct object, not just a representative of some material quantity; the interest rate is the price of liquidity, not of saving. These premises have a number of implications for climate policy. Decarbonization will be experienced as an economic boom. There is no international coordination problem There is no tradeoff between decarbonization and current living standards. Price based measures cannot be the main tools for decarbonization. Central bank support for decarbonization must take the form active credit policy. Sustained low interest rates will ease the climate transition. There is no link between the climate crisis and financial crisis. There is no problem of getting private investors to finance decarbonization. We face a political conflict involving climate and growth, this will come not because decarbonization requires accepting a lower level of growth, but because it will entail faster economic growth than existing institutions can handle. Today’s neoliberal macroeconomic model depends on limiting economic growth as a way of managing distributional conflicts. Rapid growth under decarbonization will be accompanied by disproportionate rise in wages and the power of workers. Rapid decarbonization will require considerably more centralized coordination than is usual in today’s advanced economies. If there is a fundamental conflict between capitalism and sustainability, I suggest, it is not because the drive for endless accumulation in money terms implies or requires an endless increase in material throughputs. Rather, it is because capitalism treats the collective processes of social production as the private property of individuals. (Even the language of “externalities” implicitly assumes that the normal case is one where production process involves no one but those linked by contractual money payments.) Treatment of our collective activity to transform the world as if it belonged exclusively to whoever holds the relevant property rights, is a fundamental obstacle to redirecting that activity in a rational way. Resistance on these grounds to a coordinated response to the climate crisis will be partly political and ideologically, but also concrete and organizational. Mason (2021) Climate Policy from a Keynesian Perspective 49.10 Discount Rate Bichler Nitzan With this discount rate, today’s present value (PV0) of $100 worth of climate cost incurred one hundred years from now (n=100) is approximately $10. But there is nothing to prevent Nordhaus from using a different rate. A slightly higher rate of 3% (dashed series), for example, will cause today’s present value to drop by one half, to a mere $5, give or take. And Nordhaus doesn’t have to stop there. He can go the Full Monty, push the discount rate up to 4.7% (solid series), and reduce the present value to a paltry $1. Blessed are the wonders of compound interest. The nice thing about these discount-rate ‘adjustments’ is that, unlike the commotion stirred by debates over the actual cost of climate change, here there are no messy quarrels with scientists, no raised eyebrows from journalists and no outcries from the cheated public. Only contented politicians and delighted capitalists. And that is exactly the route chosen by William D. Nordhaus. In his 2007 ‘Review of the Stern Review on the Economics of Climate Change (pdf)’, he mocked Lord Nicolas Stern’s assumption of a low discount rate of 1.4%, suggesting we should instead discount the future by his favourite rate of 6%. And that mockery succeeded wonderfully. By leveraging the capitalization ritual in the name of profit and glory, Nordhaus managed to not only help investors minimize the apparent cost of climate change, but also win the Economics Nobel Prize as the white knight of nothing less than . . . ‘sustainable global economic growth’! Who says you can’t eat your cake and have it too? If there is a civilizational lesson from this fiasco – assuming we still have time for such lessons – it is that we need to bar the capitalized fantasy of never-ending growth from any discussion regarding the ecological future of humanity. Bichler Nitzan (2018) The Nordhaus Racket 49.11 Geoff Man on Nordhaus Mann The ​ American economist William Nordhaus opened his Nobel Prize speech in 2018 with a slide of the painting El Coloso, traditionally attributed to Goya and completed sometime between 1808 and 1812. Like Goya’s better-known images of the Madrid uprising of 2 May 1808 and the bloody retribution that came after, the painting depicts the calamitous violence of the Peninsular War, which followed Napoleon’s invasion of Spain. But while Goya’s intentions are clear in El Dos and El Tres de Mayo, it is much less obvious what is going on in El Coloso. A giant man, shrouded in mist, looms over the hills. In the dark valley below, people and animals are caught in desperate flight. Only a single donkey remains still, unflustered. The giant is half-turned away from us and from the refugees, his fist raised, ready to fight. But his eyes appear to be closed. Who or what presents the threat isn’t visible to us. Is the colossus protecting the people, or is it him they fear? Does he symbolise the French armies wreaking havoc across the Spanish hills or Spain standing up to the invaders? If the giant is Spain, which Spain is he: the Bourbons demanding restoration or the liberal republicanism that flared briefly at the time Goya was painting? Nordhaus, whose speech was about the contribution he has made to the economics of climate change, didn’t engage with any of this. For him, El Coloso was simply a metaphor for global warming, which ‘menaces our planet and looms over our future’. It stands outside and above humanity, ‘the colossus of all environmental externalities’, before which any single actor – person, nation, region – is powerless. Nordhaus doesn’t claim to be an art historian, so perhaps he can be forgiven for not recognising that in the world of the colossus, there is no such thing as an ’externality. Mann (2022) Check Your Spillover 49.12 Persistence of Temperature Effects Traditional Approach Casey Abstract We use theory and empirics to distinguish between the impact of temperature on transition (temporary) and steady state (permanent) growth in output per capita. Standard economic theory suggests that the long-run growth rate of output per capita is determined entirely by the growth rate of total factor productivity (TFP). We find evidence suggesting that the level of temperature affects the level of TFP, but not the growth rate of TFP. This implies that a change in temperature will have a temporary, but not a permanent, impact on growth in output per capita. To highlight the quantitative importance of distinguishing between permanent and temporary changes in economic growth, we use our empirical estimates and theoretical framework to project the impacts of future increases in temperature from climate change. We find losses that are substantial, but smaller than those in the existing empirical literature that assumes a change in temperature permanently affects economic growth. Casey Memo Two different standard approaches to modeling the relationship between temperature and economic output. The first approach assumes a level effect: that a one time, permanent change in temperature affects the long-run level of output, but not the long-run growth rate of output. The second approach assumes a growth effect: that a one time change in temperature affects the long-run growth rate of output. Macroeconomic climate-economy models almost always assume that temperature has a level effect. In contrast, an empirical literature argues that temperature has a growth effect. Estimating the effects of temperature on TFP, instead of on GDP per capita, can circumvent the issues posed by endogenous capital dynamics and better distinguish between level and growth effects in panel data. Finds level effect of temperature on TFP, but not a growth effect of temperature on TFP. Fig: Non-linear impacts of temperature change. The vertical lines denote the average annual temperature for selected countries in 2010, indicating their vulnerability to future changes in temperature. Fig: Impact of climate change between TFP and capital per person for the 25 most populous countries. (RCP 8.5) Casey (2022) Projecting the Impact of Rising Temperatures: The Role of Macroeconomic Dynamics (pdf) New Approach Duncombe on Bastien-Olvera Do shifts in temperature have enduring economic impacts? A “clever” trick identifying climate trends gets us one step closer to addressing this long-standing question in climate economics. An open question in economics is whether shifts in temperature have long-lasting, permanent impacts on economic productivity. A drop in economic levels after an unusual event, like a heat wave, might be troubling but temporary. Economic growth deflating over a long period, however, is much more problematic. Understanding this question is essential for calculating the social cost of carbon, a critical metric for governments weighing future infrastructure and policy decisions. Now, researchers from the United States and Italy have found evidence of changes to economic growth associated with temperature shifts. These changes occurred in the gross domestic products (GDPs) of about a quarter of the world’s countries over 10–15 years. Standard economic analyses “could be dramatically underestimating the economic impacts of a warming climate.” The question here is whether an economy is able to come back from these shocks year to year, or whether the shocks permanently change the pathway of the country’s economy. Using innovative filtering, the researchers zeroed in on climate trends instead of hard-to-parse day-to-day weather shifts. Previous work relied on a classical economic technique comparing the lagged effects of temperature changes on economic metrics. Instead, Bastien-Olvera and his colleagues filtered out short-term changes in temperature but kept more extended, slower developments and patterns. They focused on the temperature swings of naturally caused climate phenomena, like the El Niño–Southern Oscillation and the Pacific Decadal Oscillation, a cycle in the ocean and atmosphere that alters the temperature of Pacific waters. They compared the temperature shifts with World Bank data on the GDPs of more than 200 countries spanning 6 decades. The bread and butter models of climate science and economics—integrated assessment models—rely on economic assumptions about, among other things, GDP growth. Specifically, these models assume that there are no persistent effects from temperature changes on GDP growth. The latest research has cast doubt on that assumption. Duncombe (2022) A New Approach to an Unresolved Mystery in Climate Economics Bastien-Olvera Abstract It is well established that temperature variability affects a range of outcomes relevant to human welfare, including health, emotion and mood, and productivity across a number of economic sectors. However, a critical and still unresolved empirical question is whether temperature variation has a long-lasting effect on economic productivity and, therefore, whether damages compound over time in response to long-lived changes in temperature expected with climate change. Several studies have identified a relationship between temperature and gross domestic product (GDP), but empirical evidence as to the persistence of these effects is still weak. This paper presents a novel approach to isolate the persistent component of temperature effects on output using lower frequency temperature variation. The effects are heterogeneous across countries but collectively, using three different GDP datasets, we find evidence of persistent effects, implying temperature affects the determinants of economic growth, not just economic productivity. This, in turn, means that the aggregate effects of climate change on GDP may be far larger and far more uncertain than currently represented in integrated assessment models used to calculate the social cost of carbon. Bastien-Olvera Memo Studies that allow climate change to affect the determ- inants of economic growth tend to produce far lar- ger aggregate climate change costs than studies that impose only level effects on production. Persistent impacts operate via effects on the growth rate compound over time, producing far larger aggregate damages over the long time frames relevant for assessing cli- mate change costs. We analyze the temperature-growth relationship with country-level regressions. The smaller temperature ranges allow us to accurately model the effects using a linear approximation. In addition, instead of using high-frequency, year-to-year tem- perature variation to estimate climate impacts on the economy, here we use lower frequency vari- ation. Our identification strategy focuses on the per- sistent effect of temperature by adjusting for time trends and country-specific dynamics (via demeaning and detrending) but uses lower-frequency temperat- ure variability instead of lags to distinguish between growth and levels effects. Using a low-pass filter instead of lags avoids adding noise terms together that could prevent identifying medium run persistent effects. Applying this test to three different datasets of economic growth, we fail to find strong evidence of only non-persistent effects. There are two key pieces of evidence. First, we found statistically significant persistent temperature impacts on economic growth in 22% (19%; 8%) of the countries using the World Bank (Maddison Project; Barro-Ursua) dataset. Sig- nificant effects in these regressions implies the persist- ence of temperature impacts at least over the 15 year period of our lowest-frequency regressions. Secondly, we examine how regression estimates change using lower frequency temperature variation. The lack of persistent effects, as posited by the vast majority of integrated assessment studies estimating climate damages, would imply convergence of these estimates towards zero. But we fail to find evidence of such con- vergence. The evidence suggests a sensitivity of aggregate economic output to temperature shocks persisting over at least the 10– 15 year time frame and a conspicuous absence of evid- ence for fully non-persistent levels impacts. Our approach is not able to distinguish between a levels effect that contin- ues compounding over the 15 year time-frame of our lowest-frequency estimates but then subsequently reverses, and a ‘pure’ growth effect in which there is no subsequent reversal. Differentiating these two types of effects is a question of what happens in time- frames longer than 15 years, which is an inherently difficult empirical question due to the relatively short time span of data available. However, either interpret- ation of the filtered results (i.e. 15 years of continu- ously worsening levels effects followed by reversal or a fully persistent effect) implies persistence of dam- ages over time periods longer than a decade. Either interpretation would imply larger aggregate climate damages than the standard approach to representing climate change costs in integrated assessment mod- els, which assumes no persistence or compounding effects. While providing evidence of persistent impacts of temperature shocks on growth, our framework does not isolate the mechanisms by which they arise. Restricting modeling of climate change damages to only non- persistent levels effects likely greatly under-states both the uncertainty and the downside risk associated with climate change. Decade-long temperature excursions used to estimate the effects here are very small in amplitude (the median amplitude for 15 year filtered temperature is 0.11 ◦ C). It is an open question whether these effect sizes can be extrapolated to much larger changes in temperature expected with climate change. This highlights a fundamental empirical chal- lenge in estimating the effects of climate change. Climate change will produce large (∼2C–4C) and sustained changes in temperature. The histor- ical record contains both large but short temperature excursions and much smaller but longer temperature variation. Our approach is not able to distinguish between a levels effect that contin- ues compounding over the 15 year time-frame of our lowest-frequency estimates but then subsequently reverses, and a ‘pure’ growth effect in which there is no subsequent reversal. Differentiating these two types of effects is a question of what happens in time- frames longer than 15 years, which is an inherently difficult empirical question due to the relatively short time span of data available. While providing evidence of persistent impacts of temperature shocks on growth, our framework does not isolate the mechanisms by which they arise. Past studies have modeled persistent impacts as resulting from a slow-down in total factor productivity growth, changes to the capital depreciation rate, or impacts to the stock of natural capital. Other studies leave the mechanism of growth rate impacts unspecified. Letta and Tol investigate this question and suggest impacts arise through effects on total factor productivity growth. Allowing for persistent damages, because of their compounding nature, vastly increases the uncertainty in climate change impact projections. The persistence of economic damages is the most important parameter determining aggregate climate change costs. Restricting modeling of climate change damages to only non- persistent levels effects likely greatly under-states both the uncertainty and the downside risk associated with climate change. Bastien-Olvera (2022) Persistent effect of temperature on GDP identified from lower frequency temperature variability (pdf) SM (pdf) 49.13 Climate economics has let us down Dessler These models create the illusion that this is a well-constrained, quantitative, and reliable result, but it is most certainly not. In reality, we have no idea how bad the impacts of climate change will be. In both of Nordhaus’ books, A Question of Balance and Climate Casino, he says that the optimal amount of warming was around 3.5°C. Not every economist was saying this at that time. A few (e.g., Stern, Weitzman) understood the challenge of climate change and I give them a lot of credit for publicly pushing back against the mainstream. Ultimately, what does strike me is the absolute confidence they have in their conclusions. There are no error bars on his numbers, no estimates of uncertainty. The good news is that I don’t hear economists talking about optimal temperature responses anymore. And, luckily, the world ignored them when it negotiated the Paris Agreement targets of well below 2°C. But the stain on climate economics is real and will be lasting. Their confident claims about small economic impacts for very large warming combined with estimates of high costs of preventing that warming have been widely used by contrarians to argue against urgent action on climate change. A dismal science, indeed. Dessler (2023) Climate economics has let us down "],["tax.html", "50 Tax 50.1 Corporate Tax", " 50 Tax 50.1 Corporate Tax Profit Shifting OECD Process There remain significant difficulties in the OECD process, with its two-pillar proposals. First, there is no common ground on ‘Pillar One’. This is the element which would go beyond the archaic arm’s length principle and introduce some element of formulary apportionment (that is, allocating a share of each multinational’s global profits to the places where they actually do business, in the form of sales and employment). The US (under Biden, as under Trump) wants Pillar One to apply to all businesses; the EU is focused on the big tech multinationals; and the OECD proposal to identify ‘consumer-facing’ businesses falls somewhere in between these. As things stand, the OECD proposal is highly complex and would retain arm’s length pricing for most profits, and therefore result in relatively little reduction in profit shifting – making it largely unattractive for most countries. At the same time, the proposal would require global treaty change, meaning that it could very easily be blocked – including by the US Congress, regardless of whether the Biden administration had come around to support it. ‘Pillar Two’ contemplates a global minimum corporate tax rate. This has the potential to go a long way to stop profit shifting, not by making it harder to achieve but by making it much less rewarding – since multinationals would, in theory, end up being taxed at the minimum rate even if they managed to shift the profits to a zero rate jurisdiction. Here again though, the current OECD proposals are highly complex, and have been very unambitious. And an argument about ‘rule order’ – in simple terms, whether the home country of a multinational goes first in levying any top-up tax, or the various host countries – has exposed the major distributional question. Despite some initial optimism, most non-OECD members have by now become thoroughly disillusioned with the process. An outcome that favours OECD members, despite lower-income countries bearing disproportionately high revenue losses due to profit shifting, would be unconscionable – but, sadly, not entirely unexpected. Lastly, the insistence that the two pillars are inseparable, and must be delivered jointly, creates a hugely complicated contraption requiring great resources to move ahead, but with little certainty over any benefits. Way Forward Stepping out of the limitations of the OECD process, things very quickly start to look much brighter. This is for three main reasons. First, the two pillars can be separated – and that means the unworkable and unambitious ‘Pillar One’ can be left behind, along with the requirement for global treaty change. Instead, a global minimum corporate tax can be taken forward by a coalition of the willing. (In fairness to the OECD secretariat, they have raised this possibility at times also, recognising the practical difficulties of their Pillar One.) With the US and Germany (and the European Commission) committed to a minimum tax, broad agreement on the shape could be reached relatively quickly. Second, the OECD leadership’s longstanding insistence on a very low minimum rate of 12.5% can be set aside. The Biden administration has indicated a rate of 21%. The Independent Commission for the Reform of International Corporate Taxation has proposed 25% as an absolute minimum; while discussions among various groups of lower-income countries have suggested higher rates still, to ensure that they are not disadvantaged. Negotiating upwards from 21% – and with the possibility of different countries taking their own approaches as appropriate – would provide a quite different dynamic. And third, the setting aside of Pillar One creates the possibility of pursuing a more ambitious approach to the minimum tax. Our proposal for the METR, or Minimum Effective Tax Rate for multinationals, does just this. We propose a method that builds on the technical efforts of the OECD secretariat, who have done sterling work in establishing various approaches to identify and to apportion taxable profits, but shifts the politics substantially. METR In effect, the METR combines the two pillars by identifying under-taxed profits, and then apportioning these for ‘top-up’ taxation on a formulary basis, according to the location of multinationals’ real activity. In this way, the METR cuts through any ‘rule order’ debates and instead treats all countries, home or host, on an equivalent basis. Politically, the momentum for globally inclusive solutions at the UN, rather than the rich countries’ club at the OECD, will continue to grow – and especially if a one-sided minimum tax solution emerges from the OECD now. The FACTI panel report called for the negotiation of a UN tax convention, which would provide the basis for an intergovernmental body under UN auspices to set corporate tax rules in future – including a global minimum tax rate designed to benefit all. That the US administration is now leading the push for an ambitious global minimum tax rate confirms this as the new norm. The decisions set to be made in the next couple of months, over technical design and political inclusion, will determine just how effective this can be in bringing an end – finally – to the decades-long race to the bottom in corporate tax. With the confirmation of this new narrative, it seems likely that what is left undone in the OECD process will be resolved through a combination of unilateral and UN action. Global Corporate Minimum Tax The Missing Profits of Nations Tørsløv Abstract By exploiting new macroeconomic data known as foreign affiliates statistics, we show that affiliates of foreign multinational firms are an order of magnitude more profitable than local firms in a number of low-tax countries. Leveraging this differential profitability, we estimate that 36% of multinational profits are shifted to tax havens globally. U.S. multinationals shift twice as much profit as other multinationals relative to the size of their foreign earnings. We analyze how the location of corporate profits would change if shifted profits were reallocated to their source countries. Domestic profits would increase by about 20% in high-tax European Union countries, 10% in the United States, and 5% in developing countries, while they would fall by 55% in tax havens. We provide a new international database of GDP, trade balances, and factor shares corrected for profit shifting. In contrast to the picture painted by official statistics, our results suggest that the corporate capital share has increased not only in North America but also in high-tax European countries. Capital is making a comeback globally, but its rise is obscured by the tax avoidance strategies of multinational companies. Tørsløv (2022) The Missing Profits of Nations (pdf) "],["welfare-1.html", "51 Welfare 51.1 The Social Guarantee 51.2 Net Domestic Consumer Surplus", " 51 Welfare 51.1 The Social Guarantee The Social Guarantee enshrines every person’s right to life’s essentials: education, health and social care, a decent home, childcare, nutritious food, clean air and water, energy, transport and access to the internet. For this to happen, all people must have access to collectively provided services that meet their needs, as well as to a fair living income. SocialGuarantee.org 51.2 Net Domestic Consumer Surplus Fouquet Roger Fouquet is currently working on estimating the Net Domestic Consumer Surplus (NDCS) for the United Kingdom over the last three hundred years. NDCS measures the area below the demand curve for each consumer category and above the price line, and sums all the consumer categories to produce an aggregate value. NDCS offers a new macroeconomic indicator of economic welfare and a complement to GDP. The preliminary results indicate that the broad trend in Net Domestic Consumer Surplus (NDCS) per capita is similar to GDP per capita – showing the improvements in welfare from the end of the nineteenth century, major increases since the 1950s and a decline following the Great Recession of 2007-8. Figure: Net Domestic Consumer Surplus (NDCS) per capita in the UK, 1700-2017 GDP per capita heavily under-estimates the consumer welfare gains at early stages of economic development and over-estimates the consumer welfare gains at later stages of economic development. For instance, each pound or dollar produced today is not generating as much consumer welfare as 90 years ago. Fouquet "],["about.html", "A About", " A About Dyre Haugen and Dyrehaugen is Webian for Jon Martin - self-owned Globian, Webian, Norwegian and Canarian with a background from industrial research policy, urban planning and economic development consulting on global, regional and urban scales. I am deeply concerned about the (insane) way humanity (i.e. capitalism) interfere with nature. In an effort to gain insights in how and why this happens stuff is collected from around the web and put together in a linked set of web-sites. The sites are operated as personal notebooks. However, these days things can be easily published to the benefit of others concerned with the same issues. But be aware - this is not polished for presentation or peer-reviewed for exactness. I offer you just to have a look at my ‘work-desk’ as it appears in the moment. Any comment or suggestion can be mailed to dyrehaugen@gmail.com You can follow me on twitter as @dyrehaugen. Thanks for visiting! "],["links.html", "B Links", " B Links Current Dyrehaugen Sites: rcap - On Capitalism (loc) rclm - On Climate Change (loc) recs - On Economics (loc) rfin - On Finance (loc) rngy - On Energy (loc) renv - On Environment (loc) rsts - On Statistics (loc) rurb - On Urbanization (loc) rvar - On Varia (loc) rwsd - On Wisdom (loc) Blogs: rde - Blog in English (loc) rdn - Blog in Norwegian (loc) Discontinued: jdt - Collection (Jekyll) (loc) hdt - Collection (Hugo) (loc) Not listed: (q:) dhe dhn jrw56 (z:) rcsa rpad rstart "],["news.html", "C NEWS C.1 221220 Market-based development finance in crisis C.2 210717 Carney calls for stronger Government Regulation", " C NEWS C.1 221220 Market-based development finance in crisis On December 13 Ghana reached staff-level agreement on a $3 bn IMF credit package. In addition it is seeking to negotiate a 30 percent haircut with private creditors on tens of billions in bonds. Already in September Ghana’s 2026 eurobonds plunged to a record low of 59.30 cents on the US dollar. By the end of October yields had surged to 38.6 %, up from less than 11% at the end of 2021. Meanwhile, inflation is headed to 40 percent and the cedi is the worst performing currency not just in Africa but of all currencies in the world. You could shrug and say that this is Ghana’s second IMF deal in 3 years and its 17th since independence in 1957. Plus ça change. But it is more than a national crisis. It is the latest sign that the entire model of market-based development financing is in crisis. Tooze (2022) Chartbook #181: Finance and the polycrisis (6): Africa’s debt crisis C.2 210717 Carney calls for stronger Government Regulation For the world to meet its climate goals, governments would have to force industries to follow clear rules, on everything from energy generation to construction and transport, and set carbon prices that would drive investment towards green ends and close down fossil fuels. “We need clear, credible and predictable regulation from government,” he said. “Air quality rules, building codes, that type of strong regulation is needed. You can have strong regulation for the future, then the financial market will start investing today, for that future. Because that’s what markets do, they always look forward.” Without such robust intervention from governments, markets would fail to address the crisis. Gurdian "],["sitelog.html", "D Sitelog", " D Sitelog Latest Additions December 12, 2023 inflation\\        Inflation 2023 is like the 1940s not the 1970s weber December 17, 2023 inflation\\        summers on lack of inflation theory December 20, 2023 business-cycles\\        4 theories ofinflation management - us soft landing 2023 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
